# Da-Kuan-Kuan

链接：https://www.zhihu.com/question/370830450/answer/1010322452

## golang为什么将method写在类外?

go本质上反对那种OOP做法的。从go的哲学来讲，简单的东西才能写正确，好理解和维护。因此在go中并不提供OOP的“类”，而仅仅提供structure和属于structure的方法。

于是属于structure的方法可以设定自己到底是希望要\(t T\)还是\(t \*T\) ，以决定自己到底要不要复制一份数据。

go表达的就是函数就是函数，数据就是数据。与数据绑定的函数提供t.foo\(\)这种写法。但也仅此而已了。

## 面向对象编程的弊端是什么？

面向对象编程是一种处理复杂问题的设计工具，本身没有什么好坏之分，只有用的好坏之分。但面向对象的问题在于长期以来的技术环境、编程语言、一些工具的推广、培训和教育都大大的过分乐观的强调了面向对象编程本身可以带来的好处。以至于很多学习编程的人都深深的相信“只要用了面向对象编程（以及基于其基础之上的的一系列设计模式、规范、工具、框架），就能得到非常容易维护、可以复用、明晰可理解的代码“。

但，**这并不是真的**。

如果你经历过很多，就会发现“只要如何如何，就一定能如何如何”这个提法一旦出现，基本上就不靠谱，不管是编程还是别的什么事情。

在大量的场景中，可以偏执的认为“万物皆对象”（或者万物皆别的什么），但是哲学上的单纯并不一定能让现实中的工程变得更“好”。如果说非得有个“万物皆XX”，那么这个XX八成就是根据众多需求综合到一起的“**折衷**”。

简单从工程讲的话，如果程序（或者说工作）是一次性的，那么怎么写得快，能work就怎么来。这个相对好理解。但是，如果程序是要长期维护的，那么**如何管理其复杂性**是核心的问题。而管理复杂性的要点在于

* 让事情本身变得简单。这说白了就是砍需求，研发和PM之间要经常沟通去避免nice to have的需求变动带来的程序复杂性的剧烈变化（比如一个1对1的实体关系，需求变动一点就变成了麻烦的多的“有时1对1，有时1对多”的混合关系）。
* 运用隔离的手段将复杂性拆解为互相影响很小的单元。一个单元对外只暴露一个简单的“接口”，隐藏内部复杂性。这就是“抽象”或者“封装“的力量。但是问题在于，这个抽象本身是否做的合适是由于问题决定的，而不是代码本身决定的。

即便是抽象，也有很多种做法。可以定义一组接口，这个接口是一组函数、一组服务的RPC还是一个class的public method都可以根据实际情况商讨。面向对象只是这里面其中一种做法而已。一个想要把程序编好的人，需要注重的是理解问题，然后尝试做出几种不同的抽象，评估各自优缺点后得到一个当时可行解的能力。而现有的大环境、教育体系，没有那么多真实的、复杂的案例，只能用一些简单的sample code来教授。并且在说明问题本身时，简化问题本身，而突出代码设计的“模式”。这就好像是在用视频教人游泳一样。学习者自己需要认识到这些培训只是个参考，玩真的还是要到项目里去体会。

即便是用面向对象做抽象也会有问题。很多时候，面向对象编程并不是一种好的“抽象”。如果抽象做得好，透过抽象出来的“接口”就可以轻易的使用这个系统。这时“大量的复杂性”被隐藏到接口后的实现里。这就像是你看电视从来都不需要拆开壳子看里面液晶屏幕和视频信号的转换，只需要知道【电源】、【调台】、【调音量】就能用。一个抽象做得好，往往要“deep”，隐藏足够的复杂度。而面向对象的文化/教育往往会鼓励程序员做很多无意义的，无性价比的抽象。看看有些代码里完全不知所云的adaptor，factory，builder等就是这种做法的产物。

此外，在大量使用继承作为设计方法时，也没有起到任何实质的隔离作用。如果你尝试扩展一个继承体系，往往需要了解整个继承体系才能写对代码——这时，复杂性并没有被隐藏起来。你也许只是代码写的少了而已。对于这**种复杂度没有降低，编写代码只是写的少，但是要看懂还是得结合整个体系才能做到的方式，不是抽象，是“压缩”。**压缩只能少写代码，却会让系统更难以理解了。

> 也许不太容易理解压缩在这里意思。比如在一段被压缩的数据中有3个bytes是“A”，“1”， “8”。但是他们的意思可能是A连续出现18次，也许是A1连续出现8次。至于到底是哪个意思，必须从头读所有的数据才能弄明白。编码也是这个道理。

再说说类型本身。一些面向对象编码对类型的定义要求的比较严格。其本质假设是“如果一个Object的类型是XXXX”，则其行为模式必然是“YYYY”。但现实当中，一个Object的行为模式不光与他的类型有关，还与这个Object“如何被使用”有关。比方说，一个User的Object，如果是用户自己看自己，就可以登陆、登出，修改昵称；如果是其他普通用户看，就只能看看看昵称和头像；如果是管理员来操作，可以reset密码、注销或者踢出登陆。这时就得界定一个Scope，来说明现在的User到底是哪个scope的User。DDD的一些理念就源自于此——找到某个上下文的某个实体概念，不能有歧义。但是即便不用DDD，也必须用各种变通的手段，把“如何用”的信息与类型信息结合到一起来实现逻辑。很郁闷的是，这个“如何用”完全没有章法，可能是“iOS App登陆“，也可能是“第一次下单时”，或者是“系统处于降级状态”时。你永远也猜不到下一次可能会有个什么条件是要纳入到上下文的。大家都知道大量用if不好，容易让代码变成麻花，无法维护。但面向对象编程本身没解决这个问题。很多文章提出面向对象某个模式可以少写if，让代码容易维护。但是这其实是建立在那个问题的上下文已经明确的基础之上。上下文易变的问题没有解决，换一个上下文，招数便不灵了，到时还得处理一坨“模式代码”，非常恶心。

最后，面向对象会倾向于将不同的代码抽象为不同相互作用的Object，但是有一些现实因素会让这么面向对象得到非常不理想的效果：

* **安全** - 如果你的代码要求非常安全，那么所有的Object都要耦合安全控制的代码；要不就是在一层对外的接口之前拦截一道处理安全问题，内部Object都无视安全问题。这也就相当于放弃了一部分的安全性。
* **性能** - 如果强调性能的话，是要尽量减少隔离的层次的。无论抽象如何做，只要隔离发生，就要经历一次转换以及相应的性能损耗。比如早期的Hibernate不支持“bulk insert”和“bulk update”，只能逼着程序员做for loop IO；而native的sql却可以轻易办到。在每多一次IO都很伤的场景下，这种隔离只能把事情做的更糟。
* **数据为中心** - 很多业务场景都是以数据为中心。也就是说DB里的那坨数据是唯一的truth。在代码层面做的只是为处理数据更加方便。这时做的很多抽象意义不大。比如你可以在ORM层强制声明读取出来的一个数据少了某个字段是invalid的。但是你没法阻止你的第三方数据提供商源给你invalid的数据。对Invalid数据的处理远不是一个Annotation就能搞定的，必须引入复杂的业务流程。
* **灵活性和成本** - 每次做某种抽象都意味着对一个系统“要做某种变化的能力做出优化”，但是同时，也就意味着或多或少对其他种变化适应性做“劣化“。如果系统变化的方向和预期的不一致，那么浪费掉的工作不说，为了再次调整设计方向的代价也会相当的大。这种情况比比皆是。

总结下，我希望所有的程序员都要理解自己的工作的最终目的是干什么的，并且活用自己所能用到的一切工具来达成自己的目标。不要在各种编程范式里迷了路。如果是初学编程的人，我衷心的希望你的编程课程讲授的是解决一些实际的问题，多了解业务，多尝试对业务的变动作出合理和准确的预。不要过早的接触高层的思想和哲学层面的问题——一个小孩看《红楼梦》又能真的看懂多少呢。

## 怎么从本质上理解面向对象的编程思想？

面向对象编程（OOP），是一种**设计思想**或者**架构风格**。OO语言之父Alan Kay，Smalltalk的发明人，在谈到OOP时是这样说的：

> I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages \(so messaging came at the very beginning -- it took a while to see how to do messaging in a programming language efficiently enough to be useful\).  
> ...  
> OOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things. It can be done in Smalltalk and in LISP.

简单解释一下上面的这几句话的大概意思：OOP应该体现一种网状结构，这个结构上的每个节点“Object”只能通过“消息”和其他节点通讯。每个节点会有内部隐藏的状态，状态不可以被直接修改，而应该通过消息传递的方式来间接的修改。

这个编程思想被设计能够编写庞大复杂的系统。

那么为什么OOP能够支撑庞大复杂的系统呢？用开公司举个例子。如果公司就只有几个人，那么大家总是一起干活，工作可以通过“上帝视角“完全搞清楚每一个细节，于是可以制定非常清晰的、明确的流程来完成这个任务。这个思想接近于传统的面向过程编程。而如果公司人数变多，达到几百上千，这种“上帝视角”是完全不可行的。在这样复杂的公司里，没有一个人能搞清楚一个工作的所有细节。为此，公司要分很多个部门，每个部门相对的独立，有自己的章程，办事方法和规则等。独立性就意味着“隐藏内部状态”。比如你只能说申请让某部门按照章程办一件事，却不能说命令部门里的谁谁谁，在什么时候之前一定要办成。这些内部的细节你管不着。类似的，更高一层，公司之间也存在大量的协作关系。一个汽车供应链可能包括几千个企业，组成了一个商业网络。通过这种松散的协作关系维系的系统可以无限扩展下去，形成庞大的，复杂的系统。这就是OOP想表达的思想。

第一门OOP语言是Ole-Johan Dahland和Kristen Nygaard发明的Simula（比smalltalk还要早）。从名字就可以看出来，是用来支撑“模拟系统”的。模拟这个场景非常适合体现OOP的这个思想。这个语言引入了object、class、subclass、inheritance、动态绑定虚拟进程等概念，甚至还有GC。Java很大程度上受了Simula的影响。我们在现在教书上讲解OOP类、实例和继承关系时，总会给出比如动物-猫-狗，或者形状-圆-矩形的例子，都源自于此。

> 还有一些带有OO特征的语言或者研究成果在Simula之前就出现，这里就不往前追溯了。

但随后在施乐Palo Alto研究中心（Xerox PARC），Alan Kay、Dan Ingalls、Adele Goldberg在1970年开发了smalltalk，主要用于当时最前沿计算模型研究。在Simula的基础之上，smalltak特别强调messaging的重要性，成为了当时最有影响力的OOP语言。与smalltalk同期进行的还有比如GUI、超文本等项目。smalltalk也最早的实现了在GUI使用MVC模型来编程。

但是，并不是说OOP程序一定要用OOP语言来写。再强调一下，OOP首先是一种**设计思想**，非仅仅是编码方式。从这个角度推演，其实OOP最成功的例子其实是互联网。（Alan Kay也是互联网前身ARPNET的设计者之一）。另外一个OOP典型的例子是Linux内核，它充分体现了多个相对独立的组件（进程调度器、内存管理器、文件系统……）之间相互协作的思想。尽管Linux内核是用C写的，但是他比很多用所谓OOP语言写的程序更加OOP。

现在很多初学者会把使用C++，Java等语言的“OOP”语法特性后的程序称为OOP。比如封装、继承、多态等特性以及class、interface、private等管家你在会被大量提及和讨论。OOP语言不能代替人类做软件设计。既然做不了设计，就只能把一些轮子和语法糖造出来，供想编写OOP程序的人使用。但是，特别强调，**是OOP设计思想在前，OOP编码在后**。简单用OOP语言写代码，程序也不会自动变成OOP，也不一定能得到OOP的各种好处。

我们在以为我们在OOP时，其实很多时候都是在处理编码的细节工作，而非OOP提倡的“独立”，“通讯”。以“class”为例，实际上我们对它的用法有：

* 表达一个类型（和父子类关系），以对应真实世界的概念，一个类型可以起到一个“模版”的作用。这个类型形成的对象会严格维护内部的状态（或者叫不变量）
* 表达一个Object（即单例），比如XXXService这种“Bean”
* 表达一个名字空间，这样就可以把一组相关的代码写到一起而不是散播的到处都是，其实这是一个“module”
* 表达一个数据结构，比如DTO这种
* 因为代码复用，硬造出来的，无法与现实概念对应，但又不得不存在的类
* 提供便利，让foo\(a\)这种代码可以写成a.foo\(\)形式

其中前两种和OOP的设计思想有关，而其他都是编写具体代码的工具，有的是为了代码得到更好的组织，有的就是为了方便。

很多地方提及OOP=封装+继承+多态。我非常反对这个提法，因为这几个术语把原本很容易理解的，直观的做事方法变的图腾化。初学者往往会觉得他们听上去很牛逼，但是使用起来又经常和现实相冲突以至于落不了地。

“封装”，是想把一段逻辑/概念抽象出来做到“相对独立”。这并不是OOP发明的，而是长久以来一直被广泛采用的方法。比如电视机就是个“封装”的好例子，几个简单的操作按钮（接口）暴露出来供使用者操作，复杂的内部电路和元器件在机器里面隐藏。再比如，Linux的文件系统接口也是非常好的“封装”的例子，它提供了open，close，read，write和seek这几个简单的接口，却封装了大量的磁盘驱动，文件系统，buffer和cache，进程的阻塞和唤醒等复杂的细节。然而它是用函数做的“封装”。好的封装设计意味着简洁的接口和复杂的被隐藏的内部细节。这并非是一个private关键字就可以表达的。一个典型的反面的例子是从数据库里读取出来的数据，几乎所有的字段都是要被处理和使用的，还有新的字段可能在处理过程中被添加进来。这时用ORM搞出一个个实体class，弄一堆private成员再加一堆getter和setter是非常愚蠢的做法。这里的数据并非是具有相对独立性的，可以进行通讯的“Object“，而仅仅是“Data Structure”。因此我非常喜欢有些语言提供“data object”的支持。

> 当然，好的ORM会体现“Active Record”这种设计模式，非常有趣，本文不展开

再说说“继承”，是希望通过类型的 is-a 关系来实现代码的复用。绝大部分OOP语言会把is-a和代码复用这两件事情合作一件事。但是我们经常会发现这二者之间并不一定总能对上。有时我们觉得A is a B，但是A并不想要B的任何代码，仅仅想表达is-a关系而已；而有时，仅仅是想把A的一段代码给B用，但是A和B之间并没有什么语义关系。这个分歧会导致严重的设计问题。比如，做类的设计时往往会希望每个类能与现实当中的实体/概念对应上；但如果从代码复用角度出发设计类，就可能会得到很多现实并不存在，但不得不存在的类。一般这种类都会有奇怪的名字和非常玄幻的意思。如果开发者换了个人，可能很难把握原来设计的微妙的思路，但又不得不改，再稳妥保守一点就绕开重新设计，造成玄幻的类越来越多…… 继承造成的问题相当多。现在人们谈论“继承”，一般都会说“Composite Over Inheritance“。

多态和OOP也不是必然的关系。所谓多态，是指让一组Object表达同一概念，并展现不同的行为。入门级的OOP的书一般会这么举例子，比如有一个基类Animal，定义了run方法。然后其子类Cat，Dog，Cow等都可以override掉run，实现自己的逻辑，因为Cat，Dog，Cow等都是Animal。例子说得挺有道理。但现实的复杂性往往会要求实现一个不是Animal的子类也能“run”，比如汽车可以run，一个程序也可以“run”等。总之只要是run就可以，并不太在意其类型表达出的包含关系。这里想表达的意思是，如果想进行极致的“多态”，is-a与否就不那么重要了。在动态语言里，一般采用duck typing来实现这种“多态”——不关是什么东西，只要觉得他可以run，就给他写个叫“run”的函数即可；而对于静态语言，一般会设计一个“IRun”的接口，然后mixin到期望得到run能力的类上。简单来说，要实现多态可以不用继承、甚至不用class。

OOP一定好吗？显然是否定的。回到OOP的本心是要**处理大型复杂系统**的设计和实现。OOP的优势一定要到了根本就不可能有一个“上帝视角”的存在，不得不把系统拆成很多Object时才会体现出来。

举个例子，smalltalk中，1 + 2 的理解方式是：向“1”这个Object发送一给消息“+”，消息的参数是“2”。的确是非常存粹的OOP思想。但是放在工程上，1 + 2理解为一般人常见的表达式可能更容易理解。对于1 + 2这样简单的逻辑，人很容易从上帝视角出发得到最直接的理解，也就有了最简单直接的代码而无用考虑“Object”。

如果是那种“第一步”、“第二步“……的程序，面向数据的程序，极致为性能做优化的程序，是不应该用OOP去实现的。但很无奈如果某些“纯OOP语言”，就不得不造一些本来就不需要的class，再绕回到这个领域适合的编码模式上。比如普通的Web系统就是典型的“面向”数据库这个中心进行数据处理（处理完了展示给用户，或者响应用户的操作）。这个用FP的思路去理解更加简单，直观。也有MVC，MVVM这样的模式被广泛应用。

还有一些领域尽管用OOP最为基础很适合，但是根据场景，已经诞生出了“领域化的OOP”，比如GUI是一个典型的例子。GUI里用OOP也是比较适合的，但是GUI里有很多细节OOP不管或者处理不好，因此好的GUI库会在OOP基础之上扩展很多。早期的MFC，.Net GUI Framework, React等都是这样。另外一个领域是游戏，用OOP也很合适，但也是有些性能和领域细节需要特殊处理，因此ECS会得到广泛的采用。

总结一下，OOP是众多设计思想中的一种。很多OOP语言把这种思想的不重要的细节工具化，但直接无脑应用这些工具不会直接得到OOP的设计。即便是OOP思想本身也有其适合的场景和不适合的场景。即便是适合的场景，也可能针对这个场景在OOP之上做更针对这个场景需求的定制的架构/框架。如果简单把OOP作为某种教条就大大的违反了这个思想的初衷，也只能得到拧巴的代码。

## 函数式编程（Functional Programming）相比面向对象编程（Object-oriented Programming）有哪些优缺点？

函数式编程（FP）和面向对象编程（OOP）直接相比是一种常见的误解。这种比较应该是FP和OOP支持者之间互相怼的产物。

事实上，FP和OOP是两种不同的看待事物的方式。

FP强调“everything is lambda"，并且强调在逻辑处理中不变性的重要性。不变到什么地步呢？原教旨主义的FP就连普通的循环都不可以写（因为循环都有个变化的idx或者条件之类的变量），必须用递归实现。这样做的结果就是把一切“状态”都消除。任何“状态”都是由确定的输入经过确定的一组函数处理得到的最终结果。

OOP强调“everything is object”，以及object之间的消息传递。通过消息传递改变每个Object的内部状态。OOP之父Alan Kay表示"OOP is all about messaging"。利用OOP建模，都会通过某种消息机制来模拟一些场景的处理。比如

```text
交易=下单Object，支付Object，积分Object等之间进行交互
```

> 当然，实际的OOP的程序运行时为了效率一般会用方法调用，而不是真的传递一个物理消息。

如果你看懂了上面两个概念，就会发现他们说的事情压根就不在一个频道里。因此各自的好处也不能证明另一方有缺点。

现在经常看到文章表达FP如何如何优于OOP，大概原因有这么两点：

第一点是，OOP早期不切实际的吹牛皮，吹爆了。很多人谈起OOP，都会有“用了OOP，代码耦合就小了，就容易维护了，扩展就方便了，代码就更容易复用了等等“的第一印象。但实际上这并不一定发生。软件设计并非因为OOP就直接自动变好了。因此很多程序员在趟坑多年后可能会感觉“我擦，学了这么多年，全是假的“。更进一步的，像Java这样的“纯OOP”语言迫使程序员并不需要OOP的情况下也得照着OOP的方式去写代码，结果啰嗦又臃肿。所以很多人越来越讨厌OOP其实是可以理解的。（比如这篇[Goodbye, Objected Oriented Programming](https://link.zhihu.com/?target=https%3A//medium.com/%40cscalfani/goodbye-object-oriented-programming-a59cda4c0e53)\)。

> 现在的Java程序大量使用反射、lambda等技术，已经不是早期那个单纯OOP语言了。

第二点是现代程序开始往并发发展。而FP的不可变，没有副作用等特性恰好让并发编程变得不容易出错。并且配合多种并发模型（如CSP、Map Reduce、Fork & Join、Promise等），可以解决很多高并发的问题，显得高、大、上、酷。

但是，我非常赞同《人月神话》的著名论断——没有银弹。不论OOP还是FP，用好了都可以发挥作用，用不好一样吃瘪。举几个例子，

一个业务领域建模，其实模拟的就是现实当中的不同角色的人/机构的工作方式。因为如果是人/机构互相协作，就是通过消息来协作的。比如博士生想发文章，先得自己写，写了老板审阅，完事发给期刊编辑，编辑找同行评议，完事发表，发表的结果会收录到某个文献索引数据库。这个过程就是多个独立的“对象”在相互协作的结果。因此OOP在这个层面上对这个流程进行抽象是很合适的。当然你也可以说，这时我用FP的各种动作函数的组织来描述这个过程，也是可以的。但是如果比较一下，这个场景用FP和OOP建模，哪个更容易理解呢？

再比如，对一组数据做加工，先查询，然后聚合，聚合后排序，再join，再排序，再聚合，再转换（map）得到最终的结果。这个过程，用FP的函数就很自然，因为这一看就是

```text
result = func1(func2(func3...funcN(x))))
```

这时用OOP呢？给每一个步骤建一个class？然后把排序、聚合等操作放在class里？抽象个基类？或者弄个XXXUtils的静态方法集合类？当然都可以做，但是很明显这不是个好的设计。

再再比如，一个业务流程，就是一组步骤：第一步如何如何，第二部如何如何……。这时用FP和OOP都不能很好的表达问题（可能FP接近点）。这其实是典型的“指令式编程“。如果业务逻辑如此，那么就照着一步一步做就是最好的，而不是抽取函数和不变状态；或者定义一些根本无意义的class。

说了这么多，其实希望表达的意思是：**到底用哪种编程模式，要看问题本身适合哪个。**哪个用起来自然，和问题本身特质搭配，那就用哪个。用对了，事半功倍；用错了，就各种纠结拧巴。你希望你一个东西模拟为Object，前提是这个东西本身容易抽象成一个Object；你希望你一个数据可以抽象为一组函数执行的组合，前提是这样理解更自然，更舒服。

此外，同一个问题可以拆解为不同的层次，不同的层次可以使用各自适合的方式。比如高层的可以OOP，具体到某个执行逻辑里可以用FP或者指令编程。

---

P.S. 关于OOP

很多教材、资料、文章等会说OOP=封装 + 继承 + 多态。其实这个提法非常的误导人。封装是普遍存在的概念，函数也可以封装。而多态也不限于OOP，接口可以多态，用duck typing也可以多态。只有继承专属于OOP，但是继承只是OOP里一个次要一级的代码复用的特性（而且目前还普遍被认为不靠谱，composite over inheritance）。OOP最关键的特征还是通过消息传递来改变Object内部的状态。

我的另外一个回答非常详细的解释了OOP到底是什么。

## 为什么数据库和数据库连接池不采用类似java nio的IO多路复用技术使用一个连接来维护和数据库的数据交换？

这是一个非常好的问题。IO多路复用被视为是非常好的性能助力器。但是一般我们在使用DB时，还是经常性采用c3p0，tomcat connection pool等技术来与DB连接，哪怕整个程序已经变成以Netty为核心。这到底是为什么？

首先纠正一个常见的误解。IO多路复用听上去好像是多个数据可以共享一个IO（socket连接），实际上并非如此。**IO多路复用不是指多个服务共享一个连接，而仅仅是指多个连接的管理可以在同一进程**。在网络服务中，IO多路复用起的作用是**一次性把多个连接的事件通知业务代码处理**。至于这些事件的处理方式，到底是业务代码循环着处理、丢到队列里，还是交给线程池处理，由业务代码决定。

对于使用DB的程序来讲，不管使用多路复用，还是连接池，都要维护一组网络连接，支持并发的查询。

为什么并发查询一定要使用多个连接才能完成呢？因为DB一般是使用连接作为Session管理的基本单元。在一个连接中，SQL语句的执行必须是串行、同步的。这是由于对于每一个Session，DB都要维护一组状态来支持查询，比如事务隔离级别，当前Session的变量等。只有单Session内串行执行，才能维护查询的正确性（试想一下一组sql在不断的增减变量，然后这组sql乱序执行会发生什么）。维护这些状态需要耗费内存，同时也会消耗CPU和磁盘IO。这样，限制对DB的连接数，就是在限制对DB资源的消耗。

因此，对DB来说，关键是要限制连接的数目。这个要求无论是DB连接池还是NIO的连接管理都能做到。

这样问题就绕回来了，为什么DB连接不能放到IO多路复用里一并执行吗？为啥大家都用连接池？

答案是，可以用IO多路复用——但是**使用JDBC不行**。JDBC是一个出现了近20年的标准，它的设计核心是BIO（因为199X年时还没有别的IO可以用）：调用者在通过JDBC时执行比如query这样的API，在没有执行完成之前，整个调用线程被卡住。而类似于Mysql Connector/J这样的driver完备的实现了这套语义。

当然如果DB Client的协议的连接处理和解析稍微改一下：

1. 将IO模式调整为Non-Blocking，这样就可以挂到IO多路复用的内核上（select、epoll、kqueue……）
2. 在Non-Blocking实现的基础之上实现数据库协议的编码和解析

就可以实现用IO多路复用来访问DB。实际上很多其他语言/框架里都是这么干的。比如Nodejs，see [https://github.com/sidorares/node-mysql2](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttps%253A%252F%252Fgithub.com%252Fsidorares%252Fnode-mysql2)；或者Vert.X 的db客户端（[https://github.com/mauricio/postgresql-async](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttps%253A%252F%252Fgithub.com%252Fmauricio%252Fpostgresql-async)，不要在意这个名字，它实际上同时支持mysql和postgres）。只不过对于IO多路复用，数据库官方似乎都没做这种支持——他们只支持JDBC、ODBC等等这些标准协议。

那么为什么基于IO多路复用的实现不能成为默认的，官方的，而要成为偏门呢？

对于数据库开发者来说。这种用法在整体的用户里占有量非常小，所以也许不值当的花大力气。只需要把协议写清楚（比如[https://dev.mysql.com/doc/internals/en/client-server-protocol.html](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttps%253A%252F%252Fdev.mysql.com%252Fdoc%252Finternals%252Fen%252Fclient-server-protocol.html)），就可以做实现。那么社区的有兴趣的人自然就可以去做。

另外一个原因是体系的支持。简单来讲，如果没有一个大的Reactive的运行环境，IO多路复用的使用会非常受限。

IO多路复用之所以能成立，是需要**整个程序要有一个IO多路复用的驱动代码**——就是select的那句调用都。整个程序必须以这个驱动代码为核心。这样就对整个代码的结构产生重大的影响。这种影响是没法用简单的接口抽象的。

Java Web容器之所以可以使用NIO是因为NIO可以被封装到容器内部。Web容器对外暴露的还是传统的多线程形式的Java EE接口。

如果DB和Web容器同时使用NIO，那么调用的DB连接库与必须与容器有一个约定描述**DB的连接管理如何接入Web容器的NIO的驱动代码**。在Java这个大环境下，不同人，不同的容器写的代码不同；又或者，不使用任何常见的容器，而是自己用NIO去封装一个。这样是无法形成代码上的约定的。那么多个独立的组件就不能很好的共享NIO的驱动代码。

上面这个用法假设整个程序应该共享一个NIO驱动代码。那么Web和DB可不可以各用各的呢？也是可以的，但是为了保证这两个NIO驱动代码不会相互block，最好要分开两个线程。这样一来就会打破一般Web服务一个请求处理用一个线程的一般做法，会让程序边的更复杂——你的业务代码和DB查询之间必须做跨线程数据交换。

相反，连接池的实现就相对独立的多，也简单的多。外界只要配好DB URL，用户名密码和连接池的容量参数，就可以做到自行管理连接。

而Nodejs和Vert.X是完全不同的。他们本质就是Reactive的。他们的NIO的驱动方式是其运行时的基础——所有要在这个基础上开发的代码都必须遵守同样的NIO+异步开发规范，使用同一个NIO的驱动。这样DB与NIO的协作就不成问题了。

最后，**有大量场景是需要BIO的DB查询支持的**。批处理数据分析代码都是这样的场景。这样的程序写成NIO就会得不偿失——代码不容易懂，也没有任何效率上的优势。类似于Nodejs这样的运行时在此场景下，反而要利用`async`或等价的语法来让代码看起来是同步的，这样才容易写。

总结一下。DB访问一般采用连接池这种现象是生态造成的。历史上的BIO+连接池的做法经过多年的发展，已经解决了主要的问题。在Java的大环境下，这个方案是非常靠谱的，成熟的。而基于IO多路复用的方式尽管在性能上可能有优势，但是其对整个程序的代码结构要求过多，过于复杂。当然，如果有特定的需要，希望使用IO多路复用管理DB连接，是完全可行的。

## 分布式事务中的最终一致具体应该如何实现？

我帮这个问题梳理一下逻辑。

分布式系统复杂业务的确需要“分布式事务”。2PC（XA），3PC是一路，因为它们实现了“Atomic Commit”。这里“Atomic”的意思是就是这个事情要不就都做了，要不就都没做，不存在做了一半的情况。生产中主要使用2PC（XA）。但这个方案有个严重的问题，只要事务管理器或者实际执行者有任何一方不工作，都可能会造成整体不工作，甚至是直接hang住了。此外这套需要XA协议的支持，并不是所有资源都支持XA。一些商业数据库和队列支持XA。而自己编写的服务接口没法用XA。此外常规的2PC实现的性能表现很差。

另外一个路数就是用TCC或者SAGA。TCC就是做2次操作：冻结 + 提交/取消。而SAGA是：执行 + 可能的回滚。这两种方式都不满足“Atomic Commit”。从数据库的角度来看，他们是多个事务，而不是一个整体。期间用户可能会有感觉。对于TCC，用户会有很短的时间发现自己的数据被冻住了。而SAGA下，用户会先发现某个业务被执行了，随后又回滚了（比如订单先下了，然后又自己撤销了）。他们都需要产品设计的配合，让这个事情显得不是很扎眼。但他们俩就是我们常规说的“最终一致”。

无论是2PC，TCC，还是SAGA，如果中间某个节在事务执行期间点挂了，恢复就会成问题。2PC在这一点上尤其严重。此外，正如题目中所说，如果一个服务挂了，那么就会连带着让上一层服务也不可用。解决的办法就是引入多个backup。在一个服务挂了之后就让其中某个backup顶上。如果服务本身无状态还相对容易，如果有状态就会很麻烦。因为必须保证所有backup的状态必须精确的和原来的服务一摸一样。保持一摸一样，就得让backup成为master的replica。

如果replica不能保证和master如完全一致，就不能解决上面的问题——实现一个带Fault Tolerant的分布式事务。为了解决这个问题就需要引入consensus算法。常规的做法是利用consensus算法为一个服务的多个实例作“leader”的选举，并以此为基础实现“state machine replication”。Consensus算法保证同一时刻只能有一个leader，如果leader挂了会重新选举，自动恢复系统 。这就避免了上面2PC难以恢复，需要人工介入的问题。

> 人工介入就意味着时间长+恢复时容易出错。谁运营谁知道。

而consensus算法本身需要实现“total order broadcast”。现实当中常用paxos（chubby等），zab （zookeeper）和raft（etcd等）实现。这个问题是分布式系统中最核心的问题。一些分布式存储，如mysql的group replication，tidb都采用这个思路实现。

但正确实现一个具有分布式一致性且Fault Tolerant的系统实在是太过于困难，中小公司一般也只能凑合。考虑到技术储备和资源怎么没法”放心“，但现实需求怼Fault Tolerant和Performance又是必须的。于是一般的做法就是凑合实现一个的分布式事务，再配合”人工对账“的形式来彻底封死错误。即使是有像TiDB这样的已经实现了分布式事务的数据库引入，谁又能担保100%不出问题呢？终归是double check下才能放心。换一个角度，如果凑合实现的分布式事务不那么靠谱，但成本极低。对账如果发现问题，就赔款。赔的钱远小过实现完备分布式一致性的成本，从业务角度也是蛮划算的（但要测算一下大概会赔多少，再做决策）。

如果是跨公司（如一个电商公司和一个支付渠道公司）的协作，技术对接都很困难，对账这种看起来有点糙但管用的办法就更加必不可少了。

## Java中线程同步锁和互斥锁有啥区别？

不要钻概念牛角尖。这样没意义。

也许java语法层面包装成了sycnchronized或者明确的XXXLock，但是道理都是一样的。无非就是哪种写起来方便。

锁的目的就是避免多个线程对同一个共享的数据并发修改带来的数据混乱。

锁的实现要处理的大概就只有这4个问题：

* “**谁拿到了锁“这个信息存哪里**（可以是当前class，当前instance的markword，还可以是某个具体的Lock的实例）
* **谁能抢到锁的规则**（只能一个人抢到 - Mutex；能抢有限多个数量 - Semaphore；自己可以反复抢 - 重入锁；读可以反复抢到但是写独占 - 读写锁……）
* **抢不到时怎么办**（抢不到玩命抢；抢不到暂时睡着，等一段时间再试/等通知再试；或者二者的结合，先玩命抢几次，还没抢到就睡着）
* **如果锁被释放了还有其他等待锁的怎么办**（不管，让等的线程通过超时机制自己抢；按照一定规则通知某一个等待的线程；通知所有线程唤醒他们，让他们一起抢……）

有了这些选择，你就可以按照业务需求组装出你需要锁。

———— 更新一下————

关于“互斥”和“同步”的概念[@chen Kingwen](//www.zhihu.com/people/ae923638b12a18db2952e9200f8f2493) 的答案很清楚了。

* 互斥就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了
* 同步就是ABCD这些线程要约定一个执行的协调顺序。比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。

这是两种典型的并发问题。恰当的使用锁，可以解决同步或者互斥的问题。

你可以说Mutex是专门被设计来解决互斥的；Barrier，Semaphore是专门来解决同步的。但是这些都离不开上述对上述4个问题的处理。同时，如果遇到了其他的具体的并发问题，你也可以定制一个锁来满足需要。

## 动态规划和递归之间的关系是什么？

DP说白了就是一个解决问题的思路——即一个大一点规模的问题可以被拆解为更小的，更容易解决的问题。

拿最简单的斐波那契问题举例子，一个大的问题f\(n\)可以被拆解为小一点的问题f\(n-1\)和f\(n-2\)，……直到然后拆到最小的问题f\(1\)和f\(2\)。你可以从f\(n\)从大往小了算，也可以先从f\(1\), f\(2\), f\(3\)……往大了算。再比如leetcode的有个正则表达式匹配的问题，你可以把问题看做是一个大的字符串的匹配pattern，拆解为字符串的一部分匹配pattern的一部分的问题；也可以反过来先匹配一小部分，再不断让可以匹配的范围变大。

很多人把从大往小算的形式称作递归，反过来从小往大了算称为DP。但实际上只要满足大规模问题可以拆解为小规模问题，这个思路本身就是DP的，无非是顺序不一样罢了。

所谓【动态规划】就是指知道了一组小规模问题的答案后，就可以用一个方案（状态转移方程）组装成大一点规模问题的答案的做法而已。为啥叫“动态”呢，因为状态转移会和几个条件相关，而不是一开始就可以无脑写死（无脑写死的一般就是贪婪）。

术语上，从大了往小算被称为”自顶向下“，而从小往大算被称为”自底向上“。尽管自顶向下和自底向上等价，但自顶向下算一个很容易发生的问题就是重复计算，比如你为了算f\(5\)，普通的递归方式要重复计算很多很多次f\(3\), f\(2\)……。这些计算都是浪费的，实际表现比自底向上差的多（但再强调下，二者的思路没本质差别，仅仅是计算浪费不浪费的问题）。但你可以加cache，每次递归的函数的参数都可以组装成一个cache的key。这样每次递归时，可以先检查一下cache是不是有，有了就不用算了，直接返回。这种带cache的递归DP一般被称为“记忆化搜索“。记忆化搜索与自顶向上的DP计算方式在算法复杂度上理论上是一样的。但cache一般会用map，其实际的复杂度要比直接用数组的自顶向上要大，二者的O\(1\)是不一样的。此外记忆化搜索还会引入函数调用的开销，所以一般记忆化搜索比等价的自底向上要慢那么一丢丢。

此外自底向上的计算方式还可以优化存储，比如斐波那契计算时，计算f\(n\)只需要f\(n-1\), f\(n-2\)，所以用两个变量就行，无须把所有的结果都记录下来。用go写大概这样：

```text
func fib(n int) int {
  f0, f1 := 0, 1
  for n > 1 {
    f1, f0 = f0 + f1, f1
    n--
  }
  return f1
}
```

但用记忆化搜索的方式，你是没法在计算得到f\(4\)后，把f\(1\)结果占用内存给方便的清理掉的。

DP最难的不在于自底向上还是自顶向下，而在于怎么拆问题。某些问题，如果之前没做过，你是很难想得到“这个问题还能这么拆”。不信？可以去看看LeetCode有一道“戳气球”的题目。我自己第一次看时虽然能猜到它是用DP，但打死都想不出来怎么拆，直到看了题解。因此大家想不出来DP，不是因为不知道DP的代码框架怎么做，而是拆解问题的思路实在太古怪，太反常识。这就需要刷题去多多适应。

> But，这些反常识的方法也是人想出来的。此时也是去了解那些聪明人的脑洞的好机会：）。你也许会惊讶于人思维方式的差异的巨大。

在现实工程中，有些问题即便是可以DP，但因为拆解方案对于大部分人来说太难想；或者即使想出来，应用面太窄，需求小小的一个变化就会让整套方案不灵了。因此很多时候，如果复杂度的要求不那么严苛，还是用暴力要好一些。因此DP更常见于面试题里，证明候选人“知道像计算机专业从业者那样思考”。但这并不是说DP不重要，在计算机科学里，这个思路非常非常重要，只不过工程上不光要考虑算法复杂度，还要考虑可维护性问题而已。如果场景恰当，当然还是要用。比方说，我们之前做的“计算基金最大回撤”就用DP，实际上就是LeetCode里“买卖股票最佳时机”解题方案的镜像——就是一个算回撤，一个算正向收益的区别。

回到DP本身，题主不妨试试这样做。既然题主的思维方式觉得自顶向下很舒服，那就总是先用递归实现自顶向下DP。通过后加上Cache用记忆化搜索的方式写代码。再改写成等价的自顶向上的做法。反复做几次看看思路上能不能贯通。但同时，我也遇到过写自底向上很舒服，反过来就别扭的人。

当然，虽然二者等价，我依然遇到过很难自底向上写的问题。此时一般是很难找到一个很简单的从小到大的规律，这就不如用递归+cache。cache的规则就很简单，算过了就cache，没算过就没cache，递归调用中参数出现的规律不需要操心。

同时，也有反过来的情况，自底向上很难写。主要是这类问题的递归方式相对好理解，但自底向上时就要让dp\[i\]\[j\]表达一些比较模糊，很难说清楚的概念，让编码变得很别扭。所以遇到这种情况就用容易理解的形式写就好。

最后，递归能够在DP这个思路里支持实现自顶向下，但也不一定就用在DP里。递归适合的场景要更加广泛。比如dfs/bfs这样的搜索场景就可以用递归做。递归还可以用来做模拟，比如A对B做一件事、B又要对C和D做一件事，D又要对A和B做一件事……。用递归来实现很直观。

## IO多路复用和线程池哪个效率更高，更有优势？

问题本身貌似有问题。简单比较两种工具的优劣意义不大。你没法说锤子和剪刀那个更好。我们一般会评价对于某个场景，哪种工具更合适。

io多路复用（这翻译真的很坑爹啊），指的是同一个进（线）程可以处理多个IO数据流。

多线程+池模型指的是每个线程处理一个IO流。

IO多路复用的优势在于，当处理的消耗对比IO几乎可以忽略不计时，可以处理大量的并发IO，而不用消耗太多CPU/内存。这就像是一个工作很高效的人，手上一个todo list，他高效的依次处理每个任务。这比每个任务单独安排一个人要节省（雇人是要发工资的……）。典型的例子是nginx做代理，代理的转发逻辑相对比较简单直接，那么IO多路复用很适合。相反，如果是一个做复杂计算的场景，计算本身可能是个 指数复杂度的东西，IO不是瓶颈。那么怎么充分利用CPU或者显卡的核心多干活才是关键。

此外，IO多路复用适合处理很多闲置的IO，因为IO socket的数量的增加并不会带来进（线）程数的增加，也就不会带来stack内存，内核对象，切换时间的损耗。因此像长链接做通知的场景非常适合。

IO多路复用 + 单进（线）程有个额外的好处，就不会有并发编程的各种坑问题，比如在nginx里，redis里，编程实现都会很简单很多。编程中处理并发冲突和一致性，原子性问题真的是很难，极易出错。

> 但是现实中，也有IO多路复用 + 多worker线程的做法，这样上面这个好处就没有了。

如果做不到“处理过程相对于IO可以忽略不计”，IO多路复用的并不一定比线程池方案更好。比如一个web的服务，用jetty 9的NIO connector，后边是spring svc + JDBC连接数据库。spring svc + JDBC连接数据库这两块的处理延迟相对于NIO来说不能忽略，所以并不能指望用jetty 9的NIO connector换了之前的BIO connector的容器，性能能高不少（实际上应该会高一些，但不会太夸张，毕竟瓶颈在后边处理和DB上）。

顺便提一句，Java世界里，因为JDBC这个东西是BIO的，所以在我们常见的Java服务里没有办法做到全部NIO化，必须得弄成多线程模型。如果要在做Java web服务这个大场景下享受IO多路复用的好处，要不就是不需要DB的，要不就是得用Vert.X一类的纯NIO框架把DB IO访问也得框进来。

最后，如果IO压力过大，一个高并发的东西和一个不那么高并发的东西，都不能正确响应，对用户来说是一样的——**就是不能用**。假如IO非常的繁重，没有空闲的连接，那么IO的压力在两种模型下表现差不多，IO多路复用的“并发“看了起来会大一些，但因为IO已经满了，所以表现出超时严重；而线程池可能表现为，所有的线程都因为IO过慢而卡死了，线程池耗尽，新的请求进不来直接报错。但不管哪一种，在极端压力下，都无法正常工作。这时，要想着怎么扩容。

简单总结一下，

* 如果压力不是很大，并且处理性能相对于IO可以忽略不计
  * IO多路复用+单进（线）程比较省资源
  * 适合处理大量的闲置的IO
  * IO多路复用+多单进（线）程与线程池方案相比有好处，但是并不会有太大的优势
* 如果压力很大，什么方案都得跪，这时就得扩容。当然因为IO多路复用+单进（线）程比较省资源，所以扩容时能省钱。

## 为什么一般操作系统中应用程序的栈空间都要设最大值，不支持动态扩展？

看下面的图：

![](https://pic1.zhimg.com/50/v2-957f4c83e14d294b9285b54ff55a02fb_hd.jpg?source=1940ef5c)

题主之所以能问出这个问题，也许是因为大多数内存的理论模型都会画成左边那样子。那么自然而然的Heap可以不断往上扩展；Stack可以往下扩展。

但现实中是  
1. 主流操作系统都是有多线程支持的，而多线程需要每个线程分配一个独立的Stack，每个Stack内部可以满足“向下增长“，但是必须要有个界限，不然没法实现了。否则下个Stack从哪开始呢？

2. heap和mmap segment的存在。mmap是有很多用途的。比如

* 加载一个so动态库，是以mmap的形式映射到内存里，再让CPU执行上面的指令的；每个程序都会加载大量的动态库。
* 我们编程意义上的heap实际上是操作系统级别heap和mmap区域的混合。当分配一大块内存时，操作系统可能会决定不从heap里切，而是独立分配一块mmap区域来用。
* mmap自己也可以被用户直接使用，比如映射一个文件，或者做内存的数据共享。

上图中之所以把heap，mmap segement和stack画成一个颜色，是因为他们本质上差不太多，都是程序运行时动态分配和调整的。stack和mmap都是“一块内存”，因此实现中并不一定非得是Stack永远比mmap的地址数值更大。只要不重叠就行了。Linux本身的api也允许创建新线程时，指定一段内存作为“Stack”，而这个内存自然也需要通过malloc得到，这又回到了mmap/heap上了。

回到应用层面，巨大的Stack除了应付非常深的递归之外没有什么太大的用处。而非常深的递归一般就是程序哪里写bug了。为了不太有用的场景去做设计并不是理智的行为。

我们常规意义上说“Stack”实际只是在用CPU对一段内存的地址做指令寄存器的push和pop而已。如果这不够用你可以自行定制一个喜欢的形式来实现对Stack的管理。有些语言可以把Stack的管理玩出花。比如go，自己实现了对go routine的管理，每个routine的Stack都可以不是连续的，这样既避免浪费，又能轻松扩展。

## 各位都是怎么进行单元测试的？

首先我先梳理下概念。因为比如像“单元测试”这个词在不同场景下可能有不同意思。我更偏向于将大家大致理解的“单元测试”称为“**开发阶段的自动化测试**“。这样就可以体现三个点：

* 一是，这个**测试是开发同学自己搞的**。开发人员搞定代码后，为新代码添加测试，并且保证新老测试都能通过，**才能提测**。开发同学不能将满是bug的代码就丢给测试同学。
* 二是，这个**测试是自动化的**，不是“手工调用接口做一次测试“那种形式的。并且执行起来要非常简单。比如任何人拿到代码，只要机器上装了对应的开发工具，都可以轻松运行测试，看到报告（比如对于java，一句mvn test, 或者对nodejs，一句npm test就能跑）。
* 三是，开发人员要做的测试，真的不一定是只验证一个“单元”的正确性（下面详细讲）

市面上大部分关于这种测试的介绍，大多集中在某个工具或者框架怎么用（比如JUnit，mockito，jest等）；要不就是一些理论上的对这种测试好处的介绍，通篇是“应该做测试”，“这么做了就容易得到高内聚的代码“等等。但是很少有关于**真正为什么这个事情难以落地的讨论**。这也是我很想多唠叨两句的地方。

最郁闷的是很多人一开始写测试代码就错了，因为压根就没搞明白要测试什么，保证什么东西是对的。这个无论什么工具都不帮不了你，只能由开发者自己思考——对于某个特定的测试代码，花那么大力气去写，去mock，去assert，到底想验证什么。一个不留神，测试代码就变成验证比如JPA的接口对不对，某个第三方系统库的函数对不对等等。这明显不是聪明的做法。

最常见的测试目标是是**验证“自己写的一小段代码是不是符合设计逻辑的“**。这个”一小段代码“就是我们经常说的“单元”。“单元测试”的本来意思也源自于此。比如，你写了个快排函数qsort，想验证下各种序列输入进去是不是就能得到排好序的结果。因为快排的逻辑稍微有一丢丢绕脑，觉得一遍写下来没有信心。于是就用N多的case去验证这个函数的输出是否总是符合预期。

这样的测试是最简单的，因为要测的东西几乎没有复杂的依赖。这也是大家经常说的“纯函数”好测的原因——函数的输入就是其所有可见的上下文了。对于一个纯函数，开发者者很容易构造其上下文。

很多文章介绍测试都只拿这种“纯函数”的测试举例子，因此大家看过之后就照猫画虎到实际项目写测试代码了，结果就是很难写出来，或者写出来起不到验证的作用，极度打击自信心，然后慢慢产生了“测试没啥用”或者“测试就应该只让测试工程师负责的“不正确的三观。

我们正常写的代码可能是业务逻辑，是数据分析的job代码，也有可能是前端构造界面的代码等。这种代码都会有依赖的。由于mock工具的存在，在做“单元测试”的时候，应该尽量保证除待验证的代码之外，所有的不纯的依赖都要尽量mock。

有些依赖特别麻烦。比如一个例子是，验证一个下单打折的函数是否能计算出正确的折扣。这个函数需要读取数据库里的折扣数据才能做计算。这就引入一个依赖。**而对依赖的处理方式的拿捏才是测试里最难的地方。**

比如，你可以直接mock掉整个DiscountDAO。这样就可以避免真的读取数据库。这时，你的假设是“**你觉得读取折扣的SQL本身是肯定没错的，因此你不验证它，你只验证读到折扣数据后，根据输入金额得到折扣金额的逻辑是正确的**“。

你也可以用embedded数据库, h2，maria4j之类的mock数据库。但如果你生产用的是mysql/postgres等，那么你在假设“对于这一段要验证的SQL，用embedded db和用生产db是等价的“。这有可能成立，有可能不成立。但也许代码里用到了私有SQL语法，只有生产数据库支持。如果发生，这样的验证就做不了。

你还可以用真的生产DB做验证。但首先，这里测试的目标已经变了，从**“验证计算折扣的逻辑”**变成“**验证计算折扣这个功能是否正确**“。这是已经不是“单元”了，而是一个函数 + 一句SQL执行 + DB功能正确的**“集成”测试**当然，这个例子里是个很局部的集成。不过也是几个组件一起测试才能实现这个验证。这样的测试代价是必须部署一个真的数据库，还要准备数据和后门。编写代价相对就比较高了。

开发也可能做（局部）“集成测试”，所以这就是另外一个我觉得应该叫“开发阶段自动化测试“，而不仅仅是“单元测试”的原因。“单元测试“容易让人引起一个误解——开发人员只应该管一小段代码是否正确，却不用管任何集成测试。这明显是**不一定**正确的。比起“单元测试”这个概念本身，我更在意的是**开发人员为了保证代码质量应该怎样做才对**。

那么到底要不要做这种“局部集成测试呢“。这要看情况。比如：

* 你的目标就是要测“一小段代码是不是正确”。你可以很有信心的保证其他依赖的正确性都能保证。那明显，这时就不用花精力做集成了。怎么简单怎么来。这样的测试甚至都不需要启动Spring这类框架，运行速度会很快。
* 你在开发一个小的lib。这个lib就很纯，没有任何复杂的依赖，那么单元测试就足够了。
* 如果团队已经安排了专人做这块的集成测试，开发人员就没必要做重复劳动了。如果这块测试的不好，应该优先去和那个测试同学沟通，看看怎么改善。沟通无效，在manager知情和同意下，再自己补。
* 如果开发自己做集成可以更容易构造全集成测试不太容易构造的例子，那么还是自己集成测试一下比较好。性价比高。
* 如果是要测试一个端到端的接口返回正确，那么唯一的办法就是集成测试——真的启动server，使用真的数据库、Redis、队列……，做端到端的测试。这时也许docker可以帮助你一键启动全套环境。
* 如果一个测试涉及到依赖的核心功能，也必须得做集成测试。比如要测试一个Exception是不是会让当前事务真的回滚，同时发生的其他事务因为隔离级别不会受到影响，那么你必须引入真的，和生产一模一样的支持事务的数据库才行。
* 如果是前端测试，基本上也必须得做集成测试。就算可以mock掉所有的后端接口，也得引入浏览器或者App框架才能测试。
* 如果整个公司没有专门的测试岗位，都是“研发”，那么不管局部集成还是全部集成，都得由开发者自己上。
* ……

所以开发的同学在写测试的之前，最好先再三确认自己到底要验证什么。至于用什么工具，如何看待覆盖率，如何设计模块和函数，以及后门，让程序“容易测”都是在目标之下的要讨论的具体问题。

还有个问题是什么时候应该写测试。是不是项目一开始就得同时写测试，甚至是TDD？

Well，这也是个Case by Case的问题。单元测试也好，局部集成测试也好，都是有代价的。我的经验是，有效的开发阶段测试会占用掉开发者30%左右的精力。当然，追求技术极致的人会觉得写测试天经地义，不写才是垃圾。30%的精力也是开发的一部分，不带讨价还价的。如果覆盖率不到90%+就是不称职等等。

我觉得不应该走极端，应该同时看到测试的成本和收益。比如在创业公司早期，就1～2个人的时候，让业务赶紧上线试错找对方向是最重要的。今天的设计和代码说不定几周后就大变样，甚至就直接丢掉了。此时功能简陋，用户量也少，小问题是可以忍的。此时团队会通过手工测试和实际使用来不断的发现和修复问题（所以这时候运行时错误log抓取 + 报警一定要做好啊）。这时强调必须完美覆盖的测试是不合时宜的。这个阶段，应该着重全集成测试（通常由一个专门的测试工程师主持），以及把那些局部的“一旦出问题公司就完蛋”的逻辑加好自动化测试。

写一个新东西，有的时候一开始想不清楚设计，类、接口、模块的划分可能都不是很完美，这时写一大堆测试基本上是会废掉的。随着不断的迭代，代码的组织才能慢慢优化和清晰，这些稳固的代码产生的接口、模型才有被测试保护的意义。

但是反过来，如果一开始图快，不写测试，随着业务慢慢稳定，团队人数和用户量慢慢多起来后，习惯性一直不写测试也是不对的。这时对代码的修改会陷入巨大的痛苦和不安中。天晓得改了一处会不会引发其他不相干代码的问题。这时，任何被心灵摧残的开发也会主动想着怎么提高代码质量的。

因此，需要找一个时间点开始，一点一点的把最重要的、稳固的逻辑的测试补充上。我的经验是一个项目代码在不断迭代大概10个月后，或者团队技术人员已经超过5个人，或者活跃用户已经超过1万人，就必须有测试保护。如果用动态语言（js，python等）编写程序，因为没有编译器的保护，就更要写测试。

但是最后要特别强调下，搭建有效的自动化测试的工作量并不小。一提起测试，很多人就只想到TestCase。但实际上为了让TestCase发挥作用，让测试容易运行，可以隔离，可重复，稳定的跑，需要学习和实施的东西非常多，甚至不亚于功能开发本身。对测试工作量的低估最终会导致让这个工作不了了之，变成面子工程。如果你是一个技术lead，想带领团队写测试，但以为“就是很小的工作量，随便写一下就会有效果”，那么必然会吃瘪。

最后，我个人对【开发阶段自动化测试】的要求总结下来是：

* 确定测试的目标，到底想验证什么
* 基于这个目标，找到和维护需要的工具，比如Runner，Mock，覆盖率统计工具等，Embedded数据库等
* 留足给测试的时间，并通过code review的手段来保证写有效的测试
* 给一些典型的场景如何做测试写一写文档，积累经验（比如如何测试要模拟时间的案例？）
* 统计测试同学给开发同学报bug的数据，盯紧代码质量不高的同学，多做沟通
* 根据出现bug的数量和scope来推动部分关键代码的测试质量的改善
* 在能达成测试目标的前提下，看看能够整合一些工具，降低维护测试依赖的成本

以及我希望大家写的时候有一个预期：

**【开发阶段自动化测试】是所有测试体系里的一个部分，而非全部，不能解决所有问题**。**不要觉得写自动化测试的Case，让覆盖率100%就能保证系统绝对不出错。这是妄想，死了这条心。**

P.S.

也许是因为我所处的环境的缘故，我不支持TDD。因为在我处的领域，基本上不太可能出现一开始就能把设计做得很完备的情况。一个功能迭代3期就会面目全非了。

此外TDD会导致一个趋向——面向功能开发，而非面向抽象开发。一个系统的核心是对其需求进行建模后的抽象。抽象变了，你就不能指望之前写好的接口会稳定。从系统的维护角度，长期看，要保持抽象迭代，总是能顺畅的反应业务模型最重要。

## Shell 是用来解决什么问题的？

操作系统对外提供的接口是“系统调用”，也就是一堆编程用的接口。这些接口一般以C函数的形式暴露给使用者。通过这些接口，开发者可以命令操作系统“启动一个进程”，“查找某个目录下的所有文件”，“将某个文件的权限配置为744”等等。

> 实际上我们平时编程用的是对系统调用的包装，比如libc里的那些库函数。但无论如何，你总是得写代码才能使用它们。

问题是我们平时使用电脑，不能每次都编写程序，再编译，再运行得到结果吧。比如你想知道一个目录下的所有文件，你肯定不会去写一段C代码，调用系统调用“readdir” （见[http://www.man7.org/linux/man-pages/man3/readdir.3.html](https://link.zhihu.com/?target=http%3A//www.man7.org/linux/man-pages/man3/readdir.3.html)），然后gcc编译，然后运行。真这么干，一个最简单的工作也要耗费很长时间。况且，一个函数的返回是“数据结构”，或者输出到stdout或文件之类的地方。你总得以某种形式把结果“画”到界面上（不管是画字符还是画图片），才能查看那个结果。这个格式化输出的工作量也很大。

正常人的思路是先写好程序，然后弄个交互的界面方便使用这个程序。只要使用者敲一组字符串，就可以调用之前写好的程序完成工作。比如我们会在命令行里输入“ls -Rl”这种字符串。这个字符串被翻译成“ls”，“-R”，“-l”。“ls”帮我们找到那个之前写好的程序，并启动它；“-R”和“-l”被作为参数传给这个程序，告诉程序走“递归所有子目录”+“输出长格式”这部分代码。

这个负责把用户输入的字符串转换到需要执行的程序，并把结果以某个形式画出来的东西，就叫做“Shell”，即帮你更方便使用操作系统接口的“壳”。这个词与操作系统内核（Kernel）对应。

在Linux中，bash负责按照某种格式把用户的输出的字符串翻译，比如对于普通非空字符翻译为程序和参数，并尝试去PATH里找对应的程序；对“空格”翻译成分隔符；对“$XXX”尝试进行环境变量的替换；对“｜”翻译为管道；对 “&gt;”翻译为输出重定向；对一个指令末尾的“&”翻译为将程序转到后台执行……

另一方面终端将stdout、stderr输出的东西画成我们可以看的一坨坨字符，包括字符、字体、颜色、换行、甚至响铃。

【bash】 + 【终端】大概可以理解为一个以字符为交互方式的“Shell”。

![](https://pic2.zhimg.com/50/v2-32ba904a3c6311d39ff134e17e5b6c72_hd.jpg?source=1940ef5c)

> 当然，大家会更习惯上说“bash”是一个“Shell”。也许是因为bash大家用的不爽可以换，可以改变配置，可以编程。总之玩法很多。但是对终端一般不怎么理会，默认的就够用了。或者也可以理解为“终端“属于操作系统的一部分。不管如何，可以习惯这些不同角度的不同看法，不需要特别细纠，理解大概意思就好。

除了bash，还有像csh，zsh等，虽然各自的规定有些区别，但干的事情就是这些。Windows里一般等价的就是cmd，与Linux这边相比弱鸡得很。不过Windows有自己的PowerShell，也可以通过cygwin或者linux subsystem的方式使用bash。

如果你自己定义一个解析字符串的“Shell”，你可以规定用分号而不是空格做分隔符；用\#而不是$代表变量……

Shell也可以做成图形界面的，比如你点击一个窗口里左边的一个目录图标，右边就展示这个目录下的所有文件。他做的事情和上面ls一模一样，只不过接受的是点击，而不是输入字符串；输出的是一幅画，而不是格式化的文本而已。内部还是转换成调用系统调用来完成工作。

![](https://pic3.zhimg.com/50/v2-5586062636423871b711ba5af151ef27_hd.jpg?source=1940ef5c)

这个是不是看上去很眼熟

如果你开心，你甚至都把Shell可以做成声控的，比如你说一段语音，你的“Shell”识别后翻译为调用程序的指令，执行成功后就可以把结果翻译为语音再返回给你。你一定熟悉“小爱同学”，“Hi Siri”这种东西吧。

Shell的思想很普遍，并不一定限制在操作系统上。比如你自己写一个程序，有大量复杂的参数和配置。为了使用方便可以写个命令行工具将一个命令翻译成对这段程序的调用。你写的命令行工具就是你自己程序的“Shell”。比如写Java的同学肯定很熟悉mvn。一句mvn install可以产生出成百上千个下载、压缩、编译、清理、测试、上传等api的调用；使用数据库的同学也会用SQL来表达自己的查询，让数据库的“Shell”解释成对存储引擎各种api的调用。

## 为什么 mysql 要额外加入一个 utf8mb4 数据类型，而不是原地升级 utf8？

mySQL为什么在这个地方犯2。以下内容仅仅为一种猜测。

先说一下utf8的标准，早期是用1～6个byte来表示一个字符。所以最早的MySQL实现，一个Char是用6个Bytes去实现的。这是正确的做法。但是MySQL为了性能，希望用户使用等长度的字符列。也就是说，一个字符如果用不到6个byte，存储里就会被填充空白符号。学过计算机的人都会明白等长字符，用数组的索引值去找到数据会非常快。

> 早期的[RFC2279](https://link.zhihu.com/?target=https%3A//www.ietf.org/rfc/rfc2279.txt)规定一个UTF8字符被编码为1～6个byte。后来才改成了1～4个。

为此，MySQL甚至搞过一个“static-format“的存储结构，就是用定长数据来加速数据访问。（但这个东西只能用在MyISAM上，那时候MySQL还没收InnoDB）[15.2.3.1 Static \(Fixed-Length\) Table Characteristics​dev.mysql.com](https://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/5.7/en/static-format.html)

但也很容易看出来，这么干实在是太浪费空间了。绝大部分的英文用utf8编码1个byte就搞定了，中文等字符是3个byte。在1Charater = 6Byte的设计里，再加上2000年初时存储并不便宜，谁看了都会心疼。

好，基础讲完，再说历史。让我们回到2002年。MySQL计划在4.1版本支持utf8。4.1的早期开发版本用最多6个byte表示一个utf8字符，这是对的。但是MySQL不知道脑子里抽了哪根筋，在2002年9月27日，for no particular reason，搞出这么一个commit，强制让utf8编码只能处理最多3个byte的序列。

![](https://pic1.zhimg.com/50/v2-e4608ef603be82b3557ae89b4b51b032_hd.jpg?source=1940ef5c)

See：[UTF8 now works with up to 3 byte sequences only · mysql/mysql-server@43a506c](https://link.zhihu.com/?target=https%3A//github.com/mysql/mysql-server/commit/43a506c0ced0e6ea101d3ab8b4b423ce3fa327d0)

这个事情我查不到任何前因后果，查不到任何的文章，讨论和相关资料。

但大致猜测就是，MySQL当时又想用定长的存储，又觉得太浪费，干脆一鼓作气，把6改成了3。

在Unicode中，3个Byte可以支持所有的BMP（basic multi-lingual plane）的字符；但是无法支持SMP（supplementary multi-lingual plane），包括emoji（这是重灾区），一些生僻的CJK字符，一部分生僻的符号等。对于主要的文字（英文、欧洲各种语种、中文、日文……），3个byte的utf8也算是够用。

但是，多年之后，也许是苹果强力推emoji，大家才发现MySQL的utf8其实并不那么utf8。直到2010年，MySQL的5.5.3版本的时候，才引入了utf8mb4（从此刻开始，utf8是“utf8mb3“的alias）。这个change非常的不起眼，在更新总体介绍时压根就没提，只在明细里写了一行。也许是因为6改3这个变更实在太过于傻缺，不想张扬？

![](https://pic1.zhimg.com/50/v2-4717c17899e16bdd128fd8b1008b3ee1_hd.jpg?source=1940ef5c)

见：[https://dev.mysql.com/doc/relnotes/mysql/5.5/en/news-5-5-3.html](https://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/relnotes/mysql/5.5/en/news-5-5-3.html)

本题问为啥不把utf8原地升级，这个非常容易理解。大量的数据库文件已经按照了utf8的格式存储。如果“原地升级”也就意味着，所有已经存在的数据库文件都要重建。数据这个东西～都懂得。如果MySQL真的这么干了，估计就不是会被骂，而是会直接被判死刑。用utf8mb4的形式慢慢过渡也算是可以理解的做法。

顺便说一句。UTF8早期的标准[RFC2279](https://link.zhihu.com/?target=https%3A//www.ietf.org/rfc/rfc2279.txt)规定一个UTF8字符是1～6个Byte。这也是为什么早期Mysql把一个UTF8字符设计为6个Byte的原因。但是2003年11月，出了新的标准[RFC3629](https://link.zhihu.com/?target=https%3A//tools.ietf.org/html/rfc3629)，规定一个UTF8字符是1～4个Byte，就比MySQL做出那个很傻的commit晚了一年。历史真的很有趣。

从2002年到2019年，已经17年过去了，MySQL 8.0刚刚GA不久，但是utf8依然是utf8mb3的别名。不知道什么时候这个在一个莫名其妙的决策下诞生的奇行种才会被完全干掉。

哎～

![](https://pic4.zhimg.com/50/v2-5070d6da158d7fc2e88ff9b397220579_hd.jpg?source=1940ef5c)

摘自MySQL8.0的文档

## Chrome 为什么多进程而不是多线程？

本回答主要内容来自于Chromium开发团队在2008年的一个Blog。[Multi Process Architecture​blog.chromium.org](https://link.zhihu.com/?target=https%3A//blog.chromium.org/2008/09/multi-process-architecture.html)

建议大家看原文，但我把要点简单的总结一下。

### 为什么要给浏览器使用多进程架构？

传统的浏览器被设计为显示网页，而Chrome的设计目标是支撑“Web App”（当时的js和相关技术已经相当发达了，Gmail等服务也很成功）。这就要求Chrome提供一个类似于“操作系统”感觉的架构，支持App的运行。而App会变得相当的复杂，这就难以避免出现bug，然后crash。同时浏览器也要面临可能运行“恶意代码”。流览器不能决定上面的js怎么写，会不会以某种形式有意无意的攻击浏览器的渲染引擎。如果将所有这些App和浏览器实现在一个进程里，一旦挂，就全挂。

因此Chrome一开始就设计为把**隔离性**作为基本的设计原则，用进程的隔离性来实现对App的隔离。这样用户就不用担心：

* 一个Web App挂掉造成其他所有的Web App全部挂掉（稳定性）
* 一个Web App可以以某种形式访问其他App的数据（安全性）

以及Web App之间是并发的，可以提供更好的响应，一个App的渲染卡顿不会影响其他App的渲染（性能）（当然这点线程也能做到）

因此，这样看起来用进程实现非常自然。

### 每个进程里都有什么在跑？

Chromium里有三种进程——浏览器、渲染器和插件。

浏览器进程只有一个，管理窗口和tab，也处理所有的与磁盘，网络，用户输入和显示的工作。这就是我们看到的“Chrome界面”。

渲染器开多个。每个渲染器负责处理HTML、CSS、js、图片等，将其转换成用户可见的数据。当时Chrome使用开源的webkit实现这个功能。

> 顺便说一句，webkit是由Apple开发的，当时有很多坑，也被长期吐槽；现在Chrome已经转成使用自家的Blink引擎了。

插件会开很多。每个类型的插件在第一次使用时会启动一个相应的进程。

总结下，渲染器进程和插件进程就是平时被杀的最多的进程了。

### 什么时候创建进程?

一般来讲每一个网站的实例都会创建一个渲染进程。但也有特例，比如一个站点通过js在新tab/window上打开同一个站点的另外的页面。这两个界面内部会共享同一个进程，也能彼此分享数据。在Chrome角度，这两个页面算是“同一个App”。但是如果用户用浏览器的地址栏开一个新的tab，而该网址已经有tab了，Chrome会算是“来自同一域名的两个App”，从而创建新的进程。

但是大家都知道进程开多了资源消耗也变大，因此Chrome会限制最大的进程数量（比如20）。当进程达到这个数量后，Chrome会倾向于去复用已有的进程（所以这时，隔离性就会被影响）。

> 我看了一下Chrome的任务管理器，发现现在Chrome可以启动不止20个进程，大概是因为现在的计算机性能比当时强很多，所以可以吃更多资源。

![](https://pic1.zhimg.com/50/v2-95402cb9bd9f9611d7e899db4663076a_hd.jpg?source=1940ef5c)

根据这篇文章，Chromium实际支持多种进程模型。[https://www.chromium.org/developers/design-documents/process-models​www.chromium.org](https://link.zhihu.com/?target=https%3A//www.chromium.org/developers/design-documents/process-models)

* 每个站点的实例创建一个新的进程；
* 每个站点创建一个新的进程（根据domain）；
* 每个Tab创建一个新的进程；
* 单进程；

浏览器在现实中可以根据场景去灵活调整怎么创建新的进程。

### 总结

从这个Blog中我能总结出两点特别重要的地方：一个是**工程中的思维方式**。

Junior的程序员可能会这么思考：直觉上线程比进程高级，轻量，性能也更好，所以应该用多线程（从技术的主观感觉推断出方案）；

Senior的程序员或者架构师会这么思考：想解决的是复杂系统中的安全和错误处理的问题。为了解决它们，最好的办法就是**严格隔离**。而实现隔离最自然的方式就是用多进程。但多进程有一些问题，所以我要这样这样这样去设计，保证能用多进程同时还能最大程度避免那些问题（从问题的重要程度找到最核心的方案，再围绕核心方案分治去处理小问题）。

所以你能理解为什么Junior是Junior。Junior成长为Senior的先决条件就是多理解每一个技术点的利和弊，适用场景；以及如何找到一个复杂问题里最关键的问题是什么。

第二点是**处理错误的方法**。一般人说“稳定性”的第一反应是“兜底”。即不管什么问题，我要兜住让系统可以继续往下运行，只要不挂就行。但实际上是大错特错的。出现程序不能处理的异常，那么很可能程序接下来做的事情就不可控了，比如乱写磁盘文件，乱发非法请求。这对于浏览器这种系统是完全不可以接受的。对这样的问题的出现就必须“fast-fail“，也就是立刻Crash。所以我们能看到iOS App闪退，桌面程序挂掉，以及操作系统蓝屏。

但，的确，这样做会让体验不好，因此首先要保证工程质量在设计时就把尽可能多的错误处理考虑进去，并用测试等手段确保不出问题。

但即便如此，问题也无法100%的解决，总有意料之外的情况发生，因此要想办法去限制Crash的范围。这也是隔离存在的意义。为了实现高可靠的系统，必然要按照某种方式进行隔离，把高度危险的，可控度不高的部分隔离出去，避免其Crash伤及系统的其他部分。

> 有趣的是Erlang把这个设计理念完全纳入了其语言和OTP的设计里。

## JavaWeb 中 Service 层异常抛到 Controller 层处理还是直接处理？

这是个非常有启发意义的问题。

一般初学者学习编码和错误处理时，先知道编程语言有一种处理错误的形式或者约定（如Java就是抛异常），然后就开始用这些工具，但是却反过来忽视这个问题的本质：

**处理错误是为了写出正确的程序**

到底怎么算“正确”呢？是要由解决的问题决定的。问题不同，解决方案就不同。

比如，一个web接口接受用户的请求，这个请求需要传入一个参数“年龄”，也许业务要求这个字段应该是个0～150之间的整数。如果用护输入的是个字符串或者负数就肯定不被接受。一般在后端的某个地方都会做输入的合法性检查，检查不过就抛异常。但是归根到底这个问题的“正确”解决方法总是要以某种形式提示用户。而提示用户是某种前端工作，这就要看这个界面到底是app，H5 + ajax还是类似于jsp那样的服务器产生的界面。不管哪种，你需要根据需求去”设计一个修复错误“的流程。比如一个常见的流程需要后端抛异常，然后一路到某个集中处理错误的代码，将其转换为某个HTTP的错误（某个特定业务错误码）提供给前端，前端再去做”提示“。如果用户输入了非法的请求，从逻辑上后端都没法自己修复，这是个“正确”的策略。

换一个例子，比如用户想上传一个头像，后端将图片发给某个云存储，结果云存储报500错误。怎么办呢？你可能想到了重试几次，因为也许问题仅仅是临时的网络抖动而已，重试就可以正常执行。但如果重试多次无效。如果做系统时设计了某种热备方案，那么就可能改为发到另外一个服务器上。“重试”和“使用备份的依赖”都是“立刻处理“。

但如果重试无效，所有的备份服务也无效，那么也许就能像上面那样把错误抛给前端，提示用户“服务器开小差”。从这个方案很容易看出来，你想把错误抛到哪里是因为那个catch的地方是处理问题最方便的地方。一个问题的解决方案可能要几个不同的错误处理组合起来才能办到。

另外一个例子，你的程序抛了一个NPE。这一般就是程序员的bug——要不就是程序员想要表达一个东西”没有“，结果在后续处理中忘了判断是否为null；要不就是在写代码时觉得100%不可能为null的地方出现了一个null。不管哪种情况，这个错误用户总会看到一个很含糊的报错信息，这远远不够。“正确”的办法是程序员自己能尽快发现它，并尽快修复。要做到这一点，需要监控系统不断的爬log，把问题报警出来。而不是等到用户找客服来吐槽。

再换一个例子，比如你的后端程序突然OOM，挂了。挂的程序是没法恢复自己的。要做到“正确”就必须得在服务之外的容器考虑这个问题。比如你的服务跑在k8s上，他们会监控你程序的状态，然后重新启动新的服务实例以弥补挂掉的服务，还得调整流量，把去往挂掉服务的流量切掉，重新换到新的实例上。这里的恢复因为跨系统所以不能仅仅用异常实现，但是道理是一样的。但光靠重启就是“正确”的吗？如果服务是完全无状态的，问题不大。但是如果是有状态的，部分用户数据可能就会被执行一半的请求搞乱套。因此重启时要留意先“恢复数据到合法状态”。这又回到了你需要知道怎么样才是“正确”的做法。只依靠简单的语法功能是不能无脑解决这个事的。

> 我们可以推广下，一个工作线程的“外部容器“是管理工作线程的“master”。一个网络请求的“外部容器”是一个web server。一个用户进程的“外部容器”是操作系统。Erlang把这种supervisor-worker的机制融入到语言的设计中。

Web程序之所以很大程度上能够把异常抛给顶层，主要由于3个原因：

* 请求来自于前端，对于因为用户请求有误（数据合法性、权限、用户上下文状态）造成的问题，最终大概率只能告诉用户。因此抛异常到一个集中处理错误的地方，把异常转换为某个业务错误码的方法是合理的。
* 后端服务一般都是无状态的。这也是互联网系统设计的一般性原则。无状态就意味着可以随意重启。对于用户的数据因为下一条一般情况下不会出问题。
* 后端对数据的修改依赖DB的事务。因此一个改了一半的没提交的事务是不会造成副副作用。

但你要清楚**上面这3条并不是总是成立的**。总会存在一些处理逻辑并非完全无状态，也并不是所有的数据修改都能用一个事务保护。尤其要注意对微服务的调用，对内存状态**的修改是没有事务保护的**，一不留神就会出现搞乱用户数据的问题。比如：

```text
 try {
   int res1 = doStep1();
   this.statusVar1 += res1;
   int res2 = doStep2();
   this.statusVar2 += res2;
   int res3 = doStep3(); // throw an exception
   this.statusVar3 = statusVar1 + statusVar2 + res3;
} catch ( ...) { 
   // ...
}
```

 先假设this.statusVar1, this.statusVar2, this.statusVar3之间需要维护某种不变的约束（invariant）。然后执行这段代码时，如果在doStep3那抛出一个异常下面对statusVar3的赋值就不会执行。这时如果不能将statusVar1和statusVar2的修改rollback回去，就会造成数据违反约束的问题。而程序员一般是很难直接发现这个数据被改坏了。而坏掉的数据可能会偷偷的导致其他依赖这个数据的代码逻辑出错（比如原本应该给积分的，结果却没给）。而这种错误一般非常难调查，从大量数据里找到不正确的那一小撮是相当困难的事。

比起上面这段更难搞得定的是这样的代码：

```text
// controller
void controllerMethod(/* some params*/) {
  try {
    return svc.doWorkAndGetResult(/* some params*/);
  } catch (Exception e) {
    return ErrorJsonObject.of(e);
  }
}

// class svc
void doWorkAndGetResult(/* some params*/) {
    int res1 = otherSvc1.doStep1(/* some params */);
    this.statusVar1 += res1;
    int res2 = otherSvc2.doStep2(/* some params */);
    this.statusVar2 += res2;
    int res3 = otherSvc3.doStep3(/* some params */);
    this.statusVar3 = statusVar1 + statusVar2 + res3;
    return SomeResult.of(this.statusVar1, this.statusVar2, this.statusVar3);
}
```

这段代码的可怕之处在于，你在写的时候可能会以为doStep1～3这种东西即使抛异常，也能被Controller里的catch。在svc这层是不用处理任何异常的，**因此不写try……catch是天经地义的**。但实际上doStep1、doStep2、doStep3任何一个抛异常都会造成svc的数据状态不一致。甚至你一开始都可以通过文档或者其他沟通方式确定doStep1、doStep2、doStep3一开始都是必然可以成功，不会抛错的，因此你写的代码一开始是对的。但是你可能无法控制他们的实现（比如他们是另外一个团队开发的lib提供的），而他们的实现可能会改成会抛错。你的代码可能在完全不自知的情况下从“不会出问题”变成了“可能出问题”…… 更可怕的是类似于这样的代码是不能正确工作的：

```text
void doWorkAndGetResult(/* some params*/) {
    try {
       int res1 = otherSvc1.doStep1(/* some params */);
       this.statusVar1 += res1;
       int res2 = otherSvc2.doStep2(/* some params */);
       this.statusVar2 += res2;
       int res3 = otherSvc3.doStep3(/* some params */);
       this.statusVar3 = statusVar1 + statusVar2 + res3;
       return SomeResult.of(this.statusVar1, this.statusVar2, this.statusVar3);
   } catch (Exception e) {
     // do rollback
   }
}
```

你可能以为这样就会处理好数据rollback了，甚至**你会觉得这种代码非常优雅**。但是实际上doStep1～3每一个地方抛错，rollback的代码都不一样。你必须得这么写：

```text
void doWorkAndGetResult(/* some params*/) {
    int res1, res2, res3;
    try {
       res1 = otherSvc1.doStep1(/* some params */);
       this.statusVar1 += res1;
    } catch (Exception e) {
       throw e;
    }

    try {
      res2 = otherSvc2.doStep2(/* some params */);
      this.statusVar2 += res2;
    } catch (Exception e) {
      // rollback statusVar1
      this.statusVar1 -= res1;
      throw e;
    }
  
    try {
      res3 = otherSvc3.doStep3(/* some params */);
      this.statusVar3 = statusVar1 + statusVar2 + res3;
    } catch (Exception e) {
      // rollback statusVar1 & statusVar2
      this.statusVar1 -= res1;
      this.statusVar2 -= res2;
      throw e;
   } 
}
```

这才是能得到正确结果的代码——在任何地方出现错误都能维护数据一致性。优雅吗？看起来很丑。这甚至比go的if err != nil还丑。但如果一定要在正确性和优雅性上作出取舍，我会毫不犹豫的选择前者。作为程序员是不能直接认为抛异常可以解决任何问题的，你必须学会写出有正确逻辑的程序，哪怕很难，并且看起来很丑。为了达成很高的正确性，你不能总是把自己大部分注意力放在“一切都OK的流程上“，而把错误看作是可以随便搞一下的工作，或者简单的相信exception可以自动搞定一切。

总结一下，我希望所有程序员对错误处理都要有起码的敬畏之心。Java这边因为Checked Exception的设计问题不得不避免使用（见[大宽宽 - Java设计出checked exception有必要吗？](https://www.zhihu.com/question/30428214/answer/528317237)），而Uncaughted Exception**实在是太过于弱鸡，是不能给程序员提供更好地帮助的**。

因此，程序员在每次抛错或者处理错误的时候都要对自己灵魂三击：这个错误的处理是正确的吗？会让用户看到什么？会不会搞乱数据？不要以为自己抛了个异常就不管了。

此外，在编译器不能帮上太多忙的时候，好好写UT来保护代码脆弱的正确性。

**为人为己，总是写正确的代码**。

## WEB开发中，使用JSON-RPC好，还是RESTful API好？

简单来说：不管哪个“好”还是不“好”，RESTful API在很多实际项目中并不使用。因此真的做了项目，你可能会发现只能用HTTP+JSON来定义接口，无法严格遵守REST风格。

为什么说不实际呢？因为这个风格太理想化了，比方说：

* REST要求要将接口以资源的形式呈现。但实际上，很多时候都不太可能将一些业务逻辑看作资源。即使强制这么干了，也会非常非常别扭。登录就是登录，而不是“创建一个session”；播放音乐就是播放，而不是“创建一个播放状态“。

> 我们之所以要定义接口，本身的动机是做一个抽象，把复杂性隐藏起来，而绝对不是把内部的实现细节给暴露出去。REST却反其道而行之，要求实现应该是“资源”并且这个实现细节要暴露在接口的形式上。  
>   
> 但一个好的接口设计就应该是简单、直观的，能够完全隐藏内部细节的，不管底层是不是资源，资源的组合还是别的什么架构。此外，让业务逻辑与接口表现一致，对系统的长期维护和演进都有极大的好处。

* REST只提供了增删改查的基本语义，其他的语义基本上不管。比如批量添加，批量删除，修改一个资源的一部分字段。区分“物理删除”和“标记删除”等等。复杂的查询更加不显示，对于像筛选这类的场景，REST明显就是个渣。这里要表扬一下GraphQL（但GraphQL有其他的问题，在此不展开）
* REST建议用HTTP的status code做错误码，以便于“统一”，实际上这非常难统一。各种业务的含义五花八门，抽象层次高低不齐，根本就无法满足需要。比如一个404到底是代表这个接口找不到，还是代表一个资源找不到。400表达请求有问题，但是我想提示用户“你登录手机号输入的格式不对“，还是“你登录手机号已经被占用了“。既然201表示“created”，为啥deleted和updated没有对应的status code，只能用200或者204（no content）？错误处理是web系统里最麻烦的，最需要细心细致的地方。REST风格在这里只能添乱。
* web请求参数可能散布在url path、querystring、body、header。服务器端处理对此完全没有什么章法。客户端和服务器端的研发之间还是要做约定。
* 在url path上的变量会对很多其他的工作带来不良影响。

> 比如监控，本来url可以作为一个接口的key统计次数/延迟，结果url里出了个变量，所以自动收集nginx的access log，自动做监控项目增加就没法弄了。  
> 再比如，想对接口做流量控制的计数，本来url可以做key，因为有变量，就得多费点事才行。

* 现实中接口要处理的真正的问题，REST基本上也没怎么管。比如认证、授权、流控、数据缓存（http的etag还起了点作用）、超时控制、数据压缩……。
* REST有很多好的工具可以便利的生成对应的代码和文档，也容易形成规范。但问题是REST在实际的项目中并没有解决很多问题，也在很多时候不合用，因此产生的代码和文档也就没什么用，必须经过二次加工才能真的用起来。因此可以基于REST+你的业务场景定义一个你自己的规范。

REST的本意是基于一个架构的假设（资源化），定义了一组风格，并基于这个风格形成约定、工具和支持。思路不错。但是因为他的架构假设就是有问题的，因此后续一系列东西都建立在了一个不稳固的基础之上。同时，REST并没有解决太多的实际问题。

是，的确，有些时候，用REST完成CRUD已经能完成任务了。此时，用REST没有什么不好的。但是，现实当中，真正的业务领域一般都会比资源的CRUD复杂的多。这时REST“基本上没解决太多实际问题”的缺点就会体现出来。我所见到的大多数情况，是会形成一种REST-like形式的接口，像REST却又不限于REST。

为了REST，我看到了太多的人在争执到底是POST还是PUT，到底用querystring还是body，到底用200还是201，到底一个单词应该用单数还是复数，到底一个请求参数应该放在url path的中间一段还是最后一段…… 真正要做的事情本身反而没人关心。而一旦把争论给一个“REST专家”看，他的回答八成是“其实你还是不懂REST”...

我觉得人生不能这么糟蹋，你觉得呢？

---- 附一个现实当中接口的开发的方式

你可以总是从REST开始，如果你要开发的东西能被自然而然的想成是一个资源。然后通过相关的工具自动生成一些代码，把这个原型和你的合作者讨论一下。这是我能想到的REST能做的一件很好的事情——快速实验。

然后如果你想认真的往下做，就可以彻底忘记REST这件事。开始自己定义业务接口，尽量不要在url里加变量。尽量只用GET和POST，减少一些沟通上的混乱。对于每个接口，好好定义可能发生的业务错误，并与PM一起协商怎么处理这些错误。认真的考虑认证、授权、流控等机制，当你开发的是和钱相关的业务尤其要留意。

最后，本文并不是说“绝对不要用REST”，而是：如果你在实际工作中用REST有了困惑，不知道某个情况下REST此时的最佳实践是什么时，不要追求“真正的REST会怎么做”，不要被REST限制住。

----- 2018-12-19更新

感谢[@David Dong](//www.zhihu.com/people/a881391b3f39dc9f33823869cba341bc) 的评论，基于他的评论，我写下我的感受。

如果看过REST最初的那篇论文[Architectural Styles and the Design of Network-based Software Architectures](https://link.zhihu.com/?target=https%3A//www.ics.uci.edu/~fielding/pubs/dissertation/top.htm)就会发现，当时想设计的目标是解决互联网级别的信息共享和互操作问题。而我们的大量开发者工作的主要目标是“为业务系统实现一个满足功能（比如登录，交易……）/非功能需求（比如认证，性能，可扩展性……）的接口“。并且设计接口时会区分“给第三方用的开放接口”、“给UI开发定制的接口“和“内部使用的接口”等。这些接口的设计目标都和REST当初制定的目标有差别。其中最接近的，是“开放接口”。因此可以看到有些开放接口用REST实现还是很不错的，比如github的接口，AWS S3的接口等。

但是其他两类接口与REST关注的点完全不一样。比如面向UI的接口的就要满足UI需要。此时资源不资源不太重要，而是尽量用少的roundtrip去返回这个界面需要的所有数据。接口是按照加载的优先级，而不是“资源”做切分。比如第一屏的显示要尽量一个接口先给出来，后续异步加载的数据可以用其他接口慢慢出。为UI提供的接口往往被划归为“大前端“的一部分。

而内部的接口，越接近DB的，越容易用表来mapping到“资源“，但是内部的接口需要考虑到数据整合的需要。比如底层的用户数据分为A、B、C三类，但这3个数据因为服务隔离不能直接在DB做join。需求要按照A的某个条件做排序分页，但要注入B和C的数据。这时就需要B和C提供batch读取和app注入的相关逻辑。此外还有复杂的查询条件，可动态改变的输出字段等要求。REST的“资源”概念在这里能帮上的忙有限。这也是GrpahQL尝试解决的问题。

再有一类问题是用接口实现分布式一致性的业务问题。比如下单+支付+扣库存+加积分问题。这时接口的形式并不重要，而能够支持实现SAGA或者TCC才是最关键的。而整个业务对外的感觉实际上是创建一个“事务”。早期一本叫做Resftul Web Services的书描述Restful接口做这个事情的方案是：

1. 调用接口创建一个事务的资源
2. 拿着事务资源的id，调用步骤1接口，步骤2接口……
3. 拿着事务资源的id，调用事务的commit接口

这种形式不仅臃肿，还把怎么做这件事的内部细节完全暴露到了调用方，造成了耦合。而我们一般常见的做法就是一个接口POST /doSomething，然后接口实现方内部维护事务，维护commit，rollback等细节。有的时候还需要添加一些异步回调。

简单总结下，写接口的目标各自不同。而REST的目标是“实现互联网级别的信息共享系统”，这个目标和大部分开发者要实现的目标完全不同，这就不难解释为何照搬REST去做另一个领域的事情可能会非常别扭。

## 如何保持mysql和redis中数据的一致性？

这实际上是个“如果要做的足够精致是非常难的“问题。缓存失效被称为计算机科学里最难的两个问题之一（另外一个是起名字）。

先对本题一致性做个说明。这里的不一致是指：假如一个数据访问者同时读取Redis和DB，他能在一段时间里发现二者不一样。

不错，如果一份数据放在DB，然后copy到Redis，然后改DB，那么Redis是不会自己魔幻般同步变更的。必须有某种机制告诉Redis该变了。这些机制包括（但不仅仅限于）：

1. Redis里的数据不立刻更新，等redis里数据自然过期。然后去DB里取，顺带重新set redis。这种用法被称作“Cache Aside”。好处是代码比较简单，坏处是会有一段时间DB和Redis里的数据不一致。这个不一致的时间取决于redis里数据设定的有效期，比如10min。但如果Redis里数据没设置有效期，这招就不灵了。

2. 更新DB时总是不直接触碰DB，而是通过代码。而代码做的显式更新DB，然后马上del掉redis里的数据。在下次取数据时，模式就恢复到了上一条说的方式。这也算是一种Cache Aside的变体。这要做的好处是，数据的一致性会比较好，一般正常情况下，数据不一致的时间会在1s以下，对于绝大部分的场景是足够了。但是有极少几率，由于更新时序，下Redis数据会和DB不一致（这个有文章解释，这里不展开）。

![](https://pic2.zhimg.com/50/v2-e8152aeae0ece1120e91a6eee52603d6_hd.jpg?source=1940ef5c)

Cache Aside，就是“Cache”在DB访问的主流程上帮个忙

**1和2的做法常规上被称为“Cache“**。而且因为1有更新不及时的问题，2有极端情况下数据会不一致的问题，所以常规Cache代码会把1+2组合起来，要求Redis里的数据必须有过期时间，并且不能太长，这样即便是不一致也能混过去。同时如果是主动对数据进行更新，Cache的数据更新也会比较及时。

并且2并不一定总是行得通。比如OLTP的服务在前面是Cache+DB的模式，而数据是由后台管理系统来更新的，总是不会触碰OLTP服务，更不会动Cache。这时将Redis看作是存储也算是一种方案。就是：

3. Redis里的数据总是不过期，但是有个背景更新任务（“定时执行的代码” 或者 “被队列驱动的代码）读取db，把最新的数据塞给Redis。**这种做法将Redis看作是“存储”。**访问者不知道背后的实际数据源，只知道Redis是唯一可以取的数据的地方。当实际数据源更新时，背景更新任务来将数据更新到Redis。这时还是会存在Redis和实际数据源不一致的问题。如果是定时任务，最长的不一致时长就是更新任务的执行间隔；如果是用类似于队列的方式来更新，那么不一致时间取决于队列产生和消费的延迟。常用的队列（或等价物）有Redis（怎么还是Redis），Kafka，AMQ，RMQ，binglog，log文件，阿里的canal等。

![](https://pic4.zhimg.com/50/v2-22afd484600aa412f86c92e6c7fe56bd_hd.jpg?source=1940ef5c)

Cache当作“存储”来用，访问者只看得到Cache

这种做法还有一种变体Write Through，写入时直接写DB，DB把数据更新Cache，而读取时读Cache。

![](https://pic2.zhimg.com/50/v2-ee738568bae9b7d678bd356d2307b4cb_hd.jpg?source=1940ef5c)

Write Through + Cache当存储

**以上方式无论如何都会有一段时间Redis和DB会不一致**。实践上，这个不一致时间短则几十ms，长可以到几十分钟。**这种程度的一致性对于很多业务场景都已经足够了。**很多时候，用户无法区分自己读取的是Redis还是DB，只能读取到其中的一个。这时数据看起来直觉上是没问题的就可以接受了。只要不出现，用户先看见了数据是A，然后看到数据是B，之后一刷新，又看到A的尴尬场景就行了。（这也可以部份解释为啥用经常使用共享式的Cache而不是本地Cache方案）。

但对于有些业务，比如协作文档编辑，电商秒杀的扣库存，银行转账等，以上的做法就不够用了。解决办法也有两大类。第一种是不要用Redis，只用DB。或者更直接点说是“只要一个单点的数据源”。这样肯定就没有一致性问题，代价就是CAP中因为CP被满足，因此A被牺牲掉。这就是为啥银行一系统升级就要停服务的原因。

> 当然实际上也有CAP兼顾，但是C要的强一点，A就得弱一点，但不至于完全牺牲掉的做法。这里不展开。

另外一种保证一致性的做法就是用某种分布式协议一致性来做，大致可以归结到

1. SAGA或者TCC - 这两种需要业务代码的大量配合。通过业务代码来补偿一致性。
2. 2PC, 3PC - 现实当中有XA协议。比如Ehcache是支持XA协议的。但是性能表现不佳，运维也麻烦，我比较少见到实际这么干的。
3. 基于Paxos或者Raft的分布式锁，然后对Redis和DB进行双写，但是除非客户端和服务器么次都去访问分布式锁，也会有一点点不一致的问题。这实际上相当于将多个地方的一致性控制交给了分布式锁的集中维护。

这些做法实施复杂度和运维复杂度太高，以至于对于像Redis + DB这种场景基本上没人这么干。本质上大家用Redis一般也就是想做个Cache而已。这些方案通常被用到比如多数据中心数据一致性维护的系统中。

综上，除了单点DB存储之外的方案，其一致性面临的窘境是

* 要么，接受“最终一致”，但到底多久之后一致，不一致时表现怎么样，有很多种做法。分布式一致性有各种各样的模型，比如线性一致性、顺序一致性等。他们都是在“不一致”和“强一致”之间提供某种折衷。这些折衷大量应用于我们常见的诸多业务之中、如社交、IM、电商不触及钱的地方等
* 要么，要求必须强一致。那么在分布式条件下就要牺牲A。比如访问一个Cache，Cache知道自己的数据不是最新的，就要和DB去Sync，Sync的过程中DB的数据还不能改。期间访问者要不收到一个错误“数据不同步，不能访问”，要不就卡在那里等着同步完成。个人以为，这还不如干脆就不要Cache，在维护强一致的同时，用其他方式来优化访问性能。

最最后提醒下，本文有很多不严谨的地方，包括对Cache的形式总结其实只有典型的几种，实际可能的要多得多；再比如对一致性的介绍也非常粗浅，原因是为了让初学者有一点点概念，能看得进去（就这样，已经很长了，评论区里也有人表示接受不了）。

对于分布式和其一致性的完整知识的学习需要耗费大量的精力，Good Luck & Best Wishes。

