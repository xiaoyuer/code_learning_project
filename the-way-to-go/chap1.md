# Da-Kuan-Kuan

链接：https://www.zhihu.com/question/370830450/answer/1010322452

## golang为什么将method写在类外?

go本质上反对那种OOP做法的。从go的哲学来讲，简单的东西才能写正确，好理解和维护。因此在go中并不提供OOP的“类”，而仅仅提供structure和属于structure的方法。

于是属于structure的方法可以设定自己到底是希望要\(t T\)还是\(t \*T\) ，以决定自己到底要不要复制一份数据。

go表达的就是函数就是函数，数据就是数据。与数据绑定的函数提供t.foo\(\)这种写法。但也仅此而已了。

## 面向对象编程的弊端是什么？

面向对象编程是一种处理复杂问题的设计工具，本身没有什么好坏之分，只有用的好坏之分。但面向对象的问题在于长期以来的技术环境、编程语言、一些工具的推广、培训和教育都大大的过分乐观的强调了面向对象编程本身可以带来的好处。以至于很多学习编程的人都深深的相信“只要用了面向对象编程（以及基于其基础之上的的一系列设计模式、规范、工具、框架），就能得到非常容易维护、可以复用、明晰可理解的代码“。

但，**这并不是真的**。

如果你经历过很多，就会发现“只要如何如何，就一定能如何如何”这个提法一旦出现，基本上就不靠谱，不管是编程还是别的什么事情。

在大量的场景中，可以偏执的认为“万物皆对象”（或者万物皆别的什么），但是哲学上的单纯并不一定能让现实中的工程变得更“好”。如果说非得有个“万物皆XX”，那么这个XX八成就是根据众多需求综合到一起的“**折衷**”。

简单从工程讲的话，如果程序（或者说工作）是一次性的，那么怎么写得快，能work就怎么来。这个相对好理解。但是，如果程序是要长期维护的，那么**如何管理其复杂性**是核心的问题。而管理复杂性的要点在于

* 让事情本身变得简单。这说白了就是砍需求，研发和PM之间要经常沟通去避免nice to have的需求变动带来的程序复杂性的剧烈变化（比如一个1对1的实体关系，需求变动一点就变成了麻烦的多的“有时1对1，有时1对多”的混合关系）。
* 运用隔离的手段将复杂性拆解为互相影响很小的单元。一个单元对外只暴露一个简单的“接口”，隐藏内部复杂性。这就是“抽象”或者“封装“的力量。但是问题在于，这个抽象本身是否做的合适是由于问题决定的，而不是代码本身决定的。

即便是抽象，也有很多种做法。可以定义一组接口，这个接口是一组函数、一组服务的RPC还是一个class的public method都可以根据实际情况商讨。面向对象只是这里面其中一种做法而已。一个想要把程序编好的人，需要注重的是理解问题，然后尝试做出几种不同的抽象，评估各自优缺点后得到一个当时可行解的能力。而现有的大环境、教育体系，没有那么多真实的、复杂的案例，只能用一些简单的sample code来教授。并且在说明问题本身时，简化问题本身，而突出代码设计的“模式”。这就好像是在用视频教人游泳一样。学习者自己需要认识到这些培训只是个参考，玩真的还是要到项目里去体会。

即便是用面向对象做抽象也会有问题。很多时候，面向对象编程并不是一种好的“抽象”。如果抽象做得好，透过抽象出来的“接口”就可以轻易的使用这个系统。这时“大量的复杂性”被隐藏到接口后的实现里。这就像是你看电视从来都不需要拆开壳子看里面液晶屏幕和视频信号的转换，只需要知道【电源】、【调台】、【调音量】就能用。一个抽象做得好，往往要“deep”，隐藏足够的复杂度。而面向对象的文化/教育往往会鼓励程序员做很多无意义的，无性价比的抽象。看看有些代码里完全不知所云的adaptor，factory，builder等就是这种做法的产物。

此外，在大量使用继承作为设计方法时，也没有起到任何实质的隔离作用。如果你尝试扩展一个继承体系，往往需要了解整个继承体系才能写对代码——这时，复杂性并没有被隐藏起来。你也许只是代码写的少了而已。对于这**种复杂度没有降低，编写代码只是写的少，但是要看懂还是得结合整个体系才能做到的方式，不是抽象，是“压缩”。**压缩只能少写代码，却会让系统更难以理解了。

> 也许不太容易理解压缩在这里意思。比如在一段被压缩的数据中有3个bytes是“A”，“1”， “8”。但是他们的意思可能是A连续出现18次，也许是A1连续出现8次。至于到底是哪个意思，必须从头读所有的数据才能弄明白。编码也是这个道理。

再说说类型本身。一些面向对象编码对类型的定义要求的比较严格。其本质假设是“如果一个Object的类型是XXXX”，则其行为模式必然是“YYYY”。但现实当中，一个Object的行为模式不光与他的类型有关，还与这个Object“如何被使用”有关。比方说，一个User的Object，如果是用户自己看自己，就可以登陆、登出，修改昵称；如果是其他普通用户看，就只能看看看昵称和头像；如果是管理员来操作，可以reset密码、注销或者踢出登陆。这时就得界定一个Scope，来说明现在的User到底是哪个scope的User。DDD的一些理念就源自于此——找到某个上下文的某个实体概念，不能有歧义。但是即便不用DDD，也必须用各种变通的手段，把“如何用”的信息与类型信息结合到一起来实现逻辑。很郁闷的是，这个“如何用”完全没有章法，可能是“iOS App登陆“，也可能是“第一次下单时”，或者是“系统处于降级状态”时。你永远也猜不到下一次可能会有个什么条件是要纳入到上下文的。大家都知道大量用if不好，容易让代码变成麻花，无法维护。但面向对象编程本身没解决这个问题。很多文章提出面向对象某个模式可以少写if，让代码容易维护。但是这其实是建立在那个问题的上下文已经明确的基础之上。上下文易变的问题没有解决，换一个上下文，招数便不灵了，到时还得处理一坨“模式代码”，非常恶心。

最后，面向对象会倾向于将不同的代码抽象为不同相互作用的Object，但是有一些现实因素会让这么面向对象得到非常不理想的效果：

* **安全** - 如果你的代码要求非常安全，那么所有的Object都要耦合安全控制的代码；要不就是在一层对外的接口之前拦截一道处理安全问题，内部Object都无视安全问题。这也就相当于放弃了一部分的安全性。
* **性能** - 如果强调性能的话，是要尽量减少隔离的层次的。无论抽象如何做，只要隔离发生，就要经历一次转换以及相应的性能损耗。比如早期的Hibernate不支持“bulk insert”和“bulk update”，只能逼着程序员做for loop IO；而native的sql却可以轻易办到。在每多一次IO都很伤的场景下，这种隔离只能把事情做的更糟。
* **数据为中心** - 很多业务场景都是以数据为中心。也就是说DB里的那坨数据是唯一的truth。在代码层面做的只是为处理数据更加方便。这时做的很多抽象意义不大。比如你可以在ORM层强制声明读取出来的一个数据少了某个字段是invalid的。但是你没法阻止你的第三方数据提供商源给你invalid的数据。对Invalid数据的处理远不是一个Annotation就能搞定的，必须引入复杂的业务流程。
* **灵活性和成本** - 每次做某种抽象都意味着对一个系统“要做某种变化的能力做出优化”，但是同时，也就意味着或多或少对其他种变化适应性做“劣化“。如果系统变化的方向和预期的不一致，那么浪费掉的工作不说，为了再次调整设计方向的代价也会相当的大。这种情况比比皆是。

总结下，我希望所有的程序员都要理解自己的工作的最终目的是干什么的，并且活用自己所能用到的一切工具来达成自己的目标。不要在各种编程范式里迷了路。如果是初学编程的人，我衷心的希望你的编程课程讲授的是解决一些实际的问题，多了解业务，多尝试对业务的变动作出合理和准确的预。不要过早的接触高层的思想和哲学层面的问题——一个小孩看《红楼梦》又能真的看懂多少呢。

P.S. 回到面向对象编程的本身，我这里有一篇回答比较详细的解释了一下[大宽宽：怎么从本质上理解面向对象的编程思想？​www.zhihu.co](https://www.zhihu.com/question/305042684/answer/550196442)

## 怎么从本质上理解面向对象的编程思想？

面向对象编程（OOP），是一种**设计思想**或者**架构风格**。OO语言之父Alan Kay，Smalltalk的发明人，在谈到OOP时是这样说的：

> I thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages \(so messaging came at the very beginning -- it took a while to see how to do messaging in a programming language efficiently enough to be useful\).  
> ...  
> OOP to me means only messaging, local retention and protection and hiding of state-process, and extreme late-binding of all things. It can be done in Smalltalk and in LISP.

简单解释一下上面的这几句话的大概意思：OOP应该体现一种网状结构，这个结构上的每个节点“Object”只能通过“消息”和其他节点通讯。每个节点会有内部隐藏的状态，状态不可以被直接修改，而应该通过消息传递的方式来间接的修改。

这个编程思想被设计能够编写庞大复杂的系统。

那么为什么OOP能够支撑庞大复杂的系统呢？用开公司举个例子。如果公司就只有几个人，那么大家总是一起干活，工作可以通过“上帝视角“完全搞清楚每一个细节，于是可以制定非常清晰的、明确的流程来完成这个任务。这个思想接近于传统的面向过程编程。而如果公司人数变多，达到几百上千，这种“上帝视角”是完全不可行的。在这样复杂的公司里，没有一个人能搞清楚一个工作的所有细节。为此，公司要分很多个部门，每个部门相对的独立，有自己的章程，办事方法和规则等。独立性就意味着“隐藏内部状态”。比如你只能说申请让某部门按照章程办一件事，却不能说命令部门里的谁谁谁，在什么时候之前一定要办成。这些内部的细节你管不着。类似的，更高一层，公司之间也存在大量的协作关系。一个汽车供应链可能包括几千个企业，组成了一个商业网络。通过这种松散的协作关系维系的系统可以无限扩展下去，形成庞大的，复杂的系统。这就是OOP想表达的思想。

第一门OOP语言是Ole-Johan Dahland和Kristen Nygaard发明的Simula（比smalltalk还要早）。从名字就可以看出来，是用来支撑“模拟系统”的。模拟这个场景非常适合体现OOP的这个思想。这个语言引入了object、class、subclass、inheritance、动态绑定虚拟进程等概念，甚至还有GC。Java很大程度上受了Simula的影响。我们在现在教书上讲解OOP类、实例和继承关系时，总会给出比如动物-猫-狗，或者形状-圆-矩形的例子，都源自于此。

> 还有一些带有OO特征的语言或者研究成果在Simula之前就出现，这里就不往前追溯了。

但随后在施乐Palo Alto研究中心（Xerox PARC），Alan Kay、Dan Ingalls、Adele Goldberg在1970年开发了smalltalk，主要用于当时最前沿计算模型研究。在Simula的基础之上，smalltak特别强调messaging的重要性，成为了当时最有影响力的OOP语言。与smalltalk同期进行的还有比如GUI、超文本等项目。smalltalk也最早的实现了在GUI使用MVC模型来编程。

但是，并不是说OOP程序一定要用OOP语言来写。再强调一下，OOP首先是一种**设计思想**，非仅仅是编码方式。从这个角度推演，其实OOP最成功的例子其实是互联网。（Alan Kay也是互联网前身ARPNET的设计者之一）。另外一个OOP典型的例子是Linux内核，它充分体现了多个相对独立的组件（进程调度器、内存管理器、文件系统……）之间相互协作的思想。尽管Linux内核是用C写的，但是他比很多用所谓OOP语言写的程序更加OOP。

现在很多初学者会把使用C++，Java等语言的“OOP”语法特性后的程序称为OOP。比如封装、继承、多态等特性以及class、interface、private等管家你在会被大量提及和讨论。OOP语言不能代替人类做软件设计。既然做不了设计，就只能把一些轮子和语法糖造出来，供想编写OOP程序的人使用。但是，特别强调，**是OOP设计思想在前，OOP编码在后**。简单用OOP语言写代码，程序也不会自动变成OOP，也不一定能得到OOP的各种好处。

我们在以为我们在OOP时，其实很多时候都是在处理编码的细节工作，而非OOP提倡的“独立”，“通讯”。以“class”为例，实际上我们对它的用法有：

* 表达一个类型（和父子类关系），以对应真实世界的概念，一个类型可以起到一个“模版”的作用。这个类型形成的对象会严格维护内部的状态（或者叫不变量）
* 表达一个Object（即单例），比如XXXService这种“Bean”
* 表达一个名字空间，这样就可以把一组相关的代码写到一起而不是散播的到处都是，其实这是一个“module”
* 表达一个数据结构，比如DTO这种
* 因为代码复用，硬造出来的，无法与现实概念对应，但又不得不存在的类
* 提供便利，让foo\(a\)这种代码可以写成a.foo\(\)形式

其中前两种和OOP的设计思想有关，而其他都是编写具体代码的工具，有的是为了代码得到更好的组织，有的就是为了方便。

很多地方提及OOP=封装+继承+多态。我非常反对这个提法，因为这几个术语把原本很容易理解的，直观的做事方法变的图腾化。初学者往往会觉得他们听上去很牛逼，但是使用起来又经常和现实相冲突以至于落不了地。

“封装”，是想把一段逻辑/概念抽象出来做到“相对独立”。这并不是OOP发明的，而是长久以来一直被广泛采用的方法。比如电视机就是个“封装”的好例子，几个简单的操作按钮（接口）暴露出来供使用者操作，复杂的内部电路和元器件在机器里面隐藏。再比如，Linux的文件系统接口也是非常好的“封装”的例子，它提供了open，close，read，write和seek这几个简单的接口，却封装了大量的磁盘驱动，文件系统，buffer和cache，进程的阻塞和唤醒等复杂的细节。然而它是用函数做的“封装”。好的封装设计意味着简洁的接口和复杂的被隐藏的内部细节。这并非是一个private关键字就可以表达的。一个典型的反面的例子是从数据库里读取出来的数据，几乎所有的字段都是要被处理和使用的，还有新的字段可能在处理过程中被添加进来。这时用ORM搞出一个个实体class，弄一堆private成员再加一堆getter和setter是非常愚蠢的做法。这里的数据并非是具有相对独立性的，可以进行通讯的“Object“，而仅仅是“Data Structure”。因此我非常喜欢有些语言提供“data object”的支持。

> 当然，好的ORM会体现“Active Record”这种设计模式，非常有趣，本文不展开

再说说“继承”，是希望通过类型的 is-a 关系来实现代码的复用。绝大部分OOP语言会把is-a和代码复用这两件事情合作一件事。但是我们经常会发现这二者之间并不一定总能对上。有时我们觉得A is a B，但是A并不想要B的任何代码，仅仅想表达is-a关系而已；而有时，仅仅是想把A的一段代码给B用，但是A和B之间并没有什么语义关系。这个分歧会导致严重的设计问题。比如，做类的设计时往往会希望每个类能与现实当中的实体/概念对应上；但如果从代码复用角度出发设计类，就可能会得到很多现实并不存在，但不得不存在的类。一般这种类都会有奇怪的名字和非常玄幻的意思。如果开发者换了个人，可能很难把握原来设计的微妙的思路，但又不得不改，再稳妥保守一点就绕开重新设计，造成玄幻的类越来越多…… 继承造成的问题相当多。现在人们谈论“继承”，一般都会说“Composite Over Inheritance“。

多态和OOP也不是必然的关系。所谓多态，是指让一组Object表达同一概念，并展现不同的行为。入门级的OOP的书一般会这么举例子，比如有一个基类Animal，定义了run方法。然后其子类Cat，Dog，Cow等都可以override掉run，实现自己的逻辑，因为Cat，Dog，Cow等都是Animal。例子说得挺有道理。但现实的复杂性往往会要求实现一个不是Animal的子类也能“run”，比如汽车可以run，一个程序也可以“run”等。总之只要是run就可以，并不太在意其类型表达出的包含关系。这里想表达的意思是，如果想进行极致的“多态”，is-a与否就不那么重要了。在动态语言里，一般采用duck typing来实现这种“多态”——不关是什么东西，只要觉得他可以run，就给他写个叫“run”的函数即可；而对于静态语言，一般会设计一个“IRun”的接口，然后mixin到期望得到run能力的类上。简单来说，要实现多态可以不用继承、甚至不用class。

OOP一定好吗？显然是否定的。回到OOP的本心是要**处理大型复杂系统**的设计和实现。OOP的优势一定要到了根本就不可能有一个“上帝视角”的存在，不得不把系统拆成很多Object时才会体现出来。

举个例子，smalltalk中，1 + 2 的理解方式是：向“1”这个Object发送一给消息“+”，消息的参数是“2”。的确是非常存粹的OOP思想。但是放在工程上，1 + 2理解为一般人常见的表达式可能更容易理解。对于1 + 2这样简单的逻辑，人很容易从上帝视角出发得到最直接的理解，也就有了最简单直接的代码而无用考虑“Object”。

如果是那种“第一步”、“第二步“……的程序，面向数据的程序，极致为性能做优化的程序，是不应该用OOP去实现的。但很无奈如果某些“纯OOP语言”，就不得不造一些本来就不需要的class，再绕回到这个领域适合的编码模式上。比如普通的Web系统就是典型的“面向”数据库这个中心进行数据处理（处理完了展示给用户，或者响应用户的操作）。这个用FP的思路去理解更加简单，直观。也有MVC，MVVM这样的模式被广泛应用。

还有一些领域尽管用OOP最为基础很适合，但是根据场景，已经诞生出了“领域化的OOP”，比如GUI是一个典型的例子。GUI里用OOP也是比较适合的，但是GUI里有很多细节OOP不管或者处理不好，因此好的GUI库会在OOP基础之上扩展很多。早期的MFC，.Net GUI Framework, React等都是这样。另外一个领域是游戏，用OOP也很合适，但也是有些性能和领域细节需要特殊处理，因此ECS会得到广泛的采用。

总结一下，OOP是众多设计思想中的一种。很多OOP语言把这种思想的不重要的细节工具化，但直接无脑应用这些工具不会直接得到OOP的设计。即便是OOP思想本身也有其适合的场景和不适合的场景。即便是适合的场景，也可能针对这个场景在OOP之上做更针对这个场景需求的定制的架构/框架。如果简单把OOP作为某种教条就大大的违反了这个思想的初衷，也只能得到拧巴的代码。

## 为什么数据库和数据库连接池不采用类似java nio的IO多路复用技术使用一个连接来维护和数据库的数据交换？

这是一个非常好的问题。IO多路复用被视为是非常好的性能助力器。但是一般我们在使用DB时，还是经常性采用c3p0，tomcat connection pool等技术来与DB连接，哪怕整个程序已经变成以Netty为核心。这到底是为什么？

首先纠正一个常见的误解。IO多路复用听上去好像是多个数据可以共享一个IO（socket连接），实际上并非如此。**IO多路复用不是指多个服务共享一个连接，而仅仅是指多个连接的管理可以在同一进程**。在网络服务中，IO多路复用起的作用是**一次性把多个连接的事件通知业务代码处理**。至于这些事件的处理方式，到底是业务代码循环着处理、丢到队列里，还是交给线程池处理，由业务代码决定。

对于使用DB的程序来讲，不管使用多路复用，还是连接池，都要维护一组网络连接，支持并发的查询。

为什么并发查询一定要使用多个连接才能完成呢？因为DB一般是使用连接作为Session管理的基本单元。在一个连接中，SQL语句的执行必须是串行、同步的。这是由于对于每一个Session，DB都要维护一组状态来支持查询，比如事务隔离级别，当前Session的变量等。只有单Session内串行执行，才能维护查询的正确性（试想一下一组sql在不断的增减变量，然后这组sql乱序执行会发生什么）。维护这些状态需要耗费内存，同时也会消耗CPU和磁盘IO。这样，限制对DB的连接数，就是在限制对DB资源的消耗。

因此，对DB来说，关键是要限制连接的数目。这个要求无论是DB连接池还是NIO的连接管理都能做到。

这样问题就绕回来了，为什么DB连接不能放到IO多路复用里一并执行吗？为啥大家都用连接池？

答案是，可以用IO多路复用——但是**使用JDBC不行**。JDBC是一个出现了近20年的标准，它的设计核心是BIO（因为199X年时还没有别的IO可以用）：调用者在通过JDBC时执行比如query这样的API，在没有执行完成之前，整个调用线程被卡住。而类似于Mysql Connector/J这样的driver完备的实现了这套语义。

当然如果DB Client的协议的连接处理和解析稍微改一下：

1. 将IO模式调整为Non-Blocking，这样就可以挂到IO多路复用的内核上（select、epoll、kqueue……）
2. 在Non-Blocking实现的基础之上实现数据库协议的编码和解析

就可以实现用IO多路复用来访问DB。实际上很多其他语言/框架里都是这么干的。比如Nodejs，see [https://github.com/sidorares/node-mysql2](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttps%253A%252F%252Fgithub.com%252Fsidorares%252Fnode-mysql2)；或者Vert.X 的db客户端（[https://github.com/mauricio/postgresql-async](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttps%253A%252F%252Fgithub.com%252Fmauricio%252Fpostgresql-async)，不要在意这个名字，它实际上同时支持mysql和postgres）。只不过对于IO多路复用，数据库官方似乎都没做这种支持——他们只支持JDBC、ODBC等等这些标准协议。

那么为什么基于IO多路复用的实现不能成为默认的，官方的，而要成为偏门呢？

对于数据库开发者来说。这种用法在整体的用户里占有量非常小，所以也许不值当的花大力气。只需要把协议写清楚（比如[https://dev.mysql.com/doc/internals/en/client-server-protocol.html](https://link.zhihu.com/?target=https%3A//link.jianshu.com/%3Ft%3Dhttps%253A%252F%252Fdev.mysql.com%252Fdoc%252Finternals%252Fen%252Fclient-server-protocol.html)），就可以做实现。那么社区的有兴趣的人自然就可以去做。

另外一个原因是体系的支持。简单来讲，如果没有一个大的Reactive的运行环境，IO多路复用的使用会非常受限。

IO多路复用之所以能成立，是需要**整个程序要有一个IO多路复用的驱动代码**——就是select的那句调用都。整个程序必须以这个驱动代码为核心。这样就对整个代码的结构产生重大的影响。这种影响是没法用简单的接口抽象的。

Java Web容器之所以可以使用NIO是因为NIO可以被封装到容器内部。Web容器对外暴露的还是传统的多线程形式的Java EE接口。

如果DB和Web容器同时使用NIO，那么调用的DB连接库与必须与容器有一个约定描述**DB的连接管理如何接入Web容器的NIO的驱动代码**。在Java这个大环境下，不同人，不同的容器写的代码不同；又或者，不使用任何常见的容器，而是自己用NIO去封装一个。这样是无法形成代码上的约定的。那么多个独立的组件就不能很好的共享NIO的驱动代码。

上面这个用法假设整个程序应该共享一个NIO驱动代码。那么Web和DB可不可以各用各的呢？也是可以的，但是为了保证这两个NIO驱动代码不会相互block，最好要分开两个线程。这样一来就会打破一般Web服务一个请求处理用一个线程的一般做法，会让程序边的更复杂——你的业务代码和DB查询之间必须做跨线程数据交换。

相反，连接池的实现就相对独立的多，也简单的多。外界只要配好DB URL，用户名密码和连接池的容量参数，就可以做到自行管理连接。

而Nodejs和Vert.X是完全不同的。他们本质就是Reactive的。他们的NIO的驱动方式是其运行时的基础——所有要在这个基础上开发的代码都必须遵守同样的NIO+异步开发规范，使用同一个NIO的驱动。这样DB与NIO的协作就不成问题了。

最后，**有大量场景是需要BIO的DB查询支持的**。批处理数据分析代码都是这样的场景。这样的程序写成NIO就会得不偿失——代码不容易懂，也没有任何效率上的优势。类似于Nodejs这样的运行时在此场景下，反而要利用`async`或等价的语法来让代码看起来是同步的，这样才容易写。

总结一下。DB访问一般采用连接池这种现象是生态造成的。历史上的BIO+连接池的做法经过多年的发展，已经解决了主要的问题。在Java的大环境下，这个方案是非常靠谱的，成熟的。而基于IO多路复用的方式尽管在性能上可能有优势，但是其对整个程序的代码结构要求过多，过于复杂。当然，如果有特定的需要，希望使用IO多路复用管理DB连接，是完全可行的。

## 分布式事务中的最终一致具体应该如何实现？

我帮这个问题梳理一下逻辑。

分布式系统复杂业务的确需要“分布式事务”。2PC（XA），3PC是一路，因为它们实现了“Atomic Commit”。这里“Atomic”的意思是就是这个事情要不就都做了，要不就都没做，不存在做了一半的情况。生产中主要使用2PC（XA）。但这个方案有个严重的问题，只要事务管理器或者实际执行者有任何一方不工作，都可能会造成整体不工作，甚至是直接hang住了。此外这套需要XA协议的支持，并不是所有资源都支持XA。一些商业数据库和队列支持XA。而自己编写的服务接口没法用XA。此外常规的2PC实现的性能表现很差。

另外一个路数就是用TCC或者SAGA。TCC就是做2次操作：冻结 + 提交/取消。而SAGA是：执行 + 可能的回滚。这两种方式都不满足“Atomic Commit”。从数据库的角度来看，他们是多个事务，而不是一个整体。期间用户可能会有感觉。对于TCC，用户会有很短的时间发现自己的数据被冻住了。而SAGA下，用户会先发现某个业务被执行了，随后又回滚了（比如订单先下了，然后又自己撤销了）。他们都需要产品设计的配合，让这个事情显得不是很扎眼。但他们俩就是我们常规说的“最终一致”。

无论是2PC，TCC，还是SAGA，如果中间某个节在事务执行期间点挂了，恢复就会成问题。2PC在这一点上尤其严重。此外，正如题目中所说，如果一个服务挂了，那么就会连带着让上一层服务也不可用。解决的办法就是引入多个backup。在一个服务挂了之后就让其中某个backup顶上。如果服务本身无状态还相对容易，如果有状态就会很麻烦。因为必须保证所有backup的状态必须精确的和原来的服务一摸一样。保持一摸一样，就得让backup成为master的replica。

如果replica不能保证和master如完全一致，就不能解决上面的问题——实现一个带Fault Tolerant的分布式事务。为了解决这个问题就需要引入consensus算法。常规的做法是利用consensus算法为一个服务的多个实例作“leader”的选举，并以此为基础实现“state machine replication”。Consensus算法保证同一时刻只能有一个leader，如果leader挂了会重新选举，自动恢复系统 。这就避免了上面2PC难以恢复，需要人工介入的问题。

> 人工介入就意味着时间长+恢复时容易出错。谁运营谁知道。

而consensus算法本身需要实现“total order broadcast”。现实当中常用paxos（chubby等），zab （zookeeper）和raft（etcd等）实现。这个问题是分布式系统中最核心的问题。一些分布式存储，如mysql的group replication，tidb都采用这个思路实现。

但正确实现一个具有分布式一致性且Fault Tolerant的系统实在是太过于困难，中小公司一般也只能凑合。考虑到技术储备和资源怎么没法”放心“，但现实需求怼Fault Tolerant和Performance又是必须的。于是一般的做法就是凑合实现一个的分布式事务，再配合”人工对账“的形式来彻底封死错误。即使是有像TiDB这样的已经实现了分布式事务的数据库引入，谁又能担保100%不出问题呢？终归是double check下才能放心。换一个角度，如果凑合实现的分布式事务不那么靠谱，但成本极低。对账如果发现问题，就赔款。赔的钱远小过实现完备分布式一致性的成本，从业务角度也是蛮划算的（但要测算一下大概会赔多少，再做决策）。

如果是跨公司（如一个电商公司和一个支付渠道公司）的协作，技术对接都很困难，对账这种看起来有点糙但管用的办法就更加必不可少了。

## Java中线程同步锁和互斥锁有啥区别？

不要钻概念牛角尖。这样没意义。

也许java语法层面包装成了sycnchronized或者明确的XXXLock，但是道理都是一样的。无非就是哪种写起来方便。

锁的目的就是避免多个线程对同一个共享的数据并发修改带来的数据混乱。

锁的实现要处理的大概就只有这4个问题：

* “**谁拿到了锁“这个信息存哪里**（可以是当前class，当前instance的markword，还可以是某个具体的Lock的实例）
* **谁能抢到锁的规则**（只能一个人抢到 - Mutex；能抢有限多个数量 - Semaphore；自己可以反复抢 - 重入锁；读可以反复抢到但是写独占 - 读写锁……）
* **抢不到时怎么办**（抢不到玩命抢；抢不到暂时睡着，等一段时间再试/等通知再试；或者二者的结合，先玩命抢几次，还没抢到就睡着）
* **如果锁被释放了还有其他等待锁的怎么办**（不管，让等的线程通过超时机制自己抢；按照一定规则通知某一个等待的线程；通知所有线程唤醒他们，让他们一起抢……）

有了这些选择，你就可以按照业务需求组装出你需要锁。

———— 更新一下————

关于“互斥”和“同步”的概念[@chen Kingwen](//www.zhihu.com/people/ae923638b12a18db2952e9200f8f2493) 的答案很清楚了。

* 互斥就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了
* 同步就是ABCD这些线程要约定一个执行的协调顺序。比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。

这是两种典型的并发问题。恰当的使用锁，可以解决同步或者互斥的问题。

你可以说Mutex是专门被设计来解决互斥的；Barrier，Semaphore是专门来解决同步的。但是这些都离不开上述对上述4个问题的处理。同时，如果遇到了其他的具体的并发问题，你也可以定制一个锁来满足需要。

## 动态规划和递归之间的关系是什么？

DP说白了就是一个解决问题的思路——即一个大一点规模的问题可以被拆解为更小的，更容易解决的问题。

拿最简单的斐波那契问题举例子，一个大的问题f\(n\)可以被拆解为小一点的问题f\(n-1\)和f\(n-2\)，……直到然后拆到最小的问题f\(1\)和f\(2\)。你可以从f\(n\)从大往小了算，也可以先从f\(1\), f\(2\), f\(3\)……往大了算。再比如leetcode的有个正则表达式匹配的问题，你可以把问题看做是一个大的字符串的匹配pattern，拆解为字符串的一部分匹配pattern的一部分的问题；也可以反过来先匹配一小部分，再不断让可以匹配的范围变大。

很多人把从大往小算的形式称作递归，反过来从小往大了算称为DP。但实际上只要满足大规模问题可以拆解为小规模问题，这个思路本身就是DP的，无非是顺序不一样罢了。

所谓【动态规划】就是指知道了一组小规模问题的答案后，就可以用一个方案（状态转移方程）组装成大一点规模问题的答案的做法而已。为啥叫“动态”呢，因为状态转移会和几个条件相关，而不是一开始就可以无脑写死（无脑写死的一般就是贪婪）。

术语上，从大了往小算被称为”自顶向下“，而从小往大算被称为”自底向上“。尽管自顶向下和自底向上等价，但自顶向下算一个很容易发生的问题就是重复计算，比如你为了算f\(5\)，普通的递归方式要重复计算很多很多次f\(3\), f\(2\)……。这些计算都是浪费的，实际表现比自底向上差的多（但再强调下，二者的思路没本质差别，仅仅是计算浪费不浪费的问题）。但你可以加cache，每次递归的函数的参数都可以组装成一个cache的key。这样每次递归时，可以先检查一下cache是不是有，有了就不用算了，直接返回。这种带cache的递归DP一般被称为“记忆化搜索“。记忆化搜索与自顶向上的DP计算方式在算法复杂度上理论上是一样的。但cache一般会用map，其实际的复杂度要比直接用数组的自顶向上要大，二者的O\(1\)是不一样的。此外记忆化搜索还会引入函数调用的开销，所以一般记忆化搜索比等价的自底向上要慢那么一丢丢。

此外自底向上的计算方式还可以优化存储，比如斐波那契计算时，计算f\(n\)只需要f\(n-1\), f\(n-2\)，所以用两个变量就行，无须把所有的结果都记录下来。用go写大概这样：

```text
func fib(n int) int {
  f0, f1 := 0, 1
  for n > 1 {
    f1, f0 = f0 + f1, f1
    n--
  }
  return f1
}
```

但用记忆化搜索的方式，你是没法在计算得到f\(4\)后，把f\(1\)结果占用内存给方便的清理掉的。

DP最难的不在于自底向上还是自顶向下，而在于怎么拆问题。某些问题，如果之前没做过，你是很难想得到“这个问题还能这么拆”。不信？可以去看看LeetCode有一道“戳气球”的题目。我自己第一次看时虽然能猜到它是用DP，但打死都想不出来怎么拆，直到看了题解。因此大家想不出来DP，不是因为不知道DP的代码框架怎么做，而是拆解问题的思路实在太古怪，太反常识。这就需要刷题去多多适应。

> But，这些反常识的方法也是人想出来的。此时也是去了解那些聪明人的脑洞的好机会：）。你也许会惊讶于人思维方式的差异的巨大。

在现实工程中，有些问题即便是可以DP，但因为拆解方案对于大部分人来说太难想；或者即使想出来，应用面太窄，需求小小的一个变化就会让整套方案不灵了。因此很多时候，如果复杂度的要求不那么严苛，还是用暴力要好一些。因此DP更常见于面试题里，证明候选人“知道像计算机专业从业者那样思考”。但这并不是说DP不重要，在计算机科学里，这个思路非常非常重要，只不过工程上不光要考虑算法复杂度，还要考虑可维护性问题而已。如果场景恰当，当然还是要用。比方说，我们之前做的“计算基金最大回撤”就用DP，实际上就是LeetCode里“买卖股票最佳时机”解题方案的镜像——就是一个算回撤，一个算正向收益的区别。

回到DP本身，题主不妨试试这样做。既然题主的思维方式觉得自顶向下很舒服，那就总是先用递归实现自顶向下DP。通过后加上Cache用记忆化搜索的方式写代码。再改写成等价的自顶向上的做法。反复做几次看看思路上能不能贯通。但同时，我也遇到过写自底向上很舒服，反过来就别扭的人。

当然，虽然二者等价，我依然遇到过很难自底向上写的问题。此时一般是很难找到一个很简单的从小到大的规律，这就不如用递归+cache。cache的规则就很简单，算过了就cache，没算过就没cache，递归调用中参数出现的规律不需要操心。

同时，也有反过来的情况，自底向上很难写。主要是这类问题的递归方式相对好理解，但自底向上时就要让dp\[i\]\[j\]表达一些比较模糊，很难说清楚的概念，让编码变得很别扭。所以遇到这种情况就用容易理解的形式写就好。

最后，递归能够在DP这个思路里支持实现自顶向下，但也不一定就用在DP里。递归适合的场景要更加广泛。比如dfs/bfs这样的搜索场景就可以用递归做。递归还可以用来做模拟，比如A对B做一件事、B又要对C和D做一件事，D又要对A和B做一件事……。用递归来实现很直观。

## IO多路复用和线程池哪个效率更高，更有优势？

问题本身貌似有问题。简单比较两种工具的优劣意义不大。你没法说锤子和剪刀那个更好。我们一般会评价对于某个场景，哪种工具更合适。

io多路复用（这翻译真的很坑爹啊），指的是同一个进（线）程可以处理多个IO数据流。

多线程+池模型指的是每个线程处理一个IO流。

IO多路复用的优势在于，当处理的消耗对比IO几乎可以忽略不计时，可以处理大量的并发IO，而不用消耗太多CPU/内存。这就像是一个工作很高效的人，手上一个todo list，他高效的依次处理每个任务。这比每个任务单独安排一个人要节省（雇人是要发工资的……）。典型的例子是nginx做代理，代理的转发逻辑相对比较简单直接，那么IO多路复用很适合。相反，如果是一个做复杂计算的场景，计算本身可能是个 指数复杂度的东西，IO不是瓶颈。那么怎么充分利用CPU或者显卡的核心多干活才是关键。

此外，IO多路复用适合处理很多闲置的IO，因为IO socket的数量的增加并不会带来进（线）程数的增加，也就不会带来stack内存，内核对象，切换时间的损耗。因此像长链接做通知的场景非常适合。

IO多路复用 + 单进（线）程有个额外的好处，就不会有并发编程的各种坑问题，比如在nginx里，redis里，编程实现都会很简单很多。编程中处理并发冲突和一致性，原子性问题真的是很难，极易出错。

> 但是现实中，也有IO多路复用 + 多worker线程的做法，这样上面这个好处就没有了。

如果做不到“处理过程相对于IO可以忽略不计”，IO多路复用的并不一定比线程池方案更好。比如一个web的服务，用jetty 9的NIO connector，后边是spring svc + JDBC连接数据库。spring svc + JDBC连接数据库这两块的处理延迟相对于NIO来说不能忽略，所以并不能指望用jetty 9的NIO connector换了之前的BIO connector的容器，性能能高不少（实际上应该会高一些，但不会太夸张，毕竟瓶颈在后边处理和DB上）。

顺便提一句，Java世界里，因为JDBC这个东西是BIO的，所以在我们常见的Java服务里没有办法做到全部NIO化，必须得弄成多线程模型。如果要在做Java web服务这个大场景下享受IO多路复用的好处，要不就是不需要DB的，要不就是得用Vert.X一类的纯NIO框架把DB IO访问也得框进来。

最后，如果IO压力过大，一个高并发的东西和一个不那么高并发的东西，都不能正确响应，对用户来说是一样的——**就是不能用**。假如IO非常的繁重，没有空闲的连接，那么IO的压力在两种模型下表现差不多，IO多路复用的“并发“看了起来会大一些，但因为IO已经满了，所以表现出超时严重；而线程池可能表现为，所有的线程都因为IO过慢而卡死了，线程池耗尽，新的请求进不来直接报错。但不管哪一种，在极端压力下，都无法正常工作。这时，要想着怎么扩容。

简单总结一下，

* 如果压力不是很大，并且处理性能相对于IO可以忽略不计
  * IO多路复用+单进（线）程比较省资源
  * 适合处理大量的闲置的IO
  * IO多路复用+多单进（线）程与线程池方案相比有好处，但是并不会有太大的优势
* 如果压力很大，什么方案都得跪，这时就得扩容。当然因为IO多路复用+单进（线）程比较省资源，所以扩容时能省钱。

## 为什么一般操作系统中应用程序的栈空间都要设最大值，不支持动态扩展？

看下面的图：

![](https://pic1.zhimg.com/50/v2-957f4c83e14d294b9285b54ff55a02fb_hd.jpg?source=1940ef5c)

题主之所以能问出这个问题，也许是因为大多数内存的理论模型都会画成左边那样子。那么自然而然的Heap可以不断往上扩展；Stack可以往下扩展。

但现实中是  
1. 主流操作系统都是有多线程支持的，而多线程需要每个线程分配一个独立的Stack，每个Stack内部可以满足“向下增长“，但是必须要有个界限，不然没法实现了。否则下个Stack从哪开始呢？

2. heap和mmap segment的存在。mmap是有很多用途的。比如

* 加载一个so动态库，是以mmap的形式映射到内存里，再让CPU执行上面的指令的；每个程序都会加载大量的动态库。
* 我们编程意义上的heap实际上是操作系统级别heap和mmap区域的混合。当分配一大块内存时，操作系统可能会决定不从heap里切，而是独立分配一块mmap区域来用。
* mmap自己也可以被用户直接使用，比如映射一个文件，或者做内存的数据共享。

上图中之所以把heap，mmap segement和stack画成一个颜色，是因为他们本质上差不太多，都是程序运行时动态分配和调整的。stack和mmap都是“一块内存”，因此实现中并不一定非得是Stack永远比mmap的地址数值更大。只要不重叠就行了。Linux本身的api也允许创建新线程时，指定一段内存作为“Stack”，而这个内存自然也需要通过malloc得到，这又回到了mmap/heap上了。

回到应用层面，巨大的Stack除了应付非常深的递归之外没有什么太大的用处。而非常深的递归一般就是程序哪里写bug了。为了不太有用的场景去做设计并不是理智的行为。

我们常规意义上说“Stack”实际只是在用CPU对一段内存的地址做指令寄存器的push和pop而已。如果这不够用你可以自行定制一个喜欢的形式来实现对Stack的管理。有些语言可以把Stack的管理玩出花。比如go，自己实现了对go routine的管理，每个routine的Stack都可以不是连续的，这样既避免浪费，又能轻松扩展。

## 各位都是怎么进行单元测试的？

首先我先梳理下概念。因为比如像“单元测试”这个词在不同场景下可能有不同意思。我更偏向于将大家大致理解的“单元测试”称为“**开发阶段的自动化测试**“。这样就可以体现三个点：

* 一是，这个**测试是开发同学自己搞的**。开发人员搞定代码后，为新代码添加测试，并且保证新老测试都能通过，**才能提测**。开发同学不能将满是bug的代码就丢给测试同学。
* 二是，这个**测试是自动化的**，不是“手工调用接口做一次测试“那种形式的。并且执行起来要非常简单。比如任何人拿到代码，只要机器上装了对应的开发工具，都可以轻松运行测试，看到报告（比如对于java，一句mvn test, 或者对nodejs，一句npm test就能跑）。
* 三是，开发人员要做的测试，真的不一定是只验证一个“单元”的正确性（下面详细讲）

市面上大部分关于这种测试的介绍，大多集中在某个工具或者框架怎么用（比如JUnit，mockito，jest等）；要不就是一些理论上的对这种测试好处的介绍，通篇是“应该做测试”，“这么做了就容易得到高内聚的代码“等等。但是很少有关于**真正为什么这个事情难以落地的讨论**。这也是我很想多唠叨两句的地方。

最郁闷的是很多人一开始写测试代码就错了，因为压根就没搞明白要测试什么，保证什么东西是对的。这个无论什么工具都不帮不了你，只能由开发者自己思考——对于某个特定的测试代码，花那么大力气去写，去mock，去assert，到底想验证什么。一个不留神，测试代码就变成验证比如JPA的接口对不对，某个第三方系统库的函数对不对等等。这明显不是聪明的做法。

最常见的测试目标是是**验证“自己写的一小段代码是不是符合设计逻辑的“**。这个”一小段代码“就是我们经常说的“单元”。“单元测试”的本来意思也源自于此。比如，你写了个快排函数qsort，想验证下各种序列输入进去是不是就能得到排好序的结果。因为快排的逻辑稍微有一丢丢绕脑，觉得一遍写下来没有信心。于是就用N多的case去验证这个函数的输出是否总是符合预期。

这样的测试是最简单的，因为要测的东西几乎没有复杂的依赖。这也是大家经常说的“纯函数”好测的原因——函数的输入就是其所有可见的上下文了。对于一个纯函数，开发者者很容易构造其上下文。

很多文章介绍测试都只拿这种“纯函数”的测试举例子，因此大家看过之后就照猫画虎到实际项目写测试代码了，结果就是很难写出来，或者写出来起不到验证的作用，极度打击自信心，然后慢慢产生了“测试没啥用”或者“测试就应该只让测试工程师负责的“不正确的三观。

我们正常写的代码可能是业务逻辑，是数据分析的job代码，也有可能是前端构造界面的代码等。这种代码都会有依赖的。由于mock工具的存在，在做“单元测试”的时候，应该尽量保证除待验证的代码之外，所有的不纯的依赖都要尽量mock。

有些依赖特别麻烦。比如一个例子是，验证一个下单打折的函数是否能计算出正确的折扣。这个函数需要读取数据库里的折扣数据才能做计算。这就引入一个依赖。**而对依赖的处理方式的拿捏才是测试里最难的地方。**

比如，你可以直接mock掉整个DiscountDAO。这样就可以避免真的读取数据库。这时，你的假设是“**你觉得读取折扣的SQL本身是肯定没错的，因此你不验证它，你只验证读到折扣数据后，根据输入金额得到折扣金额的逻辑是正确的**“。

你也可以用embedded数据库, h2，maria4j之类的mock数据库。但如果你生产用的是mysql/postgres等，那么你在假设“对于这一段要验证的SQL，用embedded db和用生产db是等价的“。这有可能成立，有可能不成立。但也许代码里用到了私有SQL语法，只有生产数据库支持。如果发生，这样的验证就做不了。

你还可以用真的生产DB做验证。但首先，这里测试的目标已经变了，从**“验证计算折扣的逻辑”**变成“**验证计算折扣这个功能是否正确**“。这是已经不是“单元”了，而是一个函数 + 一句SQL执行 + DB功能正确的**“集成”测试**当然，这个例子里是个很局部的集成。不过也是几个组件一起测试才能实现这个验证。这样的测试代价是必须部署一个真的数据库，还要准备数据和后门。编写代价相对就比较高了。

开发也可能做（局部）“集成测试”，所以这就是另外一个我觉得应该叫“开发阶段自动化测试“，而不仅仅是“单元测试”的原因。“单元测试“容易让人引起一个误解——开发人员只应该管一小段代码是否正确，却不用管任何集成测试。这明显是**不一定**正确的。比起“单元测试”这个概念本身，我更在意的是**开发人员为了保证代码质量应该怎样做才对**。

那么到底要不要做这种“局部集成测试呢“。这要看情况。比如：

* 你的目标就是要测“一小段代码是不是正确”。你可以很有信心的保证其他依赖的正确性都能保证。那明显，这时就不用花精力做集成了。怎么简单怎么来。这样的测试甚至都不需要启动Spring这类框架，运行速度会很快。
* 你在开发一个小的lib。这个lib就很纯，没有任何复杂的依赖，那么单元测试就足够了。
* 如果团队已经安排了专人做这块的集成测试，开发人员就没必要做重复劳动了。如果这块测试的不好，应该优先去和那个测试同学沟通，看看怎么改善。沟通无效，在manager知情和同意下，再自己补。
* 如果开发自己做集成可以更容易构造全集成测试不太容易构造的例子，那么还是自己集成测试一下比较好。性价比高。
* 如果是要测试一个端到端的接口返回正确，那么唯一的办法就是集成测试——真的启动server，使用真的数据库、Redis、队列……，做端到端的测试。这时也许docker可以帮助你一键启动全套环境。
* 如果一个测试涉及到依赖的核心功能，也必须得做集成测试。比如要测试一个Exception是不是会让当前事务真的回滚，同时发生的其他事务因为隔离级别不会受到影响，那么你必须引入真的，和生产一模一样的支持事务的数据库才行。
* 如果是前端测试，基本上也必须得做集成测试。就算可以mock掉所有的后端接口，也得引入浏览器或者App框架才能测试。
* 如果整个公司没有专门的测试岗位，都是“研发”，那么不管局部集成还是全部集成，都得由开发者自己上。
* ……

所以开发的同学在写测试的之前，最好先再三确认自己到底要验证什么。至于用什么工具，如何看待覆盖率，如何设计模块和函数，以及后门，让程序“容易测”都是在目标之下的要讨论的具体问题。

还有个问题是什么时候应该写测试。是不是项目一开始就得同时写测试，甚至是TDD？

Well，这也是个Case by Case的问题。单元测试也好，局部集成测试也好，都是有代价的。我的经验是，有效的开发阶段测试会占用掉开发者30%左右的精力。当然，追求技术极致的人会觉得写测试天经地义，不写才是垃圾。30%的精力也是开发的一部分，不带讨价还价的。如果覆盖率不到90%+就是不称职等等。

我觉得不应该走极端，应该同时看到测试的成本和收益。比如在创业公司早期，就1～2个人的时候，让业务赶紧上线试错找对方向是最重要的。今天的设计和代码说不定几周后就大变样，甚至就直接丢掉了。此时功能简陋，用户量也少，小问题是可以忍的。此时团队会通过手工测试和实际使用来不断的发现和修复问题（所以这时候运行时错误log抓取 + 报警一定要做好啊）。这时强调必须完美覆盖的测试是不合时宜的。这个阶段，应该着重全集成测试（通常由一个专门的测试工程师主持），以及把那些局部的“一旦出问题公司就完蛋”的逻辑加好自动化测试。

写一个新东西，有的时候一开始想不清楚设计，类、接口、模块的划分可能都不是很完美，这时写一大堆测试基本上是会废掉的。随着不断的迭代，代码的组织才能慢慢优化和清晰，这些稳固的代码产生的接口、模型才有被测试保护的意义。

但是反过来，如果一开始图快，不写测试，随着业务慢慢稳定，团队人数和用户量慢慢多起来后，习惯性一直不写测试也是不对的。这时对代码的修改会陷入巨大的痛苦和不安中。天晓得改了一处会不会引发其他不相干代码的问题。这时，任何被心灵摧残的开发也会主动想着怎么提高代码质量的。

因此，需要找一个时间点开始，一点一点的把最重要的、稳固的逻辑的测试补充上。我的经验是一个项目代码在不断迭代大概10个月后，或者团队技术人员已经超过5个人，或者活跃用户已经超过1万人，就必须有测试保护。如果用动态语言（js，python等）编写程序，因为没有编译器的保护，就更要写测试。

但是最后要特别强调下，搭建有效的自动化测试的工作量并不小。一提起测试，很多人就只想到TestCase。但实际上为了让TestCase发挥作用，让测试容易运行，可以隔离，可重复，稳定的跑，需要学习和实施的东西非常多，甚至不亚于功能开发本身。对测试工作量的低估最终会导致让这个工作不了了之，变成面子工程。如果你是一个技术lead，想带领团队写测试，但以为“就是很小的工作量，随便写一下就会有效果”，那么必然会吃瘪。

最后，我个人对【开发阶段自动化测试】的要求总结下来是：

* 确定测试的目标，到底想验证什么
* 基于这个目标，找到和维护需要的工具，比如Runner，Mock，覆盖率统计工具等，Embedded数据库等
* 留足给测试的时间，并通过code review的手段来保证写有效的测试
* 给一些典型的场景如何做测试写一写文档，积累经验（比如如何测试要模拟时间的案例？）
* 统计测试同学给开发同学报bug的数据，盯紧代码质量不高的同学，多做沟通
* 根据出现bug的数量和scope来推动部分关键代码的测试质量的改善
* 在能达成测试目标的前提下，看看能够整合一些工具，降低维护测试依赖的成本

以及我希望大家写的时候有一个预期：

**【开发阶段自动化测试】是所有测试体系里的一个部分，而非全部，不能解决所有问题**。**不要觉得写自动化测试的Case，让覆盖率100%就能保证系统绝对不出错。这是妄想，死了这条心。**

P.S.

也许是因为我所处的环境的缘故，我不支持TDD。因为在我处的领域，基本上不太可能出现一开始就能把设计做得很完备的情况。一个功能迭代3期就会面目全非了。

此外TDD会导致一个趋向——面向功能开发，而非面向抽象开发。一个系统的核心是对其需求进行建模后的抽象。抽象变了，你就不能指望之前写好的接口会稳定。从系统的维护角度，长期看，要保持抽象迭代，总是能顺畅的反应业务模型最重要。

## Shell 是用来解决什么问题的？

操作系统对外提供的接口是“系统调用”，也就是一堆编程用的接口。这些接口一般以C函数的形式暴露给使用者。通过这些接口，开发者可以命令操作系统“启动一个进程”，“查找某个目录下的所有文件”，“将某个文件的权限配置为744”等等。

> 实际上我们平时编程用的是对系统调用的包装，比如libc里的那些库函数。但无论如何，你总是得写代码才能使用它们。

问题是我们平时使用电脑，不能每次都编写程序，再编译，再运行得到结果吧。比如你想知道一个目录下的所有文件，你肯定不会去写一段C代码，调用系统调用“readdir” （见[http://www.man7.org/linux/man-pages/man3/readdir.3.html](https://link.zhihu.com/?target=http%3A//www.man7.org/linux/man-pages/man3/readdir.3.html)），然后gcc编译，然后运行。真这么干，一个最简单的工作也要耗费很长时间。况且，一个函数的返回是“数据结构”，或者输出到stdout或文件之类的地方。你总得以某种形式把结果“画”到界面上（不管是画字符还是画图片），才能查看那个结果。这个格式化输出的工作量也很大。

正常人的思路是先写好程序，然后弄个交互的界面方便使用这个程序。只要使用者敲一组字符串，就可以调用之前写好的程序完成工作。比如我们会在命令行里输入“ls -Rl”这种字符串。这个字符串被翻译成“ls”，“-R”，“-l”。“ls”帮我们找到那个之前写好的程序，并启动它；“-R”和“-l”被作为参数传给这个程序，告诉程序走“递归所有子目录”+“输出长格式”这部分代码。

这个负责把用户输入的字符串转换到需要执行的程序，并把结果以某个形式画出来的东西，就叫做“Shell”，即帮你更方便使用操作系统接口的“壳”。这个词与操作系统内核（Kernel）对应。

在Linux中，bash负责按照某种格式把用户的输出的字符串翻译，比如对于普通非空字符翻译为程序和参数，并尝试去PATH里找对应的程序；对“空格”翻译成分隔符；对“$XXX”尝试进行环境变量的替换；对“｜”翻译为管道；对 “&gt;”翻译为输出重定向；对一个指令末尾的“&”翻译为将程序转到后台执行……

另一方面终端将stdout、stderr输出的东西画成我们可以看的一坨坨字符，包括字符、字体、颜色、换行、甚至响铃。

【bash】 + 【终端】大概可以理解为一个以字符为交互方式的“Shell”。

![](https://pic2.zhimg.com/50/v2-32ba904a3c6311d39ff134e17e5b6c72_hd.jpg?source=1940ef5c)

> 当然，大家会更习惯上说“bash”是一个“Shell”。也许是因为bash大家用的不爽可以换，可以改变配置，可以编程。总之玩法很多。但是对终端一般不怎么理会，默认的就够用了。或者也可以理解为“终端“属于操作系统的一部分。不管如何，可以习惯这些不同角度的不同看法，不需要特别细纠，理解大概意思就好。

除了bash，还有像csh，zsh等，虽然各自的规定有些区别，但干的事情就是这些。Windows里一般等价的就是cmd，与Linux这边相比弱鸡得很。不过Windows有自己的PowerShell，也可以通过cygwin或者linux subsystem的方式使用bash。

如果你自己定义一个解析字符串的“Shell”，你可以规定用分号而不是空格做分隔符；用\#而不是$代表变量……

Shell也可以做成图形界面的，比如你点击一个窗口里左边的一个目录图标，右边就展示这个目录下的所有文件。他做的事情和上面ls一模一样，只不过接受的是点击，而不是输入字符串；输出的是一幅画，而不是格式化的文本而已。内部还是转换成调用系统调用来完成工作。

![](https://pic3.zhimg.com/50/v2-5586062636423871b711ba5af151ef27_hd.jpg?source=1940ef5c)

这个是不是看上去很眼熟

如果你开心，你甚至都把Shell可以做成声控的，比如你说一段语音，你的“Shell”识别后翻译为调用程序的指令，执行成功后就可以把结果翻译为语音再返回给你。你一定熟悉“小爱同学”，“Hi Siri”这种东西吧。

Shell的思想很普遍，并不一定限制在操作系统上。比如你自己写一个程序，有大量复杂的参数和配置。为了使用方便可以写个命令行工具将一个命令翻译成对这段程序的调用。你写的命令行工具就是你自己程序的“Shell”。比如写Java的同学肯定很熟悉mvn。一句mvn install可以产生出成百上千个下载、压缩、编译、清理、测试、上传等api的调用；使用数据库的同学也会用SQL来表达自己的查询，让数据库的“Shell”解释成对存储引擎各种api的调用。

