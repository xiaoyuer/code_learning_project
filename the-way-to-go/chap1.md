# Da-Kuan-Kuan

链接：https://www.zhihu.com/question/370830450/answer/1010322452

## golang为什么将method写在类外?

go本质上反对那种OOP做法的。从go的哲学来讲，简单的东西才能写正确，好理解和维护。因此在go中并不提供OOP的“类”，而仅仅提供structure和属于structure的方法。

于是属于structure的方法可以设定自己到底是希望要\(t T\)还是\(t \*T\) ，以决定自己到底要不要复制一份数据。

go表达的就是函数就是函数，数据就是数据。与数据绑定的函数提供t.foo\(\)这种写法。但也仅此而已了。

## 分布式事务中的最终一致具体应该如何实现？

我帮这个问题梳理一下逻辑。

分布式系统复杂业务的确需要“分布式事务”。2PC（XA），3PC是一路，因为它们实现了“Atomic Commit”。这里“Atomic”的意思是就是这个事情要不就都做了，要不就都没做，不存在做了一半的情况。生产中主要使用2PC（XA）。但这个方案有个严重的问题，只要事务管理器或者实际执行者有任何一方不工作，都可能会造成整体不工作，甚至是直接hang住了。此外这套需要XA协议的支持，并不是所有资源都支持XA。一些商业数据库和队列支持XA。而自己编写的服务接口没法用XA。此外常规的2PC实现的性能表现很差。

另外一个路数就是用TCC或者SAGA。TCC就是做2次操作：冻结 + 提交/取消。而SAGA是：执行 + 可能的回滚。这两种方式都不满足“Atomic Commit”。从数据库的角度来看，他们是多个事务，而不是一个整体。期间用户可能会有感觉。对于TCC，用户会有很短的时间发现自己的数据被冻住了。而SAGA下，用户会先发现某个业务被执行了，随后又回滚了（比如订单先下了，然后又自己撤销了）。他们都需要产品设计的配合，让这个事情显得不是很扎眼。但他们俩就是我们常规说的“最终一致”。

无论是2PC，TCC，还是SAGA，如果中间某个节在事务执行期间点挂了，恢复就会成问题。2PC在这一点上尤其严重。此外，正如题目中所说，如果一个服务挂了，那么就会连带着让上一层服务也不可用。解决的办法就是引入多个backup。在一个服务挂了之后就让其中某个backup顶上。如果服务本身无状态还相对容易，如果有状态就会很麻烦。因为必须保证所有backup的状态必须精确的和原来的服务一摸一样。保持一摸一样，就得让backup成为master的replica。

如果replica不能保证和master如完全一致，就不能解决上面的问题——实现一个带Fault Tolerant的分布式事务。为了解决这个问题就需要引入consensus算法。常规的做法是利用consensus算法为一个服务的多个实例作“leader”的选举，并以此为基础实现“state machine replication”。Consensus算法保证同一时刻只能有一个leader，如果leader挂了会重新选举，自动恢复系统 。这就避免了上面2PC难以恢复，需要人工介入的问题。

> 人工介入就意味着时间长+恢复时容易出错。谁运营谁知道。

而consensus算法本身需要实现“total order broadcast”。现实当中常用paxos（chubby等），zab （zookeeper）和raft（etcd等）实现。这个问题是分布式系统中最核心的问题。一些分布式存储，如mysql的group replication，tidb都采用这个思路实现。

但正确实现一个具有分布式一致性且Fault Tolerant的系统实在是太过于困难，中小公司一般也只能凑合。考虑到技术储备和资源怎么没法”放心“，但现实需求怼Fault Tolerant和Performance又是必须的。于是一般的做法就是凑合实现一个的分布式事务，再配合”人工对账“的形式来彻底封死错误。即使是有像TiDB这样的已经实现了分布式事务的数据库引入，谁又能担保100%不出问题呢？终归是double check下才能放心。换一个角度，如果凑合实现的分布式事务不那么靠谱，但成本极低。对账如果发现问题，就赔款。赔的钱远小过实现完备分布式一致性的成本，从业务角度也是蛮划算的（但要测算一下大概会赔多少，再做决策）。

如果是跨公司（如一个电商公司和一个支付渠道公司）的协作，技术对接都很困难，对账这种看起来有点糙但管用的办法就更加必不可少了。

## Java中线程同步锁和互斥锁有啥区别？

不要钻概念牛角尖。这样没意义。

也许java语法层面包装成了sycnchronized或者明确的XXXLock，但是道理都是一样的。无非就是哪种写起来方便。

锁的目的就是避免多个线程对同一个共享的数据并发修改带来的数据混乱。

锁的实现要处理的大概就只有这4个问题：

* “**谁拿到了锁“这个信息存哪里**（可以是当前class，当前instance的markword，还可以是某个具体的Lock的实例）
* **谁能抢到锁的规则**（只能一个人抢到 - Mutex；能抢有限多个数量 - Semaphore；自己可以反复抢 - 重入锁；读可以反复抢到但是写独占 - 读写锁……）
* **抢不到时怎么办**（抢不到玩命抢；抢不到暂时睡着，等一段时间再试/等通知再试；或者二者的结合，先玩命抢几次，还没抢到就睡着）
* **如果锁被释放了还有其他等待锁的怎么办**（不管，让等的线程通过超时机制自己抢；按照一定规则通知某一个等待的线程；通知所有线程唤醒他们，让他们一起抢……）

有了这些选择，你就可以按照业务需求组装出你需要锁。

———— 更新一下————

关于“互斥”和“同步”的概念[@chen Kingwen](//www.zhihu.com/people/ae923638b12a18db2952e9200f8f2493) 的答案很清楚了。

* 互斥就是线程A访问了一组数据，线程BCD就不能同时访问这些数据，直到A停止访问了
* 同步就是ABCD这些线程要约定一个执行的协调顺序。比如D要执行，B和C必须都得做完，而B和C要开始，A必须先得做完。

这是两种典型的并发问题。恰当的使用锁，可以解决同步或者互斥的问题。

你可以说Mutex是专门被设计来解决互斥的；Barrier，Semaphore是专门来解决同步的。但是这些都离不开上述对上述4个问题的处理。同时，如果遇到了其他的具体的并发问题，你也可以定制一个锁来满足需要。

## 动态规划和递归之间的关系是什么？

DP说白了就是一个解决问题的思路——即一个大一点规模的问题可以被拆解为更小的，更容易解决的问题。

拿最简单的斐波那契问题举例子，一个大的问题f\(n\)可以被拆解为小一点的问题f\(n-1\)和f\(n-2\)，……直到然后拆到最小的问题f\(1\)和f\(2\)。你可以从f\(n\)从大往小了算，也可以先从f\(1\), f\(2\), f\(3\)……往大了算。再比如leetcode的有个正则表达式匹配的问题，你可以把问题看做是一个大的字符串的匹配pattern，拆解为字符串的一部分匹配pattern的一部分的问题；也可以反过来先匹配一小部分，再不断让可以匹配的范围变大。

很多人把从大往小算的形式称作递归，反过来从小往大了算称为DP。但实际上只要满足大规模问题可以拆解为小规模问题，这个思路本身就是DP的，无非是顺序不一样罢了。

所谓【动态规划】就是指知道了一组小规模问题的答案后，就可以用一个方案（状态转移方程）组装成大一点规模问题的答案的做法而已。为啥叫“动态”呢，因为状态转移会和几个条件相关，而不是一开始就可以无脑写死（无脑写死的一般就是贪婪）。

术语上，从大了往小算被称为”自顶向下“，而从小往大算被称为”自底向上“。尽管自顶向下和自底向上等价，但自顶向下算一个很容易发生的问题就是重复计算，比如你为了算f\(5\)，普通的递归方式要重复计算很多很多次f\(3\), f\(2\)……。这些计算都是浪费的，实际表现比自底向上差的多（但再强调下，二者的思路没本质差别，仅仅是计算浪费不浪费的问题）。但你可以加cache，每次递归的函数的参数都可以组装成一个cache的key。这样每次递归时，可以先检查一下cache是不是有，有了就不用算了，直接返回。这种带cache的递归DP一般被称为“记忆化搜索“。记忆化搜索与自顶向上的DP计算方式在算法复杂度上理论上是一样的。但cache一般会用map，其实际的复杂度要比直接用数组的自顶向上要大，二者的O\(1\)是不一样的。此外记忆化搜索还会引入函数调用的开销，所以一般记忆化搜索比等价的自底向上要慢那么一丢丢。

此外自底向上的计算方式还可以优化存储，比如斐波那契计算时，计算f\(n\)只需要f\(n-1\), f\(n-2\)，所以用两个变量就行，无须把所有的结果都记录下来。用go写大概这样：

```text
func fib(n int) int {
  f0, f1 := 0, 1
  for n > 1 {
    f1, f0 = f0 + f1, f1
    n--
  }
  return f1
}
```

但用记忆化搜索的方式，你是没法在计算得到f\(4\)后，把f\(1\)结果占用内存给方便的清理掉的。

DP最难的不在于自底向上还是自顶向下，而在于怎么拆问题。某些问题，如果之前没做过，你是很难想得到“这个问题还能这么拆”。不信？可以去看看LeetCode有一道“戳气球”的题目。我自己第一次看时虽然能猜到它是用DP，但打死都想不出来怎么拆，直到看了题解。因此大家想不出来DP，不是因为不知道DP的代码框架怎么做，而是拆解问题的思路实在太古怪，太反常识。这就需要刷题去多多适应。

> But，这些反常识的方法也是人想出来的。此时也是去了解那些聪明人的脑洞的好机会：）。你也许会惊讶于人思维方式的差异的巨大。

在现实工程中，有些问题即便是可以DP，但因为拆解方案对于大部分人来说太难想；或者即使想出来，应用面太窄，需求小小的一个变化就会让整套方案不灵了。因此很多时候，如果复杂度的要求不那么严苛，还是用暴力要好一些。因此DP更常见于面试题里，证明候选人“知道像计算机专业从业者那样思考”。但这并不是说DP不重要，在计算机科学里，这个思路非常非常重要，只不过工程上不光要考虑算法复杂度，还要考虑可维护性问题而已。如果场景恰当，当然还是要用。比方说，我们之前做的“计算基金最大回撤”就用DP，实际上就是LeetCode里“买卖股票最佳时机”解题方案的镜像——就是一个算回撤，一个算正向收益的区别。

回到DP本身，题主不妨试试这样做。既然题主的思维方式觉得自顶向下很舒服，那就总是先用递归实现自顶向下DP。通过后加上Cache用记忆化搜索的方式写代码。再改写成等价的自顶向上的做法。反复做几次看看思路上能不能贯通。但同时，我也遇到过写自底向上很舒服，反过来就别扭的人。

当然，虽然二者等价，我依然遇到过很难自底向上写的问题。此时一般是很难找到一个很简单的从小到大的规律，这就不如用递归+cache。cache的规则就很简单，算过了就cache，没算过就没cache，递归调用中参数出现的规律不需要操心。

同时，也有反过来的情况，自底向上很难写。主要是这类问题的递归方式相对好理解，但自底向上时就要让dp\[i\]\[j\]表达一些比较模糊，很难说清楚的概念，让编码变得很别扭。所以遇到这种情况就用容易理解的形式写就好。

最后，递归能够在DP这个思路里支持实现自顶向下，但也不一定就用在DP里。递归适合的场景要更加广泛。比如dfs/bfs这样的搜索场景就可以用递归做。递归还可以用来做模拟，比如A对B做一件事、B又要对C和D做一件事，D又要对A和B做一件事……。用递归来实现很直观。

## IO多路复用和线程池哪个效率更高，更有优势？

问题本身貌似有问题。简单比较两种工具的优劣意义不大。你没法说锤子和剪刀那个更好。我们一般会评价对于某个场景，哪种工具更合适。

io多路复用（这翻译真的很坑爹啊），指的是同一个进（线）程可以处理多个IO数据流。

多线程+池模型指的是每个线程处理一个IO流。

IO多路复用的优势在于，当处理的消耗对比IO几乎可以忽略不计时，可以处理大量的并发IO，而不用消耗太多CPU/内存。这就像是一个工作很高效的人，手上一个todo list，他高效的依次处理每个任务。这比每个任务单独安排一个人要节省（雇人是要发工资的……）。典型的例子是nginx做代理，代理的转发逻辑相对比较简单直接，那么IO多路复用很适合。相反，如果是一个做复杂计算的场景，计算本身可能是个 指数复杂度的东西，IO不是瓶颈。那么怎么充分利用CPU或者显卡的核心多干活才是关键。

此外，IO多路复用适合处理很多闲置的IO，因为IO socket的数量的增加并不会带来进（线）程数的增加，也就不会带来stack内存，内核对象，切换时间的损耗。因此像长链接做通知的场景非常适合。

IO多路复用 + 单进（线）程有个额外的好处，就不会有并发编程的各种坑问题，比如在nginx里，redis里，编程实现都会很简单很多。编程中处理并发冲突和一致性，原子性问题真的是很难，极易出错。

> 但是现实中，也有IO多路复用 + 多worker线程的做法，这样上面这个好处就没有了。

如果做不到“处理过程相对于IO可以忽略不计”，IO多路复用的并不一定比线程池方案更好。比如一个web的服务，用jetty 9的NIO connector，后边是spring svc + JDBC连接数据库。spring svc + JDBC连接数据库这两块的处理延迟相对于NIO来说不能忽略，所以并不能指望用jetty 9的NIO connector换了之前的BIO connector的容器，性能能高不少（实际上应该会高一些，但不会太夸张，毕竟瓶颈在后边处理和DB上）。

顺便提一句，Java世界里，因为JDBC这个东西是BIO的，所以在我们常见的Java服务里没有办法做到全部NIO化，必须得弄成多线程模型。如果要在做Java web服务这个大场景下享受IO多路复用的好处，要不就是不需要DB的，要不就是得用Vert.X一类的纯NIO框架把DB IO访问也得框进来。

最后，如果IO压力过大，一个高并发的东西和一个不那么高并发的东西，都不能正确响应，对用户来说是一样的——**就是不能用**。假如IO非常的繁重，没有空闲的连接，那么IO的压力在两种模型下表现差不多，IO多路复用的“并发“看了起来会大一些，但因为IO已经满了，所以表现出超时严重；而线程池可能表现为，所有的线程都因为IO过慢而卡死了，线程池耗尽，新的请求进不来直接报错。但不管哪一种，在极端压力下，都无法正常工作。这时，要想着怎么扩容。

简单总结一下，

* 如果压力不是很大，并且处理性能相对于IO可以忽略不计
  * IO多路复用+单进（线）程比较省资源
  * 适合处理大量的闲置的IO
  * IO多路复用+多单进（线）程与线程池方案相比有好处，但是并不会有太大的优势
* 如果压力很大，什么方案都得跪，这时就得扩容。当然因为IO多路复用+单进（线）程比较省资源，所以扩容时能省钱。

## 为什么一般操作系统中应用程序的栈空间都要设最大值，不支持动态扩展？

看下面的图：

![](https://pic1.zhimg.com/50/v2-957f4c83e14d294b9285b54ff55a02fb_hd.jpg?source=1940ef5c)

题主之所以能问出这个问题，也许是因为大多数内存的理论模型都会画成左边那样子。那么自然而然的Heap可以不断往上扩展；Stack可以往下扩展。

但现实中是  
1. 主流操作系统都是有多线程支持的，而多线程需要每个线程分配一个独立的Stack，每个Stack内部可以满足“向下增长“，但是必须要有个界限，不然没法实现了。否则下个Stack从哪开始呢？

2. heap和mmap segment的存在。mmap是有很多用途的。比如

* 加载一个so动态库，是以mmap的形式映射到内存里，再让CPU执行上面的指令的；每个程序都会加载大量的动态库。
* 我们编程意义上的heap实际上是操作系统级别heap和mmap区域的混合。当分配一大块内存时，操作系统可能会决定不从heap里切，而是独立分配一块mmap区域来用。
* mmap自己也可以被用户直接使用，比如映射一个文件，或者做内存的数据共享。

上图中之所以把heap，mmap segement和stack画成一个颜色，是因为他们本质上差不太多，都是程序运行时动态分配和调整的。stack和mmap都是“一块内存”，因此实现中并不一定非得是Stack永远比mmap的地址数值更大。只要不重叠就行了。Linux本身的api也允许创建新线程时，指定一段内存作为“Stack”，而这个内存自然也需要通过malloc得到，这又回到了mmap/heap上了。

回到应用层面，巨大的Stack除了应付非常深的递归之外没有什么太大的用处。而非常深的递归一般就是程序哪里写bug了。为了不太有用的场景去做设计并不是理智的行为。

我们常规意义上说“Stack”实际只是在用CPU对一段内存的地址做指令寄存器的push和pop而已。如果这不够用你可以自行定制一个喜欢的形式来实现对Stack的管理。有些语言可以把Stack的管理玩出花。比如go，自己实现了对go routine的管理，每个routine的Stack都可以不是连续的，这样既避免浪费，又能轻松扩展。

##  

