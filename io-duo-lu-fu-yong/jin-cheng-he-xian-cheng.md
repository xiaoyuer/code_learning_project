# 进程和线程

## 进程和线程

1、程序

程序是一个按指定格式存储了一系列指令的编码序列。

打个比方的话，程序就好像一张菜谱，它原原本本精确记录了某道菜的整个制作流程。

2、操作系统

操作系统也是程序的一种。

它的作用是管理硬件，忽略厂商实现差异，给程序员提供一个统一的访问界面；管理其他程序，在用户需要时加载、运行它们。

操作系统可以从不同侧面划分成很多种类型。比如从预期响应时间可以分为实时系统和非实时系统；从是否允许多个程序同时被执行分可以分为单进程系统、多进程系统，多进程系统还可以分为非抢夺式多任务和抢夺式多任务等；从内核设计思路可以分微内核系统、宏内核系统……

总之，这里面的道道很多，不然CS专业的操作系统原理这门课也不会是那么厚的一本书、并且每年还要挂那么多人。

3、进程

我们把一个被载入内存、正在执行的程序叫做一个进程。

注意进程的关键点是“正在被执行”。比如你可以同时打开50个QQ，这50个QQ是同一个程序（QQ.exe），但它在内存的50份拷贝是50个不同的进程。

还是用菜谱来打比方的话，“西红柿炒鸡蛋”这张菜谱是“程序”；你照着菜谱做这道菜是一个进程。这道菜每天中午神州大地都有成千上万厨师在做，他们做这道菜的每一次实践也都是一个不同的进程。

4、线程

“传统”观念下，一个程序只有一个执行点，就好像一张菜谱是一个厨师炒出来的一样。

但事实上，和一张菜谱可以让多个厨师分头执行它的不同部分一样，一个程序也完全可以包含两个以上的执行点，从而利用多CPU/核心以及不同硬件同时做几件事。

比如说，完全可以在等待网络报文的同时把已有数据先排个序、建个索引什么的，不至于网络包没过来整个程序都没法动，把其他硬件晾一边。

一个程序允许多个执行点（执行现场）就叫多线程。

线程可以由操作系统直接调度，也可以由用户自己写一段代码，自己管理多个代码段的执行切换动作。后者就是所谓的“用户态多线程”——有人混淆了“有特殊硬件时某种OS的线程的具体实现”和“线程概念”本身，这是一知半解的典型表现。想想nobody权限下用户态线程库如何实现，或可帮他把搅来搅去乱成一团的糊涂认识分离开。

换句话说，线程既可以由操作系统实现，也可以自己写程序实现。前者的好处是可以利用CPU里的多个核心（或多颗CPU）；后者虽然无法利用多核心/多CPU，但可以通过灵活的执行现场切换，更方便某些有特定需要的程序的设计。

线程和进程的区别在于，进程拥有自己的资源，而线程直接使用分配给进程的资源，它自己不能占有资源。

## 进程线程和协程

需要先对 IO 的概念有一定的认识: IO在计算机中指Input/Output，也就是输入和输出。

###  **并发与并行**

  
并发：在操作系统中，某一时间段，几个程序在同一个CPU上运行，但在任意一个时间点上，只有一个程序在CPU上运行。

  
当有多个线程时，如果系统只有一个CPU，那么CPU不可能真正同时进行多个线程，CPU的运行时间会被划分成若干个时间段，每个时间段分配给各个线程去执行，一个时间段里某个线程运行时，其他线程处于挂起状态，这就是并发。并发解决了程序排队等待的问题，如果一个程序发生阻塞，其他程序仍然可以正常执行。

  
并行：当操作系统有多个CPU时，一个CPU处理A线程，另一个CPU处理B线程，两个线程互相不抢占CPU资源，可以同时进行，这种方式成为并行。

  
区别

1. 并发只是在宏观上给人感觉有多个程序在同时运行，但在实际的单CPU系统中，每一时刻只有一个程序在运行，微观上这些程序是分时交替执行。
2. 在多CPU系统中，将这些并发执行的程序分配到不同的CPU上处理，每个CPU用来处理一个程序，这样多个程序便可以实现同时执行。

知乎上高赞例子：

* 你吃饭吃到一半，电话来了，你一直到吃完了以后才去接，这就说明你不支持并发也不支持并行。
* 你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。
* 你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。

并发的关键是你有处理多个任务的能力，不一定要同时。并行的关键是你有同时处理多个任务的能力。所以我认为它们最关键的点就是：是否是**『同时』**。

###  **进程**

  
一个进程好比是一个程序，它是 资源分配的最小单位 。同一时刻执行的进程数不会超过核心数。不过如果问单核CPU能否运行多进程？答案又是肯定的。单核CPU也可以运行多进程，只不过不是同时的，而是极快地在进程间来回切换实现的多进程。举个简单的例子，就算是十年前的单核CPU的电脑，也可以聊QQ的同时看视频。

  
电脑中有许多进程需要处于「同时」开启的状态，而利用CPU在进程间的快速切换，可以实现「同时」运行多个程序。而进程切换则意味着需要保留进程切换前的状态，以备切换回去的时候能够继续接着工作。所以进程拥有自己的地址空间，全局变量，文件描述符，各种硬件等等资源。操作系统通过调度CPU去执行进程的记录、回复、切换等等。

###  **线程**

  
如果说进程和进程之间相当于程序与程序之间的关系，那么线程与线程之间就相当于程序内的任务和任务之间的关系。所以线程是依赖于进程的，也称为 「微进程」 。它是 程序执行过程中的最小单元 。

  
一个程序内包含了多种任务。打个比方，用播放器看视频的时候，视频输出的画面和声音可以认为是两种任务。当你拖动进度条的时候又触发了另外一种任务。拖动进度条会导致画面和声音都发生变化，如果进程里没有线程的话，那么可能发生的情况就是：

  
拖动进度条-&gt;画面更新-&gt;声音更新。你会明显感到画面和声音和进度条不同步。

  
但是加上了线程之后，线程能够共享进程的大部分资源，并参与CPU的调度。意味着它能够在进程间进行切换，实现「并发」，从而反馈到使用上就是拖动进度条的同时，画面和声音都同步了。所以我们经常能听到的一个词是「多线程」，就是把一个程序分成多个任务去跑，让任务更快处理。不过线程和线程之间由于某些资源是独占的，会导致锁的问题。例如Python的GIL多线程锁。

###  **进程与线程的区别**

1. 进程是CPU资源分配的基本单位，线程是独立运行和独立调度的基本单位（CPU上真正运行的是线程）。
2. 进程拥有自己的资源空间，一个进程包含若干个线程，线程与CPU资源分配无关，多个线程共享同一进程内的资源。
3. 线程的调度与切换比进程快很多。

**CPU密集型代码\(各种循环处理、计算等等\)：使用多进程。IO密集型代码\(文件处理、网络爬虫等\)：使用多线程**

###  **阻塞与非阻塞**

  
阻塞是指调用线程或者进程被操作系统挂起。  
非阻塞是指调用线程或者进程不会被操作系统挂起。

###  **同步与异步**

  
同步是阻塞模式，异步是非阻塞模式。

* 同步就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息，那么这个进程将会一直等待下去，知道收到返回信息才继续执行下去；
* 异步是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态。当有消息返回式系统会通知进程进行处理，这样可以提高执行的效率。

由调用方盲目主动问询的方式是同步调用，由被调用方主动通知调用方任务已完成的方式是异步调用。看下图  
![](https://pic4.zhimg.com/80/v2-f1118cbd6283a2626e6d4b9e7477b21b_1440w.jpg)

### **协程**

  
协程，又称微线程，纤程。英文名Coroutine。一句话说明什么是线程：协程是一种用户态的轻量级线程。

  
协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此：  
协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。

  
协程的好处：

1. 无需线程上下文切换的开销
2. 无需原子操作锁定及同步的开销
3. 方便切换控制流，简化编程模型

高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。

  
缺点：  


1. 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。
2. 进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序

### **最佳实践** 

1. 线程和协程推荐在IO密集型的任务\(比如网络调用\)中使用，而在CPU密集型的任务中，表现较差。
2. 对于CPU密集型的任务，则需要多个进程，绕开GIL的限制，利用所有可用的CPU核心，提高效率。
3. 所以大并发下的最佳实践就是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。

顺便一提，非常流行的一个爬虫框架Scrapy就是用到异步框架Twisted来进行任务的调度，这也是Scrapy框架高性能的原因之一。

  
最后推荐阅读：[深入理解 Python 异步编程\(上\)](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIxMjY5NTE0MA%3D%3D%26mid%3D2247483720%26idx%3D1%26sn%3Df016c06ddd17765fd50b705fed64429c%26scene%3D21%23wechat_redirect)

  
[阅读原文](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIwNjUxMTQyMA%3D%3D%26mid%3D2247484702%26idx%3D1%26sn%3D8ad66afa9cef1ddf45dac9f6671ea58c%26chksm%3D9721c94da056405b132894439def3aeeb3c93e3b674c63e3a8be832b07b6fad0af846b1bc3ca%26token%3D352549795%26lang%3Dzh_CN%23%23)

## 出于什么样的原因，诞生了「协程」这一概念？

程序开发的一大矛盾是，你要用控制流去完成逻辑流。也就是说，你要用指令的执行来完成逻辑链条的前因后果。

在刚开始学程序的时候，往往都是从控制流等价于执行流的情况下学起，执行到哪，就意味着逻辑走到了哪。这样的程序结构清晰，可读性好。

但是问题是中间有些过程是不能立即得到结果的，程序为了等结果就会阻塞。这种情况多见于一些io操作。为了提升效率，我们可以使用异步的api，通过回调/通知函数来响应操作结果，同时接着执行下一轮的逻辑。

异步回调/通知的问题在于，它把原本统一的逻辑流拆开成了几个阶段，这样控制流和逻辑流就不等价了。为了保证逻辑数据的传递，需要自己来维护状态，阅读起来也比较头疼。状态的维护历来就是bug层出的地方，很容易掉入无效状态而死掉。同时，这种机制调试起来还特别麻烦，因为状态所能提供的信息不够，往往还得手动跟踪调用链，这是相当费精力的。

协程是一种任务调度机制，它可以让你用逻辑流的顺序去写控制流，而且还不会导致操作系统级的线程阻塞。你发起异步请求、注册回调/通知器、保存状态，挂起控制流、收到回调/通知、恢复状态、恢复控制流的所有过程都能过一个yield来默默完成。

从代码结构上看，协程保证了编写过程中的思维连贯性，使得函数（闭包）体本身就无缝保持了程序状态。逻辑紧凑，可读性高，不易写出错的代码，可调试性强。

从实现上看，与线程相比，这种主动让出型的调度方式更为高效。一方面，它让调用者自己来决定什么时候让出，比操作系统的抢占式调度所需要的时间代价要小很多。后者为了能恢复现场会在切换线程时保存相当多的状态，并且会非常频繁地进行切换。另一方面，协程本身可以做在用户态，每个协程的体积比线程要小得多，因此一个进程可以容纳数量相当可观的逻辑流。

举个例子，openresty中的ngx\_lua组件就使用了协程来管理所有的io接口，极大地提升了服务器的负载能力。

2020.5.27 更新：

多线程阻塞的执行图如下：

![](https://pic4.zhimg.com/50/v2-35b14e688fae599083f6a7bbfbe594b1_hd.jpg?source=1940ef5c)

回调的执行图例如下：

![](https://pic1.zhimg.com/50/v2-6c6a24ee6bf8b7899a69b7de1f774e9b_hd.jpg?source=1940ef5c)

带有协程的执行图例如下：

![](https://pic2.zhimg.com/50/v2-6e7d7de3f9517bfd3f96cf3e8532c766_hd.jpg?source=1940ef5c)

从图1可以看出，多线程情形下的多逻辑能力严重依赖于程序申请到的执行流/线程的数量。线程是很重的，不仅体积庞大，还得经常进出内核。在这种模式下，线程的阻塞等待浪费了大量内存。

从图2可以看出，异步回调可以避免忙等，但是逻辑流被拆开了，收到回调之后很难恢复出是从哪里调过来的，逻辑流栈上的变量也被清空了。

图3的协程可以看出，协程不会出现图1这种死等的情况，一但遇到阻塞，那么要么从池子里挑下一个准备好的协程跑，要么由阻塞的协程指定由谁接着它跑，没有CPU时间的浪费。同时相比于图2，逻辑流在程序员看来是连续的，所有的状态都通过原生的栈延续得好好的。

这里要强调一点，协程只是一种任务调度模式，虽然很多情形下是用来解决I/O问题，但它不必然与实际的I/O相关。在上面的方式二中我们可以看出，有时候需要唤醒的下一个逻辑流其实可以通过挂起的逻辑流来指定，这在生产者-消费者模式中非常常见：

生产者生产出产品-&gt;挂起生产者，通知消费者消费-&gt;消费者被唤醒，消费产品-&gt;挂起消费者，通知生产者生产-&gt; ...

在这个模式中，不必有任何实际的I/O，但是通过协程有效地把生产者消费者交替的逻辑解耦了——即无论从生产者还是消费者来看，每方的逻辑流都是流畅且不包含另一方的具体逻辑的。这种模式也是lua的标准协程模式，由于比较少见，特别在这里介绍一下。

## java的多线程不选择协程

当我们希望引入协程，我们想解决什么问题。我想不外乎下面几点：

* 节省资源，轻量，具体就是：
  * 节省内存，每个线程需要分配一段栈内存，以及内核里的一些资源
  * 节省分配线程的开销（创建和销毁线程要各做一次syscall）
  * 节省大量线程切换带来的开销
* 与NIO配合实现非阻塞的编程，提高系统的吞吐
* 使用起来更加舒服顺畅（async+await，跑起来是异步的，但写起来感觉上是同步的）



先说内存。拿Java Web编程举例子，一个tomcat上的woker线程池的最大线程数一般会配置为50～500之间（目前springboot的默认值给的200）。也就是说同一时刻可以接受的请求最多也就是这么多。如果超过了最大值，请求直接打失败拒绝处理。假如每个线程给128KB，500个线程放一起的内存占用量大概是60+MB。如果真的有瓶颈，也许CPU，IO，带宽，DB的CPU等会有瓶颈，但这点内存量的增幅对于动辄数个GB的Java运行时进程来说似乎**并不是什么大问题**。

> 上面的讨论简化了RSS和VM的区别。实际上一个线程启动后只会在虚拟地址上占位置那么多的内存。除非实际用上，是不会真的消耗物理内存的。

换一个场景，比如IM服务器，需要同时处理大量空闲的链接（可能要几十万，上百万）。这时候用connection per thread就很不划算了。但是可以直接改用netty去处理这类问题。你可以理解为NIO + woker thread大致就是一套“协程”，只不过没有实现在语法层面，写起来不优雅而已。问题是，你的场景真的处理了并发几十万，上百万的连接吗？

再说创建/销毁线程的开销。这个问题在Java里通过线程池得到了很好的解决。你会发现即便你用vert.x或者kotlin的协程，归根到底也是要靠线程池工作的。goroutine相当于设置一个全局的“线程池”，GOMAXPROCS就是线程池的最大数量；而Java可以自由设置多个不同的线程池（比如处理请求一套，异步任务另外一套等）。kotlin利用这个机制来构建多个不同的协程scope。这看起来似乎会更灵活一点。

然后是线程的切换开销。线程的切换实际上只会发生在那些“活跃”的线程上。对于类似于Web的场景，大量的线程实际上因为IO（发请求/读DB）而挂起，根本不会参与OS的线程切换。现实当中一个最大200线程的服务器可能同一时刻的“活跃线程”总数只有数十而已。其开销没有想象的那么大。为了避免过大的线程切换开销，**真正要防范的是同时有大量“活跃线程”**。这个事情我自己上学的时候干过，当时是写了一个网络模拟器。每一个节点，每一个链路都由一个线程实现。模拟跑起来后，同时的活跃线程上千。当时整个机器瞬间卡死，直到kill掉这个程序。

此外说说与NIO的配合。在Java这个生态里Java NIO/Netty/Vert.X/rxJava/Akka可以任意选择。一般来讲，Netty可以解决绝大部分因为IO的等待造成资源浪费的问题。Vert.X/rxJava。可以让程序写的更加“优雅”一点（见仁见智）。Akka就是Java世界里对“原教旨OO“的实现，很有特色。的确，用NIO + completedFuture/handler/lambda不如async+await写起来舒服，但起码是可以干活的。

如果真的要较真Java的NIO用于业务的问题，其**核心痛点应该是JDBC**。这是个诞生了几十年的，必须使用Blocking IO的DB交互协议。其上承载了Java庞大的生态和业务逻辑。Java要改自己的编程方式，必须得重新设计和实现JDBC，就像[https://github.com/vert-x3/vertx-mysql-postgresql-client](https://link.zhihu.com/?target=https%3A//github.com/vert-x3/vertx-mysql-postgresql-client) 那样做。问题是，社区里这种“异步JDBC”还没有支持oracle、sql server等传统DB。对mysql和postgres的支持还需要继续趟坑～

如果认真阅读上面这些需要“协程”解决的问题，就会发现基本上都可以以各种方式解决。觉得线程耗资源，可以控制线程总数，可以减少线程stack的大小，可以用线程池配置max和min idle等等。想要go的channel，可以上disruptor。可以说，Java这个生态里尽管没有“协程”这个第一级别的概念，但是要解决问题的工具并不缺。

Java仅仅是**没有解决”协程“在Java中的定义，以及“写得优雅“这个问题**。从工程角度，“写得优雅”的优势并没有很多追新的人想象的那么关键。C\#也并非因为有了async await就抢了Java的市场分毫。而反过来，如果java社区全力推进这个事情，Java历史上的生态的积累却因为协程的出现而进行大换血。想像一下如果没有thread，也没有ThreadLocal，@Transactional不起作用了，又没有等价的工具，是不是很郁闷？这么看来怎么着都不是个划算的事情。我想Oracle对此并不会有太大兴趣。OpenJDK的loom能不能成，如果真的release多少Java程序员愿意使用，师母已呆。据我所知在9012年的今天，还有大量的Java6程序员。

其他新的语言历史包袱少，比较容易重新思考“什么是现代的multi-task编程的方式“这个大主题。kotlin的协程、go的goroutine、javascript的async await、python的asyncio、swift的GCD都给了各自的答案。如果真的想入坑Java这个体系的“协程”，就从kotlin开始吧，毕竟可以混合编程。

最后说一句，多线程容易出bug主要因为：

* “抢占“式的线程切换 —— 你无法确定两个线程访问数据的顺序，一切都很随机
* “同步“不可组装 —— 同步的代码组装起来也不同步，必须加个更大的同步块

协程能不能避免容易出bug的缺陷，主要看能不能避免上面两个问题。如果协程底层用的还是线程池，两个协程还是通过共享内存通讯，那么多线程该出什么bug，多协程照样出。javascript里不出这种bug是因为其用户线程就一个，不会出现线程切换，也不用同步；go是建议用channel做goroutine的通讯。如果go routine不用channel，而是用共享变量，并且没有用Sync包控制一下，还是会出bug。

## 并发与并行的区别

**串行**：喂？你在做什么呢？买菜啊？好的，到家了说一声。啊？到家了？那你到幼儿园接娃吧。

**串行的特点**：前一个任务没搞定，下一个任务就只能等着。

**并行**：来，这是你的盖浇饭，这是我的胡辣汤。咱俩一起吃。

**并行的特点**：两个任务在**同一时刻互不干扰**的同时执行。

**并发**：你去买个菜，顺路把邮件发了；路过幼儿园时带娃回家。

**并发的特点**：同时安排若干个任务，这些任务可以彼此穿插着进行；有些任务可能是并行的，比如买菜、发邮件和去幼儿园的某些路途是重叠的，这时你的确同时在做三件事；但进菜市场和发邮件和接娃三者是互斥的，每个时刻只能完成其中一件。换句话说，并发允许两个任务彼此干扰。

**要点**：串行还是并发，这都是**任务安排者**视角看到的东西。前者要求你看到前一个任务结束了，下一个任务才能安排；而后者呢，你可以同时提交许多任务，执行者（们）之间会相互协调并自己安排执行顺序（但未必合理，比如可能出现死锁）——总之，你把任务安排下去就不用管了。

相比之下，“并行”是**任务执行者**视角的东西，和前两者所处平面不同。

**典型案例**：你买了个新硬盘，打算把自己的重要文件复制过去。于是你找到music目录，把所有的音乐文件夹选中，复制50G音乐到新硬盘；然后打开photo目录，把100G照片复制到新硬盘；又打开mov目录，把800G视频复制到新硬盘……

最后，你看到Windows显示了10个文件复制窗口；其中一个窗口的提示是“还有一千六百个文件待复制，需要三天零八小时七分钟三十二秒”。

这就是典型的“并发”任务。

在这个场景里，你同时启动了10个文件复制进程，帮你复制十大类文件。

如果没有“并发”支持，你只能先复制一个文件夹，等上半小时，看它复制完了才能继续复制下一个。这当然很累人。

一旦有了并发支持，你就能同时启动十个复制任务。在计算机忙碌的同时，你完全可以出去旅个游……

但是，细心的你可能会注意到：如果这十个文件复制任务没有分成十个进程去做，而是写个批处理甚至干脆用Linux的dd命令全盘复制，那么复制完所有文件只需五六个小时。

这是因为，十个进程会彼此争抢资源；而每次进程执行权切换，硬盘就不得不重新寻道——这是非常非常浪费时间的。

其结果，就是把本来五六个小时就能搞定的事情，争抢成了三天都搞不定……

换句话说，这里面没有并行，只有并发。

说的更清晰点，对电脑操作者，你的确是“并发”了十个任务；但对程序这个执行者来说，它们仍然是“串行”使用硬盘——进程1用200ms，交出控制权；换进程2用200ms硬盘，交出控制权；然后是进程3、4、5、6、7……

它们只是快速切换执行权、从而让你得到了一个“同时执行”的假象而已。

因此，对这类任务……其实你还是自己写个批处理更好。节省你的生命，也节省硬盘的使用寿命。

当然，如果你的两台电脑分别装了块新硬盘，显然它们对各自硬盘读写就是“并行”的。互不干扰嘛。

你完全可以用第三台电脑远程登陆上去，然后分别在两者上面启动各自的复制进程——只要没有数据相关，先让电脑A复制完再去捣鼓电脑B，这显然是不智的。

类似的，同一台电脑里面，网卡收发信息和硬盘读写并不相关；CPU忙碌时让显卡空闲也是极大的浪费——换句话说，不同任务有不同的执行实体；那么我们当然不应该“在CPU上执行任务A”时“禁止任务B使用网卡”。

没错，只要执行任务的硬件不同（包括但不限于不同的CPU核心、网卡A和网卡B、C、D、显卡、硬盘、打印机等等等等），它们就可以并行工作。

一个好的程序，一方面不应该在单个硬件上造成过多切换（比如在一块硬盘上同时开10个文件复制进程就是一种极其低效的使用方式），另一方面则要尽量利用每个空闲的硬件（比如任务A使用硬盘时应该允许任务B使用网卡），这才不至于降低执行效率、使得硬件使用不够充分。

尤其是，**纠正一个错误的观念**：并不像一般人以为的“单核单线程没有并行”；事实上，哪怕用了单核单线程CPU的电脑，它上面也存在真正的“并行”。

只不过，这个并行并不是CPU内部的、线程之间的并行；而是CPU执行程序的同时，DMA控制器也在执行着网络报文收发、磁盘读写、音视频播放/录制等等任务。

综上，串行在执行单个简单任务时，执行速率是最高的。因为完全没有干扰，任何硬件想用就用。

但是，串行方式的硬件利用率不高。比如当某个任务不需要使用打印机时，在它完成之前，打印机就只能闲置。

为了解决这个问题，我们首先要允许“并发”。

“并发”的意思是，你可以同时提交多个任务，但系统并不能保证它们可以并行执行。

甚至于，在极端的、类似“单个硬盘上同时启动10个复制进程”的场景里，“并发”反而引起了过多的切换动作，成几倍甚至几十倍的降低了文件复制效率——这种场景下，并发甚至要不如串行。

想要提高并发的效率，我们就必须深入进去，关注“这些任务之间究竟有没有出现并行”。

比如，如果文件复制程序写的非常糙，那么很可能是“先从旧硬盘读取数据，然后写入新硬盘；数据写入新硬盘后，继续从旧硬盘读取数据”……

这在单硬盘上是合理的，少了一些寻道操作；但在两块硬盘的场景下，这就相当于“串行使用两块硬盘”——这个利用效率显然太低了，每块硬盘只有50%左右的利用率（当然，现代OS会主动多读一些数据到磁盘缓存，这个机制可以有效提高硬盘利用率）。

那么，如果同时启动两个复制进程，反而会不时出现“进程A读旧硬盘，同时进程B写新硬盘”这种场景，从而把每块硬盘的利用率提高到60%~80%。

换句话说，“并发”的确经常能让“并行”自然而然的出现，硬盘利用率也的确被提高了；只是这种提高缺乏保证（比如，运气不好时，复制进程A可能和进程B争着读取旧硬盘，从而导致很多不必要的寻道动作）；而且，由于并发并不保证合理的执行顺序，反而经常“搬起石头砸自己的脚”。

比如，一旦同时启动更多复制进程（比如三五个），那么过多的进程切换引起的过多的磁盘重新寻道动作就会抵消一切好处。

因此，很多时候，我们需要一个优秀的、头脑清醒的程序员；只有在他的有意识的安排下，才能在确保硬件利用率的同时、不因过多的争抢和切换降低执行效率。

## IO 多路复用是什么意思？

假设你是一个机场的空管， 你需要管理到你机场的所有的航线， 包括进港，出港， 有些航班需要放到停机坪等待，有些航班需要去登机口接乘客。

你会怎么做?

最简单的做法，就是你去招一大批空管员，然后每人盯一架飞机， 从进港，接客，排位，出港，航线监控，直至交接给下一个空港，全程监控。那么问题就来了：  


* 很快你就发现空管塔里面聚集起来一大票的空管员，交通稍微繁忙一点，新的空管员就已经挤不进来了。 
* 空管员之间需要协调，屋子里面就1, 2个人的时候还好，几十号人以后 ，基本上就成菜市场了。 
* 空管员经常需要更新一些公用的东西，比如起飞显示屏，比如下一个小时后的出港排期，最后你会很惊奇的发现，每个人的时间最后都花在了抢这些资源上。 

现实上我们的空管同时管几十架飞机稀松平常的事情， 他们怎么做的呢？  
他们用这个东西  


![](https://pic2.zhimg.com/583d5ba3cee12e78befa8e2b749f4269_b.jpg)

这个东西叫flight progress strip. 每一个块代表一个航班，不同的槽代表不同的状态，然后一个空管员可以管理一组这样的块（一组航班），而他的工作，就是在航班信息有新的更新的时候，把对应的块放到不同的槽子里面。

这个东西现在还没有淘汰哦，只是变成电子的了而已。。

是不是觉得一下子效率高了很多，一个空管塔里可以调度的航线可以是前一种方法的几倍到几十倍。

如果你把每一个航线当成一个Sock\(I/O 流\), 空管当成你的服务端Sock管理代码的话.

**第一种方法就是最传统的多进程并发模型 \(每进来一个新的I/O流会分配一个新的进程管理。\)**  
**第二种方法就是I/O多路复用 \(单个线程，通过记录跟踪每个I/O流\(sock\)的状态，来同时管理多个I/O流 。\)**

其实“I/O多路复用”这个坑爹翻译可能是这个概念在中文里面如此难理解的原因。所谓的I/O多路复用在英文中其实叫 I/O multiplexing. 如果你搜索multiplexing啥意思，基本上都会出这个图：  


![](https://pic1.zhimg.com/5d8e39d83e931da6ba3b6bc496302e5c_b.png)

于是大部分人都直接联想到"一根网线，多个sock复用" 这个概念，包括上面的几个回答， 其实不管你用多进程还是I/O多路复用， 网线都只有一根好伐。**多个Sock复用一根网线这个功能是在内核＋驱动层实现的**。  
  
**重要的事情再说一遍： I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock\(I/O流\)的状态\(对应空管塔里面的Fight progress strip槽\)来同时管理多个I/O流**. 发明它的原因，是尽量多的提高服务器的吞吐能力。

 是不是听起来好拗口，看个图就懂了.

  
在同一个线程里面， 通过拨开关的方式，来同时传输多个I/O流， \(学过EE的人现在可以站出来义正严辞说这个叫“时分复用”了）。

![](https://pic2.zhimg.com/18d8525aceddb840ea4c131002716221_b.jpg)

什么，你还没有搞懂“一个请求到来了，nginx使用epoll接收请求的过程是怎样的”， 多看看这个图就了解了。提醒下，ngnix会有很多链接进来， epoll会把他们都监视起来，然后像拨开关一样，谁有数据就拨向谁，然后调用相应的代码处理。

－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－  
了解这个基本的概念以后，其他的就很好解释了。

**select, poll, epoll 都是I/O多路复用的具体的实现，之所以有这三个鬼存在，其实是他们出现是有先后顺序的。**

I/O多路复用这个概念被提出来以后， select是第一个实现 \(1983 左右在BSD里面实现的\)。select 被实现以后，很快就暴露出了很多问题。  


* select 会修改传入的参数数组，这个对于一个需要调用很多次的函数，是非常不友好的。 
*  select 如果任何一个sock\(I/O stream\)出现了数据，select 仅仅会返回，但是并不会告诉你是那个sock上有数据，于是你只能自己一个一个的找，10几个sock可能还好，要是几万的sock每次都找一遍，这个无谓的开销就颇有海天盛筵的豪气了。 
* select 只能监视1024个链接， 这个跟草榴没啥关系哦，linux 定义在头文件中的，参见_FD\_SETSIZE。_
* select 不是线程安全的，如果你把一个sock加入到select, 然后突然另外一个线程发现，尼玛，这个sock不用，要收回。对不起，这个select 不支持的，如果你丧心病狂的竟然关掉这个sock, select的标准行为是。。呃。。不可预测的， 这个可是写在文档中的哦.

 “If a file descriptor being monitored by select\(\) is closed in another thread, the result is unspecified”  
 霸不霸气于是14年以后\(1997年）一帮人又实现了poll, poll 修复了select的很多问题，比如  


* poll 去掉了1024个链接的限制，于是要多少链接呢， 主人你开心就好。 
*  poll 从设计上来说，不再修改传入数组，不过这个要看你的平台了，所以行走江湖，还是小心为妙。

**其实拖14年那么久也不是效率问题， 而是那个时代的硬件实在太弱，一台服务器处理1千多个链接简直就是神一样的存在了，select很长段时间已经满足需求。**

但是poll仍然不是线程安全的， 这就意味着，不管服务器有多强悍，你也只能在一个线程里面处理一组I/O流。你当然可以那多进程来配合了，不过然后你就有了多进程的各种问题。

于是5年以后, 在2002, 大神 Davide Libenzi 实现了epoll.epoll 可以说是I/O 多路复用最新的一个实现，epoll 修复了poll 和select绝大部分问题, 比如：  


* epoll 现在是线程安全的。 
* epoll 现在不仅告诉你sock组里面数据，还会告诉你具体哪个sock有数据，你不用自己去找了。 

epoll 当年的patch，现在还在，下面链接可以看得到：  
[/dev/epoll Home Page](https://link.zhihu.com/?target=http%3A//www.xmailserver.org/linux-patches/nio-improve.html)

贴一张霸气的图，看看当年神一样的性能（测试代码都是死链了， 如果有人可以刨坟找出来，可以研究下细节怎么测的\).  
  
横轴Dead connections 就是链接数的意思，叫这个名字只是它的测试工具叫deadcon. 纵轴是每秒处理请求的数量，你可以看到，epoll每秒处理请求的数量基本不会随着链接变多而下降的。poll 和/dev/poll 就很惨了。

![](https://pic1.zhimg.com/5a56c4677da1c10153ed22a3f6dfeab4_b.png)

可是epoll 有个致命的缺点。。只有linux支持。比如BSD上面对应的实现是kqueue。

其实有些国内知名厂商把epoll从安卓里面裁掉这种脑残的事情我会主动告诉你嘛。什么，你说没人用安卓做服务器，尼玛你是看不起p2p软件了啦。

而ngnix 的设计原则里面， 它会使用目标平台上面最高效的I/O多路复用模型咯，所以才会有这个设置。一般情况下，如果可能的话，尽量都用epoll/kqueue吧。

详细的在这里:  
[Connection processing methods](https://link.zhihu.com/?target=http%3A//nginx.org/en/docs/events.html)PS: 上面所有这些比较分析，都建立在大并发下面，如果你的并发数太少，用哪个，其实都没有区别。 如果像是在欧朋数据中心里面的转码服务器那种动不动就是几万几十万的并发，不用epoll我可以直接去撞墙了。

## Linux下I/O多路复用系统调用\(select, poll, epoll\)介绍

I/O多路复用（multiplexing）的本质是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。

Linux中基于socket的通信本质也是一种I/O，使用socket\(\)函数创建的套接字默认都是阻塞的，这意味着当sockets API的调用不能立即完成时，线程一直处于等待状态，直到操作完成获得结果或者超时出错。会引起阻塞的socket API分为以下四种：



* 输入操作： recv\(\)、recvfrom\(\)。以阻塞套接字为参数调用该函数接收数据时，如果套接字缓冲区内没有数据可读，则调用线程在数据到来前一直睡眠。 
* 输出操作： send\(\)、sendto\(\)。以阻塞套接字为参数调用该函数发送数据时，如果套接字缓冲区没有可用空间，线程会一直睡眠，直到有空间。 
* 接受连接：accept\(\)。以阻塞套接字为参数调用该函数，等待接受对方的连接请求。如果此时没有连接请求，线程就会进入睡眠状态。 
* 外出连接：connect\(\)。对于TCP连接，客户端以阻塞套接字为参数，调用该函数向服务器发起连接。该函数在收到服务器的应答前，不会返回。这意味着TCP连接总会等待至少服务器的一次往返时间。



## [Linux IO模式及 select、poll、epoll详解](https://segmentfault.com/a/1190000003063859)

[![](https://avatar-static.segmentfault.com/156/508/1565088343-557fe8ba01bce_big64)**人云思云**](https://segmentfault.com/u/rysy)发布于 2015-08-07

> 注：本文是对众多博客的学习和总结，可能存在理解错误。请带着怀疑的眼光，同时如果有错误希望能指出。

同步IO和异步IO，阻塞IO和非阻塞IO分别是什么，到底有什么区别？不同的人在不同的上下文下给出的答案是不同的。所以先限定一下本文的上下文。

```text
本文讨论的背景是Linux环境下的network IO。
```

## 一 概念说明 <a id="item-1"></a>

在进行解释之前，首先要说明几个概念：  
- 用户空间和内核空间  
- 进程切换  
- 进程的阻塞  
- 文件描述符  
- 缓存 I/O

### 用户空间与内核空间 <a id="item-1-1"></a>

现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。

### 进程切换 <a id="item-1-2"></a>

为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。

从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：  
1. 保存处理机上下文，包括程序计数器和其他寄存器。  
2. 更新PCB信息。  
3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。  
4. 选择另一个进程执行，并更新其PCB。  
5. 更新内存管理的数据结构。  
6. 恢复处理机上下文。

注：**总而言之就是很耗资源**，具体的可以参考这篇文章：[进程切换](http://guojing.me/linux-kernel-architecture/posts/process-switch/)

### 进程的阻塞 <a id="item-1-3"></a>

正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语\(Block\)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。`当进程进入阻塞状态，是不占用CPU资源的`。

### 文件描述符fd <a id="item-1-4"></a>

文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。

文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

### 缓存 I/O <a id="item-1-5"></a>

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

**缓存 I/O 的缺点：**  
数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。

## 二 IO模式 <a id="item-2"></a>

刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：  
1. 等待数据准备 \(Waiting for the data to be ready\)  
2. 将数据从内核拷贝到进程中 \(Copying the data from the kernel to the process\)

正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。  
- 阻塞 I/O（blocking IO）  
- 非阻塞 I/O（nonblocking IO）  
- I/O 多路复用（ IO multiplexing）  
- 信号驱动 I/O（ signal driven IO）  
- 异步 I/O（asynchronous IO）

注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。

### 阻塞 I/O（blocking IO） <a id="item-2-6"></a>

在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：  


当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

> 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。

### 非阻塞 I/O（nonblocking IO） <a id="item-2-7"></a>

linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：  


当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

> 所以，nonblocking IO的特点是用户进程需要**不断的主动询问**kernel数据好了没有。

### I/O 多路复用（ IO multiplexing） <a id="item-2-8"></a>

IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。

`当用户进程调用了select，那么整个进程会被block`，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

> 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select\(\)函数就可以返回。

这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call \(select 和 recvfrom\)，而blocking IO只调用了一个system call \(recvfrom\)。但是，用select的优势在于它可以同时处理多个connection。

所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。

### 异步 I/O（asynchronous IO） <a id="item-2-9"></a>

inux下的asynchronous IO其实用得很少。先看一下它的流程：  


用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。

### 总结 <a id="item-2-10"></a>

#### blocking和non-blocking的区别

调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。

#### synchronous IO和asynchronous IO的区别

在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的：  
- A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;  
- An asynchronous I/O operation does not cause the requesting process to be blocked;

两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。

有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。

而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。

**各个IO Model的比较如图所示：**  


通过上面的图片，可以发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。

## 三 I/O 多路复用之select、poll、epoll详解 <a id="item-3"></a>

select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下）

### select <a id="item-3-11"></a>

```text
int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。

select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。

### poll <a id="item-3-12"></a>

```text
int poll (struct pollfd *fds, unsigned int nfds, int timeout);
```

不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。

```text
struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。

> 从上面看，select和poll都需要在返回后，`通过遍历文件描述符来获取已经就绪的socket`。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

### epoll <a id="item-3-13"></a>

epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

#### 一 epoll操作过程

epoll操作过程需要三个接口，分别如下：

```text
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

**1. int epoll\_create\(int size\);**  
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select\(\)中的第一个参数，给出最大监听的fd+1的值，`参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议`。  
当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close\(\)关闭，否则可能导致fd被耗尽。

**2. int epoll\_ctl\(int epfd, int op, int fd, struct epoll\_event \*event\)；**  
函数是对指定描述符fd执行op操作。  
- epfd：是epoll\_create\(\)的返回值。  
- op：表示op操作，用三个宏来表示：添加EPOLL\_CTL\_ADD，删除EPOLL\_CTL\_DEL，修改EPOLL\_CTL\_MOD。分别添加、删除和修改对fd的监听事件。  
- fd：是需要监听的fd（文件描述符）  
- epoll\_event：是告诉内核需要监听什么事，struct epoll\_event结构如下：

```text
struct epoll_event {
  __uint32_t events;  /* Epoll events */
  epoll_data_t data;  /* User data variable */
};

//events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
```

**3. int epoll\_wait\(int epfd, struct epoll\_event \* events, int maxevents, int timeout\);**  
等待epfd上的io事件，最多返回maxevents个事件。  
参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll\_create\(\)时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。

#### 二 工作模式

　epoll对文件描述符的操作有两种模式：**LT（level trigger）**和**ET（edge trigger）**。LT模式是默认模式，LT模式与ET模式的区别如下：  
　　**LT模式**：当epoll\_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序可以不立即处理该事件`。下次调用epoll\_wait时，会再次响应应用程序并通知此事件。  
　　**ET模式**：当epoll\_wait检测到描述符事件发生并将此事件通知应用程序，`应用程序必须立即处理该事件`。如果不处理，下次调用epoll\_wait时，不会再次响应应用程序并通知此事件。

**1. LT模式**

LT\(level triggered\)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。

**2. ET模式**

ET\(edge-triggered\)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了\(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作\(从而导致它再次变成未就绪\)，内核不会发送更多的通知\(only once\)

ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

**3. 总结**

**假如有这样一个例子：**  
1. 我们已经把一个用来从管道中读取数据的文件句柄\(RFD\)添加到epoll描述符  
2. 这个时候从管道的另一端被写入了2KB的数据  
3. 调用epoll\_wait\(2\)，并且它会返回RFD，说明它已经准备好读取操作  
4. 然后我们读取了1KB的数据  
5. 调用epoll\_wait\(2\)......

**LT模式：**  
如果是LT模式，那么在第5步调用epoll\_wait\(2\)之后，仍然能受到通知。

**ET模式：**  
如果我们在第1步将RFD添加到epoll描述符的时候使用了EPOLLET标志，那么在第5步调用epoll\_wait\(2\)之后将有可能会挂起，因为剩余的数据还存在于文件的输入缓冲区内，而且数据发出端还在等待一个针对已经发出数据的反馈信息。只有在监视的文件句柄上发生了某个事件的时候 ET 工作模式才会汇报事件。因此在第5步的时候，调用者可能会放弃等待仍在存在于文件输入缓冲区内的剩余数据。

当使用epoll的ET模型来工作时，当产生了一个EPOLLIN事件后，  
读数据的时候需要考虑的是当recv\(\)返回的大小如果等于请求的大小，那么很有可能是缓冲区还有数据未读完，也意味着该次事件还没有处理完，所以还需要再次读取：

```text
while(rs){
  buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0);
  if(buflen < 0){
    // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读
    // 在这里就当作是该次事件已处理处.
    if(errno == EAGAIN){
        break;
    }
    else{
        return;
    }
  }
  else if(buflen == 0){
     // 这里表示对端的socket已正常关闭.
  }

 if(buflen == sizeof(buf){
      rs = 1;   // 需要再次读取
 }
 else{
      rs = 0;
 }
}
```

> **Linux中的EAGAIN含义**

Linux环境下开发经常会碰到很多错误\(设置errno\)，其中EAGAIN是其中比较常见的一个错误\(比如用在非阻塞操作中\)。  
从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞\(non-blocking\)操作\(对文件或socket\)的时候。

例如，以 O\_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。  
又例如，当一个系统调用\(比如fork\)因为没有足够的资源\(比如虚拟内存\)而执行失败，返回EAGAIN提示其再调用一次\(也许下次就能成功\)。

#### 三 代码演示

下面是一段不完整的代码且格式不对，意在表述上面的过程，去掉了一些模板代码。

```text
#define IPADDRESS   "127.0.0.1"
#define PORT        8787
#define MAXSIZE     1024
#define LISTENQ     5
#define FDSIZE      1000
#define EPOLLEVENTS 100

listenfd = socket_bind(IPADDRESS,PORT);

struct epoll_event events[EPOLLEVENTS];

//创建一个描述符
epollfd = epoll_create(FDSIZE);

//添加监听描述符事件
add_event(epollfd,listenfd,EPOLLIN);

//循环等待
for ( ; ; ){
    //该函数返回已经准备好的描述符事件数目
    ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1);
    //处理接收到的连接
    handle_events(epollfd,events,ret,listenfd,buf);
}

//事件处理函数
static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf)
{
     int i;
     int fd;
     //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。
     for (i = 0;i < num;i++)
     {
         fd = events[i].data.fd;
        //根据描述符的类型和事件类型进行处理
         if ((fd == listenfd) &&(events[i].events & EPOLLIN))
            handle_accpet(epollfd,listenfd);
         else if (events[i].events & EPOLLIN)
            do_read(epollfd,fd,buf);
         else if (events[i].events & EPOLLOUT)
            do_write(epollfd,fd,buf);
     }
}

//添加事件
static void add_event(int epollfd,int fd,int state){
    struct epoll_event ev;
    ev.events = state;
    ev.data.fd = fd;
    epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&ev);
}

//处理接收到的连接
static void handle_accpet(int epollfd,int listenfd){
     int clifd;     
     struct sockaddr_in cliaddr;     
     socklen_t  cliaddrlen;     
     clifd = accept(listenfd,(struct sockaddr*)&cliaddr,&cliaddrlen);     
     if (clifd == -1)         
     perror("accpet error:");     
     else {         
         printf("accept a new client: %s:%d\n",inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port);                       //添加一个客户描述符和事件         
         add_event(epollfd,clifd,EPOLLIN);     
     } 
}

//读处理
static void do_read(int epollfd,int fd,char *buf){
    int nread;
    nread = read(fd,buf,MAXSIZE);
    if (nread == -1)     {         
        perror("read error:");         
        close(fd); //记住close fd        
        delete_event(epollfd,fd,EPOLLIN); //删除监听 
    }
    else if (nread == 0)     {         
        fprintf(stderr,"client close.\n");
        close(fd); //记住close fd       
        delete_event(epollfd,fd,EPOLLIN); //删除监听 
    }     
    else {         
        printf("read message is : %s",buf);        
        //修改描述符对应的事件，由读改为写         
        modify_event(epollfd,fd,EPOLLOUT);     
    } 
}

//写处理
static void do_write(int epollfd,int fd,char *buf) {     
    int nwrite;     
    nwrite = write(fd,buf,strlen(buf));     
    if (nwrite == -1){         
        perror("write error:");        
        close(fd);   //记住close fd       
        delete_event(epollfd,fd,EPOLLOUT);  //删除监听    
    }else{
        modify_event(epollfd,fd,EPOLLIN); 
    }    
    memset(buf,0,MAXSIZE); 
}

//删除事件
static void delete_event(int epollfd,int fd,int state) {
    struct epoll_event ev;
    ev.events = state;
    ev.data.fd = fd;
    epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&ev);
}

//修改事件
static void modify_event(int epollfd,int fd,int state){     
    struct epoll_event ev;
    ev.events = state;
    ev.data.fd = fd;
    epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&ev);
}

//注：另外一端我就省了
```

#### 四 epoll总结

在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而**epoll事先通过epoll\_ctl\(\)来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll\_wait\(\) 时便得到通知**。\(`此处去掉了遍历文件描述符，而是通过监听回调的的机制`。这正是epoll的魅力所在。\)

**epoll的优点主要是一下几个方面：**  
1. 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案\( Apache就是这样实现的\)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。

1. IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。

> 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。

## 参考 <a id="item-4"></a>

[用户空间与内核空间，进程上下文与中断上下文\[总结\]](http://www.cnblogs.com/Anker/p/3269106.html)  
[进程切换](http://guojing.me/linux-kernel-architecture/posts/process-switch/)  
[维基百科-文件描述符](https://zh.wikipedia.org/wiki/%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6)  
[Linux 中直接 I/O 机制的介绍](http://www.ibm.com/developerworks/cn/linux/l-cn-directio/)  
[IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）](http://blog.csdn.net/historyasamirror/article/details/5778378)  
[Linux中select poll和epoll的区别](http://www.cnblogs.com/bigwangdi/p/3182958.html)  
[IO多路复用之select总结](http://www.cnblogs.com/Anker/archive/2013/08/14/3258674.html)  
[IO多路复用之poll总结](http://www.cnblogs.com/Anker/archive/2013/08/15/3261006.html)  
[IO多路复用之epoll总结](http://www.cnblogs.com/Anker/archive/2013/08/17/3263780.html)

## Epoll的本质

从事服务端开发，少不了要接触网络编程。epoll作为linux下高性能网络服务器的必备技术至关重要，nginx、redis、skynet和大部分游戏服务器都使用到这一多路复用技术。

**文**/罗培羽

因为epoll的重要性，不少游戏公司（如某某九九）在招聘服务端同学时，可能会问及epoll相关的问题。比如epoll和select的区别是什么？epoll高效率的原因是什么？如果只靠背诵，显然不能算上深刻的理解。

网上虽然也有不少讲解epoll的文章，但要不是过于浅显，就是陷入源码解析，很少能有通俗易懂的。于是决定编写此文，让缺乏专业背景知识的读者也能够明白epoll的原理。文章核心思想是：

### **要让读者清晰明白EPOLL为什么性能好。**

本文会从网卡接收数据的流程讲起，串联起CPU中断、操作系统进程调度等知识；再一步步分析阻塞接收数据、select到epoll的进化过程；最后探究epoll的实现细节。目录：

> 一、从网卡接收数据说起  
> 二、如何知道接收了数据？  
> 三、进程阻塞为什么不占用cpu资源？  
> 四、内核接收网络数据全过程  
> 五、同时监视多个socket的简单方法  
> 六、epoll的设计思路  
> 七、epoll的原理和流程  
> 八、epoll的实现细节  
> 九、结论

### **一、从网卡接收数据说起**

下图是一个典型的计算机结构图，计算机由CPU、存储器（内存）、网络接口等部件组成。了解epoll本质的**第一步**，要从**硬件**的角度看计算机怎样接收网络数据。![](https://pic2.zhimg.com/80/v2-e549406135abf440331de9dd8c3925e9_1440w.jpg)计算机结构图（图片来源：linux内核完全注释之微型计算机组成结构）

下图展示了网卡接收数据的过程。在①阶段，网卡收到网线传来的数据；经过②阶段的硬件电路的传输；最终将数据写入到内存中的某个地址上（③阶段）。这个过程涉及到DMA传输、IO通路选择等硬件有关的知识，但我们只需知道：**网卡会把接收到的数据写入内存。**![](https://pic2.zhimg.com/80/v2-6827b63c9fb42823dcd1913ea5433b15_1440w.jpg)网卡接收数据的过程

通过硬件传输，网卡接收的数据存放到内存中。操作系统就可以去读取它们。

### **二、如何知道接收了数据？**

了解epoll本质的**第二步**，要从**CPU**的角度来看数据接收。要理解这个问题，要先了解一个概念——中断。

计算机执行程序时，会有优先级的需求。比如，当计算机收到断电信号时（电容可以保存少许电量，供CPU运行很短的一小段时间），它应立即去保存数据，保存数据的程序具有较高的优先级。

一般而言，由硬件产生的信号需要cpu立马做出回应（不然数据可能就丢失），所以它的优先级很高。cpu理应中断掉正在执行的程序，去做出响应；当cpu完成对硬件的响应后，再重新执行用户程序。中断的过程如下图，和函数调用差不多。只不过函数调用是事先定好位置，而中断的位置由“信号”决定。![](https://pic4.zhimg.com/80/v2-89a9490f1d5c316167ff4761184239f7_1440w.jpg)中断程序调用

以键盘为例，当用户按下键盘某个按键时，键盘会给cpu的中断引脚发出一个高电平。cpu能够捕获这个信号，然后执行键盘中断程序。下图展示了各种硬件通过中断与cpu交互。![](https://pic3.zhimg.com/80/v2-c756381c0f63f9104f9102d280759d22_1440w.jpg)cpu中断（图片来源：net.pku.edu.cn）

现在可以回答本节提出的问题了：当网卡把数据写入到内存后，**网卡向cpu发出一个中断信号，操作系统便能得知有新数据到来**，再通过网卡**中断程序**去处理数据。

### **三、进程阻塞为什么不占用cpu资源？**

了解epoll本质的**第三步**，要从**操作系统进程调度**的角度来看数据接收。阻塞是进程调度的关键一环，指的是进程在等待某事件（如接收到网络数据）发生之前的等待状态，recv、select和epoll都是阻塞方法。**了解“进程阻塞为什么不占用cpu资源？”，也就能够了解这一步**。

为简单起见，我们从普通的recv接收开始分析，先看看下面代码：

```text
//创建socket
int s = socket(AF_INET, SOCK_STREAM, 0);   
//绑定
bind(s, ...)
//监听
listen(s, ...)
//接受客户端连接
int c = accept(s, ...)
//接收客户端数据
recv(c, ...);
//将数据打印出来
printf(...)
```

这是一段最基础的网络编程代码，先新建socket对象，依次调用bind、listen、accept，最后调用recv接收数据。recv是个阻塞方法，当程序运行到recv时，它会一直等待，直到接收到数据才往下执行。

> 插入：如果您还不太熟悉网络编程，欢迎阅读我编写的《Unity3D网络游戏实战\(第2版\)》，会有详细的介绍。

那么阻塞的原理是什么？

**工作队列**

操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待”等几种状态。运行状态是进程获得cpu使用权，正在执行代码的状态；等待状态是阻塞状态，比如上述程序运行到recv时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务。

下图中的计算机中运行着A、B、C三个进程，其中进程A执行着上述基础网络程序，一开始，这3个进程都被操作系统的工作队列所引用，处于运行状态，会分时执行。![](https://pic3.zhimg.com/80/v2-2f3b71710f1805669a780a2d634f0626_1440w.jpg)工作队列中有A、B和C三个进程

**等待队列**

当进程A执行到创建socket的语句时，操作系统会创建一个由文件系统管理的socket对象（如下图）。这个socket对象包含了发送缓冲区、接收缓冲区、等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该socket事件的进程。![](https://pic3.zhimg.com/80/v2-7ce207c92c9dd7085fb7b823e2aa5872_1440w.jpg)创建socket

当程序执行到recv时，操作系统会将进程A从工作队列移动到该socket的等待队列中（如下图）。由于工作队列只剩下了进程B和C，依据进程调度，cpu会轮流执行这两个进程的程序，不会执行进程A的程序。**所以进程A被阻塞，不会往下执行代码，也不会占用cpu资源**。![](https://pic1.zhimg.com/80/v2-1c7a96c8da16f123388e46f88772e6d8_1440w.jpg)socket的等待队列

ps：操作系统添加等待队列只是添加了对这个“等待中”进程的引用，以便在接收到数据时获取进程对象、将其唤醒，而非直接将进程管理纳入自己之下。上图为了方便说明，直接将进程挂到等待队列之下。

**唤醒进程**

当socket接收到数据后，操作系统将该socket等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。也由于socket的接收缓冲区已经有了数据，recv可以返回接收到的数据。

**以下内容待续**

四、内核接收网络数据全过程

五、同时监视多个socket的简单方法

六、epoll的设计思路

七、epoll的原理和流程

八、epoll的实现细节

九、结论

既然说到网络编程，笔者的**《Unity3D网络游戏实战（第2版）》**是一本专门介绍如何开发**多人网络游戏**的书籍，用实例介绍开发游戏的全过程，非常实用。书中对网络编程有详细的讲解，全书用一个大例子贯穿，真正的“实战”教程。

致谢：本文力图详细说明epoll的原理，特别感谢 [@陆俊壕](https://www.zhihu.com/people/e622f8ea68620104614bcc9a4ce3855d) [@AllenKong12](https://www.zhihu.com/people/8887d646fe997ca00f7ff99b724dd230) 雄爷、堂叔 等同事审阅了文章并给予修改意见。![](https://pic1.zhimg.com/80/v2-6ab25b1164ab427bdbe7eccaff7e9570_1440w.jpg)  
**上篇回顾**

一、从网卡接收数据说起

二、如何知道接收了数据？

三、进程阻塞为什么不占用cpu资源？

**系列文章**

[罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （1）](https://zhuanlan.zhihu.com/p/63179839)

[罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （2）](https://zhuanlan.zhihu.com/p/64138532)

[罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （3）](https://zhuanlan.zhihu.com/p/64746509)

### **四、内核接收网络数据全过程**

**这一步，贯穿网卡、中断、进程调度的知识，叙述阻塞recv下，内核接收数据全过程。**

如下图所示，进程在recv阻塞期间，计算机收到了对端传送的数据（步骤①）。数据经由网卡传送到内存（步骤②），然后网卡通过中断信号通知cpu有数据到达，cpu执行中断程序（步骤③）。此处的中断程序主要有两项功能，先将网络数据写入到对应socket的接收缓冲区里面（步骤④），再唤醒进程A（步骤⑤），重新将进程A放入工作队列中。![](https://pic4.zhimg.com/80/v2-696b131cae434f2a0b5ab4d6353864af_1440w.jpg)内核接收数据全过程

唤醒进程的过程如下图所示。![](https://pic3.zhimg.com/80/v2-3e1d0a82cdc86f03343994f48d938922_1440w.jpg)唤醒进程

**以上是内核接收数据全过程**

这里留有两个思考题，大家先想一想。

其一，操作系统如何知道网络数据对应于哪个socket？

其二，如何同时监视多个socket的数据？

（——我是分割线，想好了才能往下看哦~）

公布答案的时刻到了。

第一个问题：因为一个socket对应着一个端口号，而网络数据包中包含了ip和端口的信息，内核可以通过端口号找到对应的socket。当然，为了提高处理速度，操作系统会维护端口号到socket的索引结构，以快速读取。

第二个问题是**多路复用的重中之重，**是本文后半部分的重点！

### **五、同时监视多个socket的简单方法**

服务端需要管理多个客户端连接，而recv只能监视单个socket，这种矛盾下，人们开始寻找监视多个socket的方法。epoll的要义是**高效**的监视多个socket。从历史发展角度看，必然先出现一种不太高效的方法，人们再加以改进。只有先理解了不太高效的方法，才能够理解epoll的本质。

假如能够预先传入一个socket列表，**如果列表中的socket都没有数据，挂起进程，直到有一个socket收到数据，唤醒进程**。这种方法很直接，也是select的设计思想。

为方便理解，我们先复习select的用法。在如下的代码中，先准备一个数组（下面代码中的fds），让fds存放着所有需要监视的socket。然后调用select，如果fds中的所有socket都没有数据，select会阻塞，直到有一个socket接收到数据，select返回，唤醒进程。用户可以遍历fds，通过FD\_ISSET判断具体哪个socket收到数据，然后做出处理。

```text
int s = socket(AF_INET, SOCK_STREAM, 0);  
bind(s, ...)
listen(s, ...)

int fds[] =  存放需要监听的socket

while(1){
    int n = select(..., fds, ...)
    for(int i=0; i < fds.count; i++){
        if(FD_ISSET(fds[i], ...)){
            //fds[i]的数据处理
        }
    }
}
```

**select的流程**

select的实现思路很直接。假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中。![](https://pic4.zhimg.com/80/v2-0cccb4976f8f2c2f8107f2b3a5bc46b3_1440w.jpg)操作系统把进程A分别加入这三个socket的等待队列中

当任何一个socket收到数据后，中断程序将唤起进程。下图展示了sock2接收到了数据的处理流程。

> ps：recv和select的中断回调可以设置成不同的内容。

![](https://pic1.zhimg.com/80/v2-85dba5430f3c439e4647ea4d97ba54fc_1440w.jpg)sock2接收到了数据，中断程序唤起进程A

所谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面。如下图所示。![](https://pic4.zhimg.com/80/v2-a86b203b8d955466fff34211d965d9eb_1440w.jpg)将进程A从所有等待队列中移除，再加入到工作队列里面

经由这些步骤，当进程A被唤醒后，它知道至少有一个socket接收了数据。程序只需遍历一遍socket列表，就可以得到就绪的socket。

这种简单方式**行之有效**，在几乎所有操作系统都有对应的实现。

**但是简单的方法往往有缺点，主要是：**

其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。

其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。

那么，有没有减少遍历的方法？有没有保存就绪socket的方法？这两个问题便是epoll技术要解决的。

> 补充说明： 本节只解释了select的一种情形。当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞。

### **六、epoll的设计思路**

epoll是在select出现N多年后才被发明的，是select和poll的增强版本。epoll通过以下一些措施来改进效率。

**措施一：功能分离**

select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用select都需要这两步操作，然而大多数应用场景中，需要监视的socket相对固定，并不需要每次都修改。epoll将这两个操作分开，先用epoll\_ctl维护等待队列，再调用epoll\_wait阻塞进程。显而易见的，效率就能得到提升。![](https://pic2.zhimg.com/80/v2-5ce040484bbe61df5b484730c4cf56cd_1440w.jpg)相比select，epoll拆分了功能

为方便理解后续的内容，我们先复习下epoll的用法。如下的代码中，先用epoll\_create创建一个epoll对象epfd，再通过epoll\_ctl将需要监视的socket添加到epfd中，最后调用epoll\_wait等待数据。

```text
int s = socket(AF_INET, SOCK_STREAM, 0);   
bind(s, ...)
listen(s, ...)

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中

while(1){
    int n = epoll_wait(...)
    for(接收到数据的socket){
        //处理
    }
}
```

功能分离，使得epoll有了优化的可能。

**措施二：就绪列表**

select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。如下图所示，计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据。![](https://pic4.zhimg.com/80/v2-5c552b74772d8dbc7287864999e32c4f_1440w.jpg)就绪列表示意图

**以下内容待续**

七、epoll的原理和流程

八、epoll的实现细节

九、结论

除了网络编程，「同步」也是网络游戏开发的核心课题。多人游戏中，玩家在世界中的位置旋转以及各种属性都会对游戏表现产生影响，需要同步给其他玩家。然而由于网络通信存在延迟和抖动，很难做到完美的同步效果。如果没能处理好，游戏将不同步和卡顿，这是玩家所不能容忍的。笔者即将开展一场知乎live**《网络游戏同步算法》**，分析同步技术，欢迎收听。[网络游戏同步算法​www.zhihu.co](https://www.zhihu.com/lives/1104162893850898432)

**上篇回顾**

四、内核接收网络数据全过程

五、同时监视多个socket的简单方法

六、epoll的设计思路

**系列文章**

[罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （1）](https://zhuanlan.zhihu.com/p/63179839)

[罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （2）](https://zhuanlan.zhihu.com/p/64138532)

[罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！ （3）](https://zhuanlan.zhihu.com/p/64746509)

### **七、epoll的原理和流程**

本节会以示例和图表来讲解epoll的原理和流程。

**创建epoll对象**

如下图所示，当某个进程调用epoll\_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列。![](https://pic4.zhimg.com/80/v2-e3467895734a9d97f0af3c7bf875aaeb_1440w.jpg)内核创建eventpoll对象

创建一个代表该epoll的eventpoll对象是必须的，因为内核要维护“就绪列表”等数据，“就绪列表”可以作为eventpoll的成员。

**维护监视列表**

创建epoll对象后，可以用epoll\_ctl添加或删除所要监听的socket。以添加socket为例，如下图，如果通过epoll\_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中。![](https://pic2.zhimg.com/80/v2-b49bb08a6a1b7159073b71c4d6591185_1440w.jpg)添加所要监听的socket

当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程。

**接收数据**

当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用。如下图展示的是sock2和sock3收到数据后，中断程序让rdlist引用这两个socket。![](https://pic1.zhimg.com/80/v2-18b89b221d5db3b5456ab6a0f6dc5784_1440w.jpg)给就绪列表添加引用

eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。

当程序执行到epoll\_wait时，如果rdlist已经引用了socket，那么epoll\_wait直接返回，如果rdlist为空，阻塞进程。

**阻塞和唤醒进程**

假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll\_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程。![](https://pic1.zhimg.com/80/v2-90632d0dc3ded7f91379b848ab53974c_1440w.jpg)epoll\_wait阻塞进程

当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。![](https://pic4.zhimg.com/80/v2-40bd5825e27cf49b7fd9a59dfcbe4d6f_1440w.jpg)epoll唤醒进程

### **八、epoll的实现细节**

至此，相信读者对epoll的本质已经有一定的了解。但我们还留有一个问题，**eventpoll的数据结构**是什么样子？

再留两个问题，**就绪队列**应该应使用什么数据结构？eventpoll应使用什么数据结构来管理通过epoll\_ctl添加或删除的socket？

（——我是分割线，想好了才能往下看哦~）

如下图所示，eventpoll包含了lock、mtx、wq（等待队列）、rdlist等成员。rdlist和rbr是我们所关心的。![](https://pic4.zhimg.com/80/v2-e63254878f67751dcc07a25b93f974bb_1440w.jpg)epoll原理示意图，图片来源：《深入理解Nginx：模块开发与架构解析\(第二版\)》，陶辉

**就绪列表的数据结构**

就绪列表引用着就绪的socket，所以它应能够快速的插入数据。

程序可能随时调用epoll\_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。

所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）。

**索引结构**

既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O\(log\(N\)\)，效率较好。epoll使用了红黑树作为索引结构（对应上图的rbr）。

> ps：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist并非直接引用socket，而是通过epitem间接引用，红黑树的节点也是epitem对象。同样，文件系统也并非直接引用着socket。为方便理解，本文中省略了一些间接结构。

### **九、结论**

epoll在select和poll（poll和select基本一样，有少量改进）的基础引入了eventpoll作为中间层，使用了先进的数据结构，是一种高效的多路复用技术。

再留一点**作业**！

下表是个很常见的表，描述了select、poll和epoll的区别。读完本文，读者能否解释select和epoll的时间复杂度为什么是O\(n\)和O\(1\)？![](https://pic2.zhimg.com/80/v2-14e0536d872474b0851b62572b732e39_1440w.jpg)select、poll和epoll的区别。图片来源《Linux高性能服务器编程》

笔者的**《Unity3D网络游戏实战（第2版）》**是一本专门介绍如何开发**多人网络游戏**的书籍，用实例介绍开发游戏的全过程，手把手教你如何制作一款多人开房间的坦克对战游戏。

「同步」也是网络游戏开发的核心课题。如何恰当的使用不同的同步算法？帧同步的应用场景和优越有哪些？笔者即将开展一场知乎live**《网络游戏同步算法》**，欢迎收听。[网络游戏同步算法​www.zhihu.com](https://www.zhihu.com/lives/1104162893850898432)

## Epoll使用详解

在linux的网络编程中，很长的时间都在使用select来做事件触发。在linux新的内核中，有了一种替换它的机制，就是epoll。  
相比于select，epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。因为在内核中的select实现中，它是采用轮询来处理的，轮询的fd数目越多，自然耗时越多。并且，在linux/posix\_types.h头文件有这样的声明：  
\#define \_\_FD\_SETSIZE    1024  
表示select最多同时监听1024个fd，当然，可以通过修改头文件再重编译内核来扩大这个数目，但这似乎并不治本。  
  
epoll的接口非常简单，一共就三个函数：  
1. int epoll\_create\(int size\);  
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select\(\)中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close\(\)关闭，否则可能导致fd被耗尽。  
  
  
2. int epoll\_ctl\(int epfd, int op, int fd, struct epoll\_event \*event\);  
epoll的事件注册函数，它不同与select\(\)是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。第一个参数是epoll\_create\(\)的返回值，第二个参数表示动作，用三个宏来表示：  
EPOLL\_CTL\_ADD：注册新的fd到epfd中；  
EPOLL\_CTL\_MOD：修改已经注册的fd的监听事件；  
EPOLL\_CTL\_DEL：从epfd中删除一个fd；  
第三个参数是需要监听的fd，第四个参数是告诉内核需要监听什么事，struct epoll\_event结构如下：  
  
[![&#x590D;&#x5236;&#x4EE3;&#x7801;](https://common.cnblogs.com/images/copycode.gif)](javascript:void%280%29;)

```text
 1 typedef union epoll_data {
 2     void *ptr;
 3     int fd;
 4     __uint32_t u32;
 5     __uint64_t u64;
 6 } epoll_data_t;
 7 
 8 struct epoll_event {
 9     __uint32_t events; /* Epoll events */
10     epoll_data_t data; /* User data variable */
11 };
```

[![&#x590D;&#x5236;&#x4EE3;&#x7801;](https://common.cnblogs.com/images/copycode.gif)](javascript:void%280%29;)

  
events可以是以下几个宏的集合：  
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；  
EPOLLOUT：表示对应的文件描述符可以写；  
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；  
EPOLLERR：表示对应的文件描述符发生错误；  
EPOLLHUP：表示对应的文件描述符被挂断；  
EPOLLET： 将EPOLL设为边缘触发\(Edge Triggered\)模式，这是相对于水平触发\(Level Triggered\)来说的。  
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里  
  
  
3. int epoll\_wait\(int epfd, struct epoll\_event \* events, int maxevents, int timeout\);  
等待事件的产生，类似于select\(\)调用。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个 maxevents的值不能大于创建epoll\_create\(\)时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。  
  
  
4、关于ET、LT两种工作模式：  
可以得出这样的结论:  
ET模式仅当状态发生变化的时候才获得通知,这里所谓的状态的变化并不包括缓冲区中还有未处理的数据,也就是说,如果要采用ET模式,需要一直read/write直到出错为止,很多人反映为什么采用ET模式只接收了一部分数据就再也得不到通知了,大多因为这样;而LT模式是只要有数据没有处理就会一直通知下去的.  
  
  
那么究竟如何来使用epoll呢？其实非常简单。  
通过在包含一个头文件\#include &lt;sys/epoll.h&gt; 以及几个简单的API将可以大大的提高你的网络服务器的支持人数。  
  
首先通过create\_epoll\(int maxfds\)来创建一个epoll的句柄，其中maxfds为你epoll所支持的最大句柄数。这个函数会返回一个新的epoll句柄，之后的所有操作将通过这个句柄来进行操作。在用完之后，记得用close\(\)来关闭这个创建出来的epoll句柄。  
  
之后在你的网络主循环里面，每一帧的调用epoll\_wait\(int epfd, epoll\_event events, int max events, int timeout\)来查询所有的网络接口，看哪一个可以读，哪一个可以写了。基本的语法为：  
nfds = epoll\_wait\(kdpfd, events, maxevents, -1\);  
其中kdpfd为用epoll\_create创建之后的句柄，events是一个epoll\_event\*的指针，当epoll\_wait这个函数操作成功之后，epoll\_events里面将储存所有的读写事件。max\_events是当前需要监听的所有socket句柄数。最后一个timeout是 epoll\_wait的超时，为0的时候表示马上返回，为-1的时候表示一直等下去，直到有事件范围，为任意正整数的时候表示等这么长的时间，如果一直没有事件，则范围。一般如果网络主循环是单独的线程的话，可以用-1来等，这样可以保证一些效率，如果是和主逻辑在同一个线程的话，则可以用0来保证主循环的效率。  
  
epoll\_wait范围之后应该是一个循环，遍利所有的事件。  
  
几乎所有的epoll程序都使用下面的框架：  
  
   [![&#x590D;&#x5236;&#x4EE3;&#x7801;](https://common.cnblogs.com/images/copycode.gif)](javascript:void%280%29;)

```text
 1  for( ; ; )
 2     {
 3         nfds = epoll_wait(epfd,events,20,500);
 4         for(i=0;i<nfds;++i)
 5         {
 6             if(events[i].data.fd==listenfd) //有新的连接
 7             {
 8                 connfd = accept(listenfd,(sockaddr *)&clientaddr, &clilen); //accept这个连接
 9                 ev.data.fd=connfd;
10                 ev.events=EPOLLIN|EPOLLET;
11                 epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&ev); //将新的fd添加到epoll的监听队列中
12             }
13             else if( events[i].events&EPOLLIN ) //接收到数据，读socket
14             {
15                 n = read(sockfd, line, MAXLINE)) < 0    //读
16                 ev.data.ptr = md;     //md为自定义类型，添加数据
17                 ev.events=EPOLLOUT|EPOLLET;
18                 epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev);//修改标识符，等待下一个循环时发送数据，异步处理的精髓
19             }
20             else if(events[i].events&EPOLLOUT) //有数据待发送，写socket
21             {
22                 struct myepoll_data* md = (myepoll_data*)events[i].data.ptr;    //取数据
23                 sockfd = md->fd;
24                 send( sockfd, md->ptr, strlen((char*)md->ptr), 0 );        //发送数据
25                 ev.data.fd=sockfd;
26                 ev.events=EPOLLIN|EPOLLET;
27                 epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev); //修改标识符，等待下一个循环时接收数据
28             }
29             else
30             {
31                 //其他的处理
32             }
33         }
34     }
```

[![&#x590D;&#x5236;&#x4EE3;&#x7801;](https://common.cnblogs.com/images/copycode.gif)](javascript:void%280%29;)

下面给出一个完整的服务器端例子：[![&#x590D;&#x5236;&#x4EE3;&#x7801;](https://common.cnblogs.com/images/copycode.gif)](javascript:void%280%29;)

```text
  1 #include <iostream>
  2 #include <sys/socket.h>
  3 #include <sys/epoll.h>
  4 #include <netinet/in.h>
  5 #include <arpa/inet.h>
  6 #include <fcntl.h>
  7 #include <unistd.h>
  8 #include <stdio.h>
  9 #include <errno.h>
 10 
 11 using namespace std;
 12 
 13 #define MAXLINE 5
 14 #define OPEN_MAX 100
 15 #define LISTENQ 20
 16 #define SERV_PORT 5000
 17 #define INFTIM 1000
 18 
 19 void setnonblocking(int sock)
 20 {
 21     int opts;
 22     opts=fcntl(sock,F_GETFL);
 23     if(opts<0)
 24     {
 25         perror("fcntl(sock,GETFL)");
 26         exit(1);
 27     }
 28     opts = opts|O_NONBLOCK;
 29     if(fcntl(sock,F_SETFL,opts)<0)
 30     {
 31         perror("fcntl(sock,SETFL,opts)");
 32         exit(1);
 33     }
 34 }
 35 
 36 int main(int argc, char* argv[])
 37 {
 38     int i, maxi, listenfd, connfd, sockfd,epfd,nfds, portnumber;
 39     ssize_t n;
 40     char line[MAXLINE];
 41     socklen_t clilen;
 42 
 43 
 44     if ( 2 == argc )
 45     {
 46         if( (portnumber = atoi(argv[1])) < 0 )
 47         {
 48             fprintf(stderr,"Usage:%s portnumber/a/n",argv[0]);
 49             return 1;
 50         }
 51     }
 52     else
 53     {
 54         fprintf(stderr,"Usage:%s portnumber/a/n",argv[0]);
 55         return 1;
 56     }
 57 
 58 
 59 
 60     //声明epoll_event结构体的变量,ev用于注册事件,数组用于回传要处理的事件
 61 
 62     struct epoll_event ev,events[20];
 63     //生成用于处理accept的epoll专用的文件描述符
 64 
 65     epfd=epoll_create(256);
 66     struct sockaddr_in clientaddr;
 67     struct sockaddr_in serveraddr;
 68     listenfd = socket(AF_INET, SOCK_STREAM, 0);
 69     //把socket设置为非阻塞方式
 70 
 71     //setnonblocking(listenfd);
 72 
 73     //设置与要处理的事件相关的文件描述符
 74 
 75     ev.data.fd=listenfd;
 76     //设置要处理的事件类型
 77 
 78     ev.events=EPOLLIN|EPOLLET;
 79     //ev.events=EPOLLIN;
 80 
 81     //注册epoll事件
 82 
 83     epoll_ctl(epfd,EPOLL_CTL_ADD,listenfd,&ev);
 84     bzero(&serveraddr, sizeof(serveraddr));
 85     serveraddr.sin_family = AF_INET;
 86     char *local_addr="127.0.0.1";
 87     inet_aton(local_addr,&(serveraddr.sin_addr));//htons(portnumber);
 88 
 89     serveraddr.sin_port=htons(portnumber);
 90     bind(listenfd,(sockaddr *)&serveraddr, sizeof(serveraddr));
 91     listen(listenfd, LISTENQ);
 92     maxi = 0;
 93     for ( ; ; ) {
 94         //等待epoll事件的发生
 95 
 96         nfds=epoll_wait(epfd,events,20,500);
 97         //处理所发生的所有事件
 98 
 99         for(i=0;i<nfds;++i)
100         {
101             if(events[i].data.fd==listenfd)//如果新监测到一个SOCKET用户连接到了绑定的SOCKET端口，建立新的连接。
102 
103             {
104                 connfd = accept(listenfd,(sockaddr *)&clientaddr, &clilen);
105                 if(connfd<0){
106                     perror("connfd<0");
107                     exit(1);
108                 }
109                 //setnonblocking(connfd);
110 
111                 char *str = inet_ntoa(clientaddr.sin_addr);
112                 cout << "accapt a connection from " << str << endl;
113                 //设置用于读操作的文件描述符
114 
115                 ev.data.fd=connfd;
116                 //设置用于注测的读操作事件
117 
118                 ev.events=EPOLLIN|EPOLLET;
119                 //ev.events=EPOLLIN;
120 
121                 //注册ev
122 
123                 epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&ev);
124             }
125             else if(events[i].events&EPOLLIN)//如果是已经连接的用户，并且收到数据，那么进行读入。
126 
127             {
128                 cout << "EPOLLIN" << endl;
129                 if ( (sockfd = events[i].data.fd) < 0)
130                     continue;
131                 if ( (n = read(sockfd, line, MAXLINE)) < 0) {
132                     if (errno == ECONNRESET) {
133                         close(sockfd);
134                         events[i].data.fd = -1;
135                     } else
136                         std::cout<<"readline error"<<std::endl;
137                 } else if (n == 0) {
138                     close(sockfd);
139                     events[i].data.fd = -1;
140                 }
141                 line[n] = '/0';
142                 cout << "read " << line << endl;
143                 //设置用于写操作的文件描述符
144 
145                 ev.data.fd=sockfd;
146                 //设置用于注测的写操作事件
147 
148                 ev.events=EPOLLOUT|EPOLLET;
149                 //修改sockfd上要处理的事件为EPOLLOUT
150 
151                 //epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev);
152 
153             }
154             else if(events[i].events&EPOLLOUT) // 如果有数据发送
155 
156             {
157                 sockfd = events[i].data.fd;
158                 write(sockfd, line, n);
159                 //设置用于读操作的文件描述符
160 
161                 ev.data.fd=sockfd;
162                 //设置用于注测的读操作事件
163 
164                 ev.events=EPOLLIN|EPOLLET;
165                 //修改sockfd上要处理的事件为EPOLIN
166 
167                 epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&ev);
168             }
169         }
170     }
171     return 0;
172 
```

## 如何理解：程序、进程、线程、并发、并行、高并发？



* 为啥大家说的进程的意思有出入？
* 为啥并发那么难理解？
* 为啥高并发不仅仅是“高”+“并发”的意思？
* 为啥这些概念到了现实当中就不一样了？

可以被称作一个【进程】（Process），或者你可以理解为“一个做事的办法/步骤/方案“。进程的英文Process本意就是“过程”的意思，是一个抽象的概念。这个活有没有真得干并不重要，重要的是你已经预先想好了这个活该怎么干，**有了一个可行思路**。注意，这里【进程】仅仅是描述这个方案的。至于这个方案是在脑海里，还是已经被执行了，是不重要的。

把这套铺路的方案用纸张写出来就得到一个【程序】。在软件开发中也是如此，只不过用的不是纸笔，而是键盘+存储器+某种编程语言。  
  
大家更加熟知的进程往往指的是另外一个意思，是指“程序在操作系统中运行的实例“。所谓“实例”是指同一个程序可以同时在操作系统里实际的运行。就像是如果你的铺路程序写好了，可以铺好几条路。每一个具体的铺路工作是一个“实例”。

In computing, a **process** is the [instance](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Instance_%28computer_science%29) of a [computer program](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Computer_program) that is being executed by one or many threads.

为了避免混淆，我在下文中将操作系统的这个进程概念称为【OS进程】。而对上一节里面讲的“想办法”的进程称为【P进程】

【OS进程】到底怎么实现呢？铺路的工作真的开干时，要不断记录买了什么料，已经花了多少钱，哪一块已经铺好了，哪一块刚铺完沥青得晾着等等。这些信息只有工作真的开干才会有。【OS进程】也是一样，因此比如Linux将进程实现为“task\_struct"，里面记录了CPU要完成这个工作的一整套数据。比如一个事情A，CPU没做完，被程序员要求做另外一件事情B。就得找个地方记录做了一半的A的那些数据，以便于CPU回过头来再做A时能够继续。

* 【P进程】指的是如何想明白做一件事情的过程。他用来帮助你理清做事的思路。这个事情做与没做，对于【P进程】这个概念不重要。
* 【OS进程】是指程序真的运行起来的实例，可以被实现为存放调度给CPU的任务和状态的数据结构。

软件设计里有一个经典的4 + 1 View，其中一个View叫做“Process View”，里面的Process就是指这里的【P进程】。“Process View”的目标就是“把怎么解决问题的方案说明白”。

## 线程

上面wiki的定义指出一个【OS进程】是由一到多个【线程】组成。这里的【线程】（Thread）是一个抽象概念。

但在Linux中，【线程】是被实现为“轻量级进程”的。也就是说在Linux中的进程和线程实现的本质是一样的。只不过在以下2点上有显著区别：

* 在资源消耗上进程的消耗多，线程消耗相对少，以及；
* 内存空间上有一些不同：进程的虚拟内存彼此隔离，而线程则共享同一虚拟内存空间有些不同。

但Linux中【OS进程】和【线程】都用作任务调度单位。因此，**Linux这种实现方式和理论上的概念不是很吻合**，但是大量的程序已经跑在这个模式上了。而且大家早就已经习惯了。其他操作系统对【OS进程】和【线程】的实现会有所不同。如果碰到了不要惊讶。

## 并发

【并发】（Concurrency）是由【P进程】引申出来的抽象概念。

上面说到了你可以假设自己一个人按照一定的步骤来铺路，一个人从头干到尾，这是一个“串行”的【P进程】。  


不管怎样拆，都意味着你得到了【并发】的【P进程】。换成说人话就是，**你有一套方案，可以让多个人一起把事情做的更高效**。注意是“**可以**“让事情更高效，而不是“必然“让事情更高效。是不是更高效要看到底是怎么执行的，后边会讲。

这时就会出现一个问题，当你想把一个【并发】的【P进程】写成程序时，你怎么用编程语言告诉操作系统你的程序的一些步骤是【并发】的。更确切地说，你需要一个写法（可能是语法，也可能是函数库）表达：

* 几个任务是【并发】的
* 【并发】的任务之间是怎么交互协作的

为了解决这两个问题，人们总结了一些方法，并将其称为“并发模型”。  
  
比如：

* Fork & Join模型（大任务拆解为小任务并发的跑，结果再拼起来）
* Actor模型（干活的步骤之间直接发消息）
* CSP模型（干活的步骤之间订阅通话的频道来协作）
* 线程&锁模型（干活的人共享一个小本本，用来协作。注意小本本不能改乱套了，所以得加锁）
* ……

以Java中的线程为例，大家想表达【并发】就启动新的Thread（或者某种等价操作，如利用线程池）；想让Thread之间交互，就要依靠共享内容。但是【并发】的Thread如果同时修改同一份数据就有可能出错（被称为竞争问题），为了解决这个问题就要引入锁（Lock，或者一些高级的同步工具，如CountdownLatch，Semaphore）。  
  


> 特别强调下，Java的线程是表达并发的概念的类。这个类在绝大部分操作系统上使用操作系统内核中的【线程】实现。二者之间还是有一些细微的差异。即用开发者用Java Thread写代码表达思路，和操作系统调度线程执行是两个层面的事情。请努力认识到这一点。

再比如Erlang是基于Actor的并发模型（其实这是原教旨主义的OO）。那么就是每个参与【并发】的任务称为Process（又一个进程……，和【P进程】以及【OS进程都不太一样】，叫【E进程】好了，Erlang中的"进程“）。【E进程】之间通过消息来协作。每个【E进程】要不是在处理消息，要不就是在等新的消息。

如果你用go，那么表达并发的工具就是goroutine，goroutine之间协作要用channel。（当然也可以用Sync包加锁，不展开）。  
  
对于并发模型《7周7并发模型》这本书讲的非常好。推荐阅读。书中展示了七种最经典的并发模型和大量的编码实例。

### 并行

但如果你真的雇了10个人，就可以很容易的让第1个人干第1人份和第2人份的活，第2个人干第3和第4人份的活…… 而这10个人同时在工地上干活，就是【并行】（Parallelism）。

在软件系统中，【程序】是否能【并行】运行，要看物理上有多少个CPU核心可以同时干活（或者再扩展一下，有多少台可用的物理主机）。  


比如你写了个Java程序，同时启动了4个线程，但CPU只有单核，那么同一时刻只有一个线程在运行。如果有4个CPU核心，那么可以做到4个线程完全【并行】运行。如果有2个核心，那么就处于一种中间态。比如你可以用“并发度=4“，”并行度=2“形容这种情况。

### 为啥要并发

把事情设计为【并发】有什么好处呢？假如能同时干活的人只有1个，其实并没有什么好处。【并发】的方法的总耗时总会&gt;=串行的方法。因为【并发】或多或少总会引入需要协作和沟通成本。最小的代价就是不需要沟通，此时【并发】的方法和串行的方法工作量是一样的。

但是【并发】的巨大优势是在可以干活的人数量变多时，马上得到【并行】的好处。假如我们可以得到一个【并发】的【P进程】，并且真的为其配备足够多的人，那么做事的效率就会高很多。回到软件系统，假如有一个【并发】的【程序】，它在只有1个CPU的核心的机器上可以跑，在2个的CPU的也可以跑，在4核CPU上也可以跑。物理上可用CPU核心越多，程序能够越快执行完。而不管在哪里跑，程序本身不用做变化。编程是一件成本很高的事，能够做到程序不变而适应各种环境，可以极大的降低开发成本。你能想象下为1核心CPU开发的Office软件和4核心的不一样吗？  
  


### 并发和并行的关系是什么

【并发】（Concurrency）这个词的本意是指两件事没有谁先谁后的关系，或者说关系不确定。举个通俗的例子，自然数任何两个数字都可以比较大小。我们可以明确地说5 &gt; 3。但是如果换一个领域，并不是任何两个元素都有明确的顺序关系，或者说“谁在前面谁在后面都是可以的“。

对于任务执行这个领域，对于两个任务A和B，如果我们说他们俩是【并发】的，这就要求不能在任务B里使用A的结果，也不能让A执行时使用B的结果。因此在执行层面，A可以在B之前执行，也可以在B执行，或者A和B交替执行，或者A和B【并行】的执行。不管执行层面怎么折腾，结果都是对的。

反过来，如果A的执行需要B的结果，那也就意味着A和B不是【并发】的，必须让B先执行完，A才可以开始。在实现层面，就可以用加锁、channel等方式来表达“先B后A”。

Rob Pike在一个Talk里（[https://blog.golang.org/concurrency-is-not-parallelism](https://link.zhihu.com/?target=https%3A//blog.golang.org/concurrency-is-not-parallelism)）提到了很重要的两个观点：

* **Concurrency is not Parallelism**
* **Concurrency enables parallelism & makes parallelism \(and scaling and everything else\) easy**

前一个观点【并发】和【并行】不是一件事，我们都可以理解了。【并发】说的是**处理**（Deal）的方法；【并行】说的是**执行**（Execution）的方法。

后一个观点指的是，如果想让一个事情变得容易【并行】，先得让制定一个【并发】的方法。倘若一个事情压根就没有【并发】的方法，那么无论有多少个可以干活的人，也不能【并行】。比如你让20个人不铺路，而是一起去拧同一个灯泡，也只能有一个人踩在梯子上去拧，其他19个人只能看着，啥也干不了。

对于一个问题，能不能找到【并发】的办法，取决于问题本身。有些问题很容易【并发】，有些问题可以一部分【并发】其余的串行（比如对数组排序就是，无论怎么拆，最终也要把每个拆开的问题结果合并到一起再排序才行），有些问题则根本上就不能【并发】。找不到【并发】的方法也就意味着不管有多少CPU核心，也没法【并行】执行。

换一个极端，假如为最多20个人设计了【并发】的方法，结果来了40个人，就意味着40人里有20个人是闲着的，是浪费。也就是说【并行】的上限是由【并发】的方法的设计决定的。这就解释了你吃鸡的时候，4核CPU和8核差别不大，因为这个游戏压根就没设计成可以利用这么多个CPU核心。（BTW，但游戏被设计为能充分利用显卡的多核心）

其实上面只是将CPU核心当作是“做事的人“，再广义一点，比如显卡，网卡，磁盘都是独立的可以干活的人。这些组件之间也可以并行的跑。因此，在设计程序的时候，可以比如把计算和IO任务拆开设计一个【并发】的方法，然后利用CPU和网卡是两个零件来【并行】的跑。

作者：大宽宽  
链接：https://www.zhihu.com/question/307100151/answer/894486042  
来源：知乎  
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  
  


### 常见的误解

你可能看到过下面的论断：

> **并发**是多个任务交替使用CPU，同一时刻只有一个任务在跑；**并行**是多个任务同时跑

这个理解不能说全错，但是合到一起就形成了错误的理解。这个错误的理解就是：并发和并行是两个并列的，非此即彼的概念，一个状态要不就是并行的，要不就是并发的。这是完全错误的，实际上看到上面的解释你就会发现【并发】和【并行】描述的是两个频道的事情。正如Rob Pike所言，一个是“如何处理”，一个是“如何执行”。因此，对于：

> **并发**是多个任务交替使用CPU，同一时刻只有一个任务在跑

其实正确的理解是：针对一个问题，想到了一个可以拆解为多个【并发】的任务，这些任务执行时因为只有一个CPU只能“切换”的跑。

对于：

> **并行**是多个任务同时跑

其实的意思是：如果这些并行执行的任务是解决同一个问题的，那么他们既是【并发】的，同时也是【并行】的。

那么可不可以做到只【并行】，而不【并发】呢？当然可以，但这也就意味着【并行】的程序之间没有什么关联，各干各的，就像大街上来来往往的陌生人一样。这的确是【并行】，并且是这个世界的常态。但是一群不认识的，各干各的人是不能一起解决问题的，要一起就得有同一个目标，制定一套沟通的方法，形成【并发】的方案。这种形式在现实当中就是“公司”。  
  


### 为什么要这么理解并发

将并发理解为一种解决问题的方法，其主要用意是表达：一个问题的解决方案是可以由许许多多的并发任务组装（compose）到一起的。这有点像OOP里表达一个类可以由其他类的成员组装到一起一样。

**将大的任务拆解为许许多多小的可以并发的任务是重要的编程思想**。

比如当你在编写一个GET /user/:userId接口时，实际上底层要去3个地方取用户的基本信息（头像、昵称），活动的积分，当前已经下的订单，再组装到一起返回，用nodejs大概可以写成：

```text
const userId = await doGetUserIdByToken(token);

const [userBasic, userScore, userProcessingOrders] = 
  await Promise.all([ // 并发执行下面3个任务
   doGetUserBasic(userId),
   doGetUserScore(userId),
   doGetProcessingOrders(userId)
]);

const user = {...userBasic, ...userScore, ...userProcessingOrders};
return user;
```

这段代码表达的就是这样的流程：![](https://pic1.zhimg.com/50/v2-af9703a9bfa56b1db318445e3336e36b_hd.jpg?source=1940ef5c)

如果把一个并发任务以函数的方式去写就刚好把函数式编程（FP）与并发编程结合起来，就容易得到写起来很舒服，并且有利于并行执行的代码。这也是为什么很多FP语言都天然很适合做并发程序设计的原因。  


### 知道了这些如何做事

我们做事的最终目标是1）能够得到正确的结果；2）能够尽量高效。高效有两个手段：一是优化做事的办法，这相当于改进算法，比如排序用快排而不是冒泡排序，这一点本文就不赘述了；另外一种方式就是让多个worker【并行】干。而为了【并行】，必须先得找到一个【并发】的方案。

我把这个思路的流程画成一张图供大家参考。

![](https://pic1.zhimg.com/80/v2-0058293ffe6f762d58f3881577d756c9_1440w.jpg?source=1940ef5c)

如果你理解了我在说什么就会发现，不管是写程序还是做任何事情，关键点**是想到一个好的做事办法，一个可以Scale的，未来如果资源足够可以容易扩展到并行的办法**。有了这个办法，具体怎么实施，用什么工具是次一级要考虑的问题。

### 高并发

最后再说说【高并发】。其实【高并发】的意思和前面说的【并发】的意思不止是差了一个“高”字，而是个宽泛得多的概念。【高并发】是指可以让软件系统在一段时间内能够处理大量的请求。比如每秒钟可以完成10万个请求。这是互联网系统的一个重要的特征。

不像【并发】说的是“处理”，【并行】说的是“执行”，【高并发】说的是**最终效果**。只要能达到效果，不管怎么实现都行。因此，极端一点【高并发】甚至并不一定需要【并行】，只要处理速度快的足够满足要求就可以。如启动一个nginx的【OS进程】，它只能用到一个CPU核心，也就不可能【并行】。但是他如果能每秒能处理10万个请求，而业务需求只要求8万个请求就可以了，那么这个单进程的nginx本身就算【高并发】了。  
  
当然，现实当中【高并发】的要求会相当“高“（双十一都刷过吧），说的也是完整的业务流程请求，而非简单的HTTP转发。这样的系统大量应用各种【并发】的集中人类智慧的各种方法，并尽可能的【并行】。

除了【并发】和【并行】，【高并发】还需要：

* 数据表普遍被分库分表，否则单机放不下，或者查询性能不足
* 解决分布式事务
* 因为机器都可能坏，为了保证少数机器坏掉不会影响处理的性能，必须引入HA机制

          HA\(High Availability高可用性\)这里指的是hadoop的高可用机制

* 因为系统都有极限，超过极限响应能力就会急剧下降。因此必须引入限流的方案来保护系统
* 这么复杂的系统会涉及到N个service，N个存储，N个队列…… 这些资源的管理又成为了新的问题，这又需要对集群和服务做管理
* 这么多服务，肯定要解决分布式的Tracing和报警问题
* ……

当面试的时候提起【高并发】，大概率是希望面试者聊聊上面这些主题。但请特别特别留意，不同领域的【高并发】实际的意思（怎么算“高”，如何达成，哪些问题是关键问题）会非常不同。电商的高并发，抖音的高并发，12306卖火车票的高并发，基金交易系统的高并发，海量数据处理的高并发，这些问题其实都很不同。所以我很建议每次都讨论具体的问题，而非泛泛谈论【高并发】这个名词。  
  
  


### 商业世界的高并发

拓展一下，从商业上考虑【高并发】，其实际的意思是“**用尽可能少的资源实现足够满足需要的并发请求数量，以形成竞争优势**“。能用有限资源短时间内处理大量请求，也就意味着：

1）单个请求处理成本的降低。比如传统企业处理一单交易成本是10元，而互联网企业压低到了0.1元。这就形成了“规模经济下的低成本结构“，是一种碾压式的竞争优势。

2）提高转化效率。为了获客，市场部门都会拼命做如做拼团、发红包的工作。假设两家公司花同样的预算做获客。公司A的下单系统只能支持1000单/s；而B公司能做到成本不比A公司多很多的情况下实现10000单/s，那么过一段时间，A公司将被彻底打垮。如果你是老板，并且对用户需求很有信息，你会玩命砸技术投入，避免系统成为商业闭环的瓶颈（如果发生了，真坑啊）。

这也就是为啥有些公司突然火起来，然后玩命招技术人员。而做技术的同学能够有工作机会的原因。

但如果【高并发】并非是一个公司的商业闭环的关键问题。公司的商业价值是建立在客户关系之类的事情上，或者单笔交易金额比较大，没必要搞很多用户（比如卖保险）。就没有必要在技术上投入大量资源了。相反，聘请许多好的销售，公关人员才是更重要的。我想你一定看过房产中介公司每天早上喊口号对吧。因此，想要在技术上精进的同学最好也要避免去那些公司。不管在哪里做事情，一定要保证自己做的**直接和商业价值挂钩的事情**才能有更多机会成长。

### 恭喜你

恭喜你看到这里，因为你已经打败了世界上99%的用户。非常高兴你没有被讲懵逼。但为了验证一下你到底懂没懂，我这里有个问题，请不要打我：）

本文中到底提到了哪几种Process？分别都是什么意思？

答案：共3种

* 表示“做事方法”
* 操作系统里表示程序执行实例
* Erlang语言中的并发单元，彼此相互隔离，又俗称“Actor”



